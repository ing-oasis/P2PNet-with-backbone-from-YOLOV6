{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570b5400-49f5-4bc5-9e9f-abd5fd03fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as standard_transforms\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from engine import *\n",
    "from models import build_model\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d29bfcc-835b-4039-8e37-e99d4e934c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Set parameters for P2PNet evaluation', add_help=False)\n",
    "    \n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='vgg16_bn', type=str,\n",
    "                        help=\"name of the convolutional backbone to use\")\n",
    "\n",
    "    parser.add_argument('--row', default=2, type=int,\n",
    "                        help=\"row number of anchor points\")\n",
    "    parser.add_argument('--line', default=2, type=int,\n",
    "                        help=\"line number of anchor points\")\n",
    "\n",
    "    parser.add_argument('--output_dir', default='./logs/',\n",
    "                        help='path where to save')\n",
    "    parser.add_argument('--weight_path', default='./weights/SHTechA.pth',\n",
    "                        help='path where the trained weights saved')\n",
    "\n",
    "    parser.add_argument('--gpu_id', default=0, type=int, help='the gpu used for evaluation')\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a0680c-7107-427e-a994-2ad76e5b503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backbone='vgg16_bn', row=2, line=2, output_dir='./logs/', weight_path='./weights/SHTechA.pth', gpu_id=0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2478/3183036292.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/ding/P2PNet/ckpt/best_mae.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build' is not defined"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('P2PNet evaluation script', parents=[get_args_parser()])\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{}'.format(args.gpu_id)\n",
    "\n",
    "print(args)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = build(args, False)\n",
    "\n",
    "args.resume = '/home/ding/P2PNet/ckpt/best_mae.pth'\n",
    "checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# create the pre-processing transform\n",
    "transform = standard_transforms.Compose([\n",
    "    standard_transforms.ToTensor(), \n",
    "    standard_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa115cf3-c7a7-4312-9f52-458a6b883d58",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backbone='vgg16_bn', row=2, line=2, output_dir='./logs/', weight_path='./weights/SHTechA.pth', gpu_id=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "P2PNet(\n",
       "  (backbone): Backbone_VGG(\n",
       "    (body1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ReLU(inplace=True)\n",
       "    )\n",
       "    (body2): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU(inplace=True)\n",
       "    )\n",
       "    (body3): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU(inplace=True)\n",
       "    )\n",
       "    (body4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (regression): RegressionModel(\n",
       "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act2): ReLU()\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act3): ReLU()\n",
       "    (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act4): ReLU()\n",
       "    (output): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (classification): ClassificationModel(\n",
       "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act2): ReLU()\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act3): ReLU()\n",
       "    (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act4): ReLU()\n",
       "    (output): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (output_act): Sigmoid()\n",
       "  )\n",
       "  (anchor_points): AnchorPoints()\n",
       "  (fpn): Decoder(\n",
       "    (P5_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (P5_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (P5_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (P4_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (P4_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (P4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (P3_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (P3_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (P3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('P2PNet evaluation script', parents=[get_args_parser()])\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "from models import build_model\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{}'.format(args.gpu_id)\n",
    "\n",
    "print(args)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# get the P2PNet\n",
    "model = build_model(args)\n",
    "# move to GPU\n",
    "model.to(device)\n",
    "# load trained model\n",
    "if args.weight_path is not None:\n",
    "    checkpoint = torch.load(args.weight_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "# convert to eval mode\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe8393a2-e513-44aa-8e3a-29aae1cd1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = '/home/ding/Datasets/ShanghaiTech_Crowd_Counting_Dataset/part_A_final/train_data/images'\n",
    "\n",
    "# create the pre-processing transform\n",
    "transform = standard_transforms.Compose([\n",
    "    standard_transforms.ToTensor(), \n",
    "    standard_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# read image file\n",
    "img_names = sorted(os.listdir(img_file))\n",
    "\n",
    "img_raw = Image.open(os.path.join(img_file, img_names[0])).convert('RGB')\n",
    "# round the size\n",
    "width, height = img_raw.size\n",
    "new_width = width // 128 * 128\n",
    "new_height = height // 128 * 128\n",
    "img_raw = img_raw.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "# pre-proccessing\n",
    "img = transform(img_raw)\n",
    "\n",
    "samples = torch.Tensor(img).unsqueeze(0)\n",
    "samples = samples.to(device)\n",
    "\n",
    "#features = model.backbone(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c70a867-018e-40e6-94c9-83911b4f97ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 384, 512])\n",
      "torch.Size([1, 256, 192, 256])\n",
      "torch.Size([1, 512, 96, 128])\n",
      "torch.Size([1, 512, 48, 64])\n"
     ]
    }
   ],
   "source": [
    "for feature in features:\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead83dc5-6702-4bed-9807-9475e1322ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from yolov6.layers.common import RepBlock, SimConv, Transpose\n",
    "\n",
    "\n",
    "class RepPANNeck(nn.Module):\n",
    "    \"\"\"RepPANNeck Module\n",
    "    EfficientRep is the default backbone of this model.\n",
    "    RepPANNeck has the balance of feature fusion ability and hardware efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_list=None,\n",
    "        num_repeats=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert channels_list is not None\n",
    "        assert num_repeats is not None\n",
    "\n",
    "        self.Rep_p4 = RepBlock(\n",
    "            in_channels=channels_list[3] + channels_list[5],\n",
    "            out_channels=channels_list[5],\n",
    "            n=num_repeats[5],\n",
    "        )\n",
    "\n",
    "        self.Rep_p3 = RepBlock(\n",
    "            in_channels=channels_list[2] + channels_list[6],\n",
    "            out_channels=channels_list[6],\n",
    "            n=num_repeats[6]\n",
    "        )\n",
    "\n",
    "        self.Rep_n3 = RepBlock(\n",
    "            in_channels=channels_list[6] + channels_list[7],\n",
    "            out_channels=channels_list[8],\n",
    "            n=num_repeats[7],\n",
    "        )\n",
    "\n",
    "        self.Rep_n4 = RepBlock(\n",
    "            in_channels=channels_list[5] + channels_list[9],\n",
    "            out_channels=channels_list[10],\n",
    "            n=num_repeats[8]\n",
    "        )\n",
    "\n",
    "        self.reduce_layer0 = SimConv(\n",
    "            in_channels=channels_list[4],\n",
    "            out_channels=channels_list[5],\n",
    "            kernel_size=1,\n",
    "            stride=1\n",
    "        )\n",
    "\n",
    "        self.upsample0 = Transpose(\n",
    "            in_channels=channels_list[5],\n",
    "            out_channels=channels_list[5],\n",
    "        )\n",
    "\n",
    "        self.reduce_layer1 = SimConv(\n",
    "            in_channels=channels_list[5],\n",
    "            out_channels=channels_list[6],\n",
    "            kernel_size=1,\n",
    "            stride=1\n",
    "        )\n",
    "\n",
    "        self.upsample1 = Transpose(\n",
    "            in_channels=channels_list[6],\n",
    "            out_channels=channels_list[6]\n",
    "        )\n",
    "\n",
    "        self.downsample2 = SimConv(\n",
    "            in_channels=channels_list[6],\n",
    "            out_channels=channels_list[7],\n",
    "            kernel_size=3,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        self.downsample1 = SimConv(\n",
    "            in_channels=channels_list[8],\n",
    "            out_channels=channels_list[9],\n",
    "            kernel_size=3,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        (C3, C4, C5) = input\n",
    "\n",
    "        fpn_out0 = self.reduce_layer0(C5)\n",
    "        upsample_feat0 = self.upsample0(fpn_out0)\n",
    "        f_concat_layer0 = torch.cat([upsample_feat0, C4], 1)\n",
    "        f_out0 = self.Rep_p4(f_concat_layer0)\n",
    "\n",
    "        fpn_out1 = self.reduce_layer1(f_out0)\n",
    "        upsample_feat1 = self.upsample1(fpn_out1)\n",
    "        f_concat_layer1 = torch.cat([upsample_feat1, C3], 1)\n",
    "        P3 = self.Rep_p3(f_concat_layer1)\n",
    "\n",
    "        down_feat1 = self.downsample2(pan_out2)\n",
    "        p_concat_layer1 = torch.cat([down_feat1, fpn_out1], 1)\n",
    "        P4 = self.Rep_n3(p_concat_layer1)\n",
    "\n",
    "        down_feat0 = self.downsample1(pan_out1)\n",
    "        p_concat_layer2 = torch.cat([down_feat0, fpn_out0], 1)\n",
    "        P5 = self.Rep_n4(p_concat_layer2)\n",
    "\n",
    "        outputs = [P3, P4, P5]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c17f19-2039-49bd-8cd7-700ca27fafe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# run inference\\noutputs = model(samples)\\noutputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]\\noutputs_points = outputs['pred_points'][0]\\n\\n# filter the predictions\\npoints = outputs_points[outputs_scores > threshold].detach().cpu().numpy().tolist()\\npredict_cnt = int((outputs_scores > threshold).sum())\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run inference\n",
    "outputs = model(samples)\n",
    "outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]\n",
    "outputs_points = outputs['pred_points'][0]\n",
    "\n",
    "# filter the predictions\n",
    "points = outputs_points[outputs_scores > threshold].detach().cpu().numpy().tolist()\n",
    "predict_cnt = int((outputs_scores > threshold).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958a93ae-a76f-495b-82a8-d26738613dd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2478/3904539954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# load the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# 展示ShanghaiTech数据集中的预测情况\n",
    "\n",
    "img_file = '/home/ding/Datasets/test_video/2_img'\n",
    "threshold = 0.5\n",
    "\n",
    "img_names = sorted(os.listdir(img_file))\n",
    "\n",
    "# load the images\n",
    "img_raw = Image.open(os.path.join(img_file, img_names[0])).convert('RGB')\n",
    "# round the size\n",
    "width, height = img_raw.size\n",
    "new_width = width // 128 * 128\n",
    "new_height = height // 128 * 128\n",
    "img_raw = img_raw.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "# pre-proccessing\n",
    "img = transform(img_raw)\n",
    "\n",
    "samples = torch.Tensor(img).unsqueeze(0)\n",
    "samples = samples.to(device)\n",
    "# run inference\n",
    "outputs = model(samples)\n",
    "outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]\n",
    "outputs_points = outputs['pred_points'][0]\n",
    "\n",
    "# filter the predictions\n",
    "points = outputs_points[outputs_scores > threshold].detach().cpu().numpy().tolist()\n",
    "predict_cnt = int((outputs_scores > threshold).sum())\n",
    "\n",
    "# draw the predictions\n",
    "img_to_draw = cv2.cvtColor(np.array(img_raw), cv2.COLOR_RGB2BGR)\n",
    "for p in points:\n",
    "    img_to_draw = cv2.circle(img_to_draw, (int(p[0]), int(p[1])), 5, (0, 255, 0), -1)\n",
    "img_to_draw = cv2.resize(img_to_draw, (width, height))\n",
    "# draw the predictions on the image\n",
    "plt.figure(figsize=(16,9))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "plt.imshow(img_to_draw)\n",
    "plt.title(\"prediction: \"+str(predict_cnt))\n",
    "\n",
    "'''\n",
    "# draw the ground truths on the image\n",
    "gt_path = os.path.join(gt_file, 'GT_'+image_name.split('.')[0]+'.mat')\n",
    "gt_m= loadmat(gt_path)\n",
    "gt_point = gt_m[\"image_info\"][0,0][0,0][0]\n",
    "gt_cnt = gt_m[\"image_info\"][0,0][0,0][1][0][0]\n",
    "img_raw = Image.open(img_path).convert('RGB')\n",
    "gt_img_to_draw = cv2.cvtColor(np.array(img_raw), cv2.COLOR_RGB2BGR)\n",
    "for p in gt_point:\n",
    "    gt_img_to_draw = cv2.circle(gt_img_to_draw, (int(p[0]), int(p[1])), 2, (255, 0, 0), -1)\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "plt.imshow(gt_img_to_draw)\n",
    "plt.title(\"ground truth: \"+str(gt_cnt))\n",
    "'''\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8411a175-665a-445f-9949-576e7cce8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, C2_size, C3_size, C4_size, C5_size, feature_size=256):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # upsample C5 to get P5 from the FPN paper\n",
    "        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P5 elementwise to C4\n",
    "        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P4 elementwise to C3\n",
    "        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P3_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # add P3 elementwise to C2\n",
    "        self.P2_1 = nn.Conv2d(C2_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P2_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P2_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        C2, C3, C4, C5 = inputs\n",
    "\n",
    "        P5_x = self.P5_1(C5)\n",
    "        P5_upsampled_x = self.P5_upsampled(P5_x)\n",
    "        P5_x = self.P5_2(P5_x)\n",
    "\n",
    "        P4_x = self.P4_1(C4)\n",
    "        P4_x = P5_upsampled_x + P4_x\n",
    "        P4_upsampled_x = self.P4_upsampled(P4_x)\n",
    "        P4_x = self.P4_2(P4_x)\n",
    "\n",
    "        P3_x = self.P3_1(C3)\n",
    "        P3_x = P4_upsampled_x + P3_x\n",
    "        P3_upsampled_x = self.P3_upsampled(P3_x)\n",
    "        P3_x = self.P3_2(P3_x)\n",
    "        \n",
    "        P2_x = self.P2_1(C2)\n",
    "        P2_x = P2_x + P3_upsampled_x\n",
    "        P2_x = self.P2_2(P2_x)\n",
    "\n",
    "        return [P2_x, P3_x, P4_x, P5_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e8e85c-a6c6-4b1f-bf6d-7c0f2cc8b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from util.misc import (NestedTensor, nested_tensor_from_tensor_list,\n",
    "                       accuracy, get_world_size, interpolate,\n",
    "                       is_dist_avail_and_initialized)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchor_points=4, feature_size=128):\n",
    "        super(RegressionModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        #self.conv3 = nn.Conv2d(feature_size, out_channels=96, kernel_size=1)\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchor_points * 2, kernel_size=3, padding=1)\n",
    "    # sub-branch forward\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "        \n",
    "        #out = self.conv3(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        return out.contiguous().view(out.shape[0], -1, 2)\n",
    "    \n",
    "\n",
    "# the network frmawork of the classification branch\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchor_points=4, num_classes=80, prior=0.01, feature_size=128):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchor_points = num_anchor_points\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchor_points * num_classes, kernel_size=3, padding=1)\n",
    "        self.output_act = nn.Sigmoid()\n",
    "    # sub-branch forward\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "\n",
    "        out1 = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        batch_size, width, height, _ = out1.shape\n",
    "\n",
    "        out2 = out1.view(batch_size, width, height, self.num_anchor_points, self.num_classes)\n",
    "\n",
    "        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n",
    "    \n",
    "def generate_anchor_points(stride=16, row=3, line=3):\n",
    "    row_step = stride / row\n",
    "    line_step = stride / line\n",
    "\n",
    "    shift_x = (np.arange(1, line + 1) - 0.5) * line_step - stride / 2\n",
    "    shift_y = (np.arange(1, row + 1) - 0.5) * row_step - stride / 2\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    anchor_points = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel()\n",
    "    )).transpose()\n",
    "\n",
    "    return anchor_points\n",
    "# shift the meta-anchor to get an acnhor points\n",
    "def shift(shape, stride, anchor_points):\n",
    "    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n",
    "    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    shifts = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel()\n",
    "    )).transpose()\n",
    "\n",
    "    A = anchor_points.shape[0]\n",
    "    K = shifts.shape[0]\n",
    "    all_anchor_points = (anchor_points.reshape((1, A, 2)) + shifts.reshape((1, K, 2)).transpose((1, 0, 2)))\n",
    "    all_anchor_points = all_anchor_points.reshape((K * A, 2))\n",
    "\n",
    "    return all_anchor_points\n",
    "\n",
    "# this class generate all reference points on all pyramid levels\n",
    "class AnchorPoints(nn.Module):\n",
    "    def __init__(self, pyramid_levels=None, strides=None, row=3, line=3):\n",
    "        super(AnchorPoints, self).__init__()\n",
    "\n",
    "        if pyramid_levels is None:\n",
    "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
    "        else:\n",
    "            self.pyramid_levels = pyramid_levels\n",
    "\n",
    "        if strides is None:\n",
    "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
    "\n",
    "        self.row = row\n",
    "        self.line = line\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_shape = image.shape[2:]\n",
    "        image_shape = np.array(image_shape)\n",
    "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
    "\n",
    "        all_anchor_points = np.zeros((0, 2)).astype(np.float32)\n",
    "        # get reference points for each level\n",
    "        for idx, p in enumerate(self.pyramid_levels):\n",
    "            anchor_points = generate_anchor_points(2**p, row=self.row, line=self.line)\n",
    "            shifted_anchor_points = shift(image_shapes[idx], self.strides[idx], anchor_points)\n",
    "            all_anchor_points = np.append(all_anchor_points, shifted_anchor_points, axis=0)\n",
    "\n",
    "        all_anchor_points = np.expand_dims(all_anchor_points, axis=0)\n",
    "        # send reference points to device\n",
    "        if torch.cuda.is_available():\n",
    "        #if False:\n",
    "            return torch.from_numpy(all_anchor_points.astype(np.float32)).cuda()\n",
    "        else:\n",
    "            return torch.from_numpy(all_anchor_points.astype(np.float32))\n",
    "        \n",
    "class SetCriterion_Crowd(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[0] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_points):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
    "        \"\"\"\n",
    "        assert 'pred_logits' in outputs\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], 0,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def loss_points(self, outputs, targets, indices, num_points):\n",
    "\n",
    "        assert 'pred_points' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_points = outputs['pred_points'][idx]\n",
    "        target_points = torch.cat([t['point'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_bbox = F.mse_loss(src_points, target_points, reduction='none')\n",
    "\n",
    "        losses = {}\n",
    "        losses['loss_point'] = loss_bbox.sum() / num_points\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_points, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'points': self.loss_points,\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_points, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        output1 = {'pred_logits': outputs['pred_logits'], 'pred_points': outputs['pred_points']}\n",
    "\n",
    "        indices1 = self.matcher(output1, targets)\n",
    "\n",
    "        num_points = sum(len(t[\"labels\"]) for t in targets)\n",
    "        num_points = torch.as_tensor([num_points], dtype=torch.float, device=next(iter(output1.values())).device)\n",
    "        if is_dist_avail_and_initialized():\n",
    "            torch.distributed.all_reduce(num_points)\n",
    "        num_boxes = torch.clamp(num_points / get_world_size(), min=1).item()\n",
    "\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, output1, targets, indices1, num_boxes))\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c1e9057-4ba2-4a76-a9e9-59a3fc34e1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfeatures = backbone(samples)\\nfpn = Decoder(96,192,384,1408).to(device)\\nfeatures_fpn = fpn(features)\\nbatch_size = features[0].shape[0]\\n# run the regression and classification branch\\nregression_module = RegressionModel(num_features_in=256, num_anchor_points=4).to(device)\\nregression = regression_module(features_fpn[1]) * 64 # 8x\\nclassification_module = ClassificationModel(num_features_in=256, num_classes=2, num_anchor_points=4).to(device)\\nclassification = classification_module(features_fpn[1])\\nanchor_points = model.anchor_points(samples).repeat(batch_size, 1, 1)\\n# decode the points as prediction\\noutput_coord = regression + anchor_points\\noutput_class = classification\\noutputs = {'pred_logits': output_class, 'pred_points': output_coord}\\n\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "features = backbone(samples)\n",
    "fpn = Decoder(96,192,384,1408).to(device)\n",
    "features_fpn = fpn(features)\n",
    "batch_size = features[0].shape[0]\n",
    "# run the regression and classification branch\n",
    "regression_module = RegressionModel(num_features_in=256, num_anchor_points=4).to(device)\n",
    "regression = regression_module(features_fpn[1]) * 64 # 8x\n",
    "classification_module = ClassificationModel(num_features_in=256, num_classes=2, num_anchor_points=4).to(device)\n",
    "classification = classification_module(features_fpn[1])\n",
    "anchor_points = model.anchor_points(samples).repeat(batch_size, 1, 1)\n",
    "# decode the points as prediction\n",
    "output_coord = regression + anchor_points\n",
    "output_class = classification\n",
    "outputs = {'pred_logits': output_class, 'pred_points': output_coord}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3489326e-451a-49ce-a33f-84b5e829c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the defenition of the P2PNet model\n",
    "class New_P2PNet(nn.Module):\n",
    "    def __init__(self, backbone, neck, row=2, line=2):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        #backbone.load_state_dict(torch.load('/home/ding/P2PNet/RepVGG-A2-train.pth'))\n",
    "        self.num_classes = 2\n",
    "        # the number of all anchor points\n",
    "        num_anchor_points = row * line\n",
    "\n",
    "        self.regression = RegressionModel(num_features_in=128, num_anchor_points=num_anchor_points)\n",
    "        self.classification = ClassificationModel(num_features_in=128, \\\n",
    "                                            num_classes=self.num_classes, \\\n",
    "                                            num_anchor_points=num_anchor_points)\n",
    "\n",
    "        self.anchor_points = AnchorPoints(pyramid_levels=[4,], row=row, line=line)\n",
    "\n",
    "        #self.fpn = Decoder(256, 512, 512)\n",
    "        self.neck = neck\n",
    "\n",
    "    def forward(self, samples: NestedTensor):\n",
    "        # get the backbone features\n",
    "        # 冻结RepVGG的参数，不更新权重\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(samples)\n",
    "        features_fpn = self.neck(features)\n",
    "\n",
    "        batch_size = features[1].shape[0]\n",
    "        # run the regression and classification branch\n",
    "        regression = self.regression(features_fpn[1]) * 64 # 8x\n",
    "        classification = self.classification(features_fpn[1])\n",
    "        anchor_points = self.anchor_points(samples).repeat(batch_size, 1, 1)\n",
    "        # decode the points as prediction\n",
    "        output_coord = regression + anchor_points\n",
    "        output_class = classification\n",
    "        out = {'pred_logits': output_class, 'pred_points': output_coord}\n",
    "       \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97bd70a7-d430-421c-bd46-39b64d5d00d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 1 GPU for training... \n",
      "Model: Model(\n",
      "  (backbone): EfficientRep(\n",
      "    (stem): RepVGGBlock(\n",
      "      (nonlinearity): ReLU(inplace=True)\n",
      "      (se): Identity()\n",
      "      (rbr_dense): Sequential(\n",
      "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (rbr_1x1): Sequential(\n",
      "        (conv): Conv2d(3, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (ERBlock_2): Sequential(\n",
      "      (0): RepVGGBlock(\n",
      "        (nonlinearity): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (rbr_dense): Sequential(\n",
      "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (rbr_1x1): Sequential(\n",
      "          (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): RepBlock(\n",
      "        (conv1): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (block): Sequential(\n",
      "          (0): RepVGGBlock(\n",
      "            (nonlinearity): ReLU(inplace=True)\n",
      "            (se): Identity()\n",
      "            (rbr_identity): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (rbr_dense): Sequential(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (rbr_1x1): Sequential(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ERBlock_3): Sequential(\n",
      "      (0): RepVGGBlock(\n",
      "        (nonlinearity): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (rbr_dense): Sequential(\n",
      "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (rbr_1x1): Sequential(\n",
      "          (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): RepBlock(\n",
      "        (conv1): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (block): Sequential(\n",
      "          (0): RepVGGBlock(\n",
      "            (nonlinearity): ReLU(inplace=True)\n",
      "            (se): Identity()\n",
      "            (rbr_identity): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (rbr_dense): Sequential(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (rbr_1x1): Sequential(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): RepVGGBlock(\n",
      "            (nonlinearity): ReLU(inplace=True)\n",
      "            (se): Identity()\n",
      "            (rbr_identity): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (rbr_dense): Sequential(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (rbr_1x1): Sequential(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (2): RepVGGBlock(\n",
      "            (nonlinearity): ReLU(inplace=True)\n",
      "            (se): Identity()\n",
      "            (rbr_identity): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (rbr_dense): Sequential(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (rbr_1x1): Sequential(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ERBlock_4): Sequential(\n",
      "      (0): RepVGGBlock(\n",
      "        (nonlinearity): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (rbr_dense): Sequential(\n",
      "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (rbr_1x1): Sequential(\n",
      "          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): RepBlock(\n",
      "        (conv1): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (block): Sequential(\n",
      "          (0): RepVGGBlock(\n",
      "            (nonlinearity): ReLU(inplace=True)\n",
      "            (se): Identity()\n",
      "            (rbr_identity): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (rbr_dense): Sequential(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (rbr_1x1): Sequential(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): RepVGGBlock(\n",
      "            (nonlinearity): ReLU(inplace=True)\n",
      "            (se): Identity()\n",
      "            (rbr_identity): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (rbr_dense): Sequential(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (rbr_1x1): Sequential(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (2): RepVGGBlock(\n",
      "            (nonlinearity): ReLU(inplace=True)\n",
      "            (se): Identity()\n",
      "            (rbr_identity): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (rbr_dense): Sequential(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (rbr_1x1): Sequential(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (3): RepVGGBlock(\n",
      "            (nonlinearity): ReLU(inplace=True)\n",
      "            (se): Identity()\n",
      "            (rbr_identity): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (rbr_dense): Sequential(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (rbr_1x1): Sequential(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (4): RepVGGBlock(\n",
      "            (nonlinearity): ReLU(inplace=True)\n",
      "            (se): Identity()\n",
      "            (rbr_identity): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (rbr_dense): Sequential(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (rbr_1x1): Sequential(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ERBlock_5): Sequential(\n",
      "      (0): RepVGGBlock(\n",
      "        (nonlinearity): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (rbr_dense): Sequential(\n",
      "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (rbr_1x1): Sequential(\n",
      "          (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): RepBlock(\n",
      "        (conv1): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (block): Sequential(\n",
      "          (0): RepVGGBlock(\n",
      "            (nonlinearity): ReLU(inplace=True)\n",
      "            (se): Identity()\n",
      "            (rbr_identity): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (rbr_dense): Sequential(\n",
      "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (rbr_1x1): Sequential(\n",
      "              (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): SimSPPF(\n",
      "        (cv1): SimConv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (cv2): SimConv(\n",
      "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): ReLU(inplace=True)\n",
      "        )\n",
      "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (neck): RepPANNeck(\n",
      "    (Rep_p4): RepBlock(\n",
      "      (conv1): RepVGGBlock(\n",
      "        (nonlinearity): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (rbr_dense): Sequential(\n",
      "          (conv): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (rbr_1x1): Sequential(\n",
      "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (block): Sequential(\n",
      "        (0): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (Rep_p3): RepBlock(\n",
      "      (conv1): RepVGGBlock(\n",
      "        (nonlinearity): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (rbr_dense): Sequential(\n",
      "          (conv): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (rbr_1x1): Sequential(\n",
      "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (block): Sequential(\n",
      "        (0): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (Rep_n3): RepBlock(\n",
      "      (conv1): RepVGGBlock(\n",
      "        (nonlinearity): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (rbr_identity): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (rbr_dense): Sequential(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (rbr_1x1): Sequential(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (block): Sequential(\n",
      "        (0): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (Rep_n4): RepBlock(\n",
      "      (conv1): RepVGGBlock(\n",
      "        (nonlinearity): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (rbr_identity): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (rbr_dense): Sequential(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (rbr_1x1): Sequential(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (block): Sequential(\n",
      "        (0): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): RepVGGBlock(\n",
      "          (nonlinearity): ReLU(inplace=True)\n",
      "          (se): Identity()\n",
      "          (rbr_identity): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (rbr_dense): Sequential(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (rbr_1x1): Sequential(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reduce_layer0): SimConv(\n",
      "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "    )\n",
      "    (upsample0): Transpose(\n",
      "      (upsample_transpose): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (reduce_layer1): SimConv(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "    )\n",
      "    (upsample1): Transpose(\n",
      "      (upsample_transpose): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (downsample2): SimConv(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "    )\n",
      "    (downsample1): SimConv(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (detect): Detect(\n",
      "    (cls_convs): ModuleList(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (reg_convs): ModuleList(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (cls_preds): ModuleList(\n",
      "      (0): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (reg_preds): ModuleList(\n",
      "      (0): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (obj_preds): ModuleList(\n",
      "      (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (stems): ModuleList(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def get_yolo_args_parser(add_help=True):\n",
    "    parser = argparse.ArgumentParser(description='YOLOv6 PyTorch Training', add_help=add_help)\n",
    "    parser.add_argument('--data-path', default='./data/coco.yaml', type=str, help='dataset path')\n",
    "    parser.add_argument('--conf-file', default='./configs/yolov6s.py', type=str, help='experiment description file')\n",
    "    parser.add_argument('--img-size', type=int, default=640, help='train, val image size (pixels)')\n",
    "    parser.add_argument('--batch-size', default=32, type=int, help='total batch size for all GPUs')\n",
    "    parser.add_argument('--epochs', default=400, type=int, help='number of total epochs to run')\n",
    "    parser.add_argument('--workers', default=8, type=int, help='number of data loading workers (default: 8)')\n",
    "    parser.add_argument('--device', default='0', type=str, help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--noval', action='store_true', help='only evaluate in final epoch')\n",
    "    parser.add_argument('--check-images', action='store_true', help='check images when initializing datasets')\n",
    "    parser.add_argument('--check-labels', action='store_true', help='check label files when initializing datasets')\n",
    "    parser.add_argument('--output-dir', default='./runs/train', type=str, help='path to save outputs')\n",
    "    parser.add_argument('--name', default='exp', type=str, help='experiment name, save to output_dir/name')\n",
    "    parser.add_argument('--dist_url', type=str, default=\"tcp://127.0.0.1:8888\")\n",
    "    parser.add_argument('--gpu_count', type=int, default=0)\n",
    "    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')\n",
    "\n",
    "    return parser\n",
    "\n",
    "def check_and_init(args):\n",
    "    '''check config files and device, and initialize '''\n",
    "\n",
    "    # check files\n",
    "    args.save_dir = osp.join(args.output_dir, args.name)\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    cfg = Config.fromfile(args.conf_file)\n",
    "\n",
    "    # check device\n",
    "    device = select_device(args.device)\n",
    "\n",
    "    # set random seed\n",
    "    set_random_seed(1+args.rank, deterministic=(args.rank == -1))\n",
    "\n",
    "    # save args\n",
    "    save_yaml(vars(args), osp.join(args.save_dir, 'args.yaml'))\n",
    "\n",
    "    return cfg, device\n",
    "\n",
    "from yolov6.utils.envs import get_envs, select_device, set_random_seed\n",
    "from yolov6.utils.config import Config\n",
    "from yolov6.utils.events import LOGGER, save_yaml\n",
    "import os.path as osp\n",
    "\n",
    "yolo_parser = get_yolo_args_parser()\n",
    "yolo_args = yolo_parser.parse_known_args()[0]\n",
    "yolo_args.rank, yolo_args.local_rank, yolo_args.world_size = get_envs()\n",
    "yolo_cfg, device = check_and_init(yolo_args)\n",
    "\n",
    "from yolov6.models.yolo import build_model\n",
    "def get_model(args, cfg, nc, device):\n",
    "    model = build_model(cfg, nc, device)\n",
    "    weights = cfg.model.pretrained\n",
    "    if weights:  # finetune if pretrained model is set\n",
    "        LOGGER.info(f'Loading state_dict from {weights} for fine-tuning...')\n",
    "        model = load_state_dict(weights, model, map_location=device)\n",
    "    LOGGER.info('Model: {}'.format(model))\n",
    "    return model\n",
    "\n",
    "yolo_model = get_model(yolo_args, yolo_cfg, 2, device)\n",
    "\n",
    "from models.matcher import build_matcher_crowd\n",
    "\n",
    "def build(args, training):\n",
    "    # treats persons as a single class\n",
    "    num_classes = 1\n",
    "\n",
    "    backbone = yolo_model.backbone\n",
    "    neck = yolo_model.neck\n",
    "    model = New_P2PNet(backbone, neck, args.row, args.line)\n",
    "    if not training: \n",
    "        return model\n",
    "\n",
    "    weight_dict = {'loss_ce': 1, 'loss_points': args.point_loss_coef}\n",
    "    losses = ['labels', 'points']\n",
    "    matcher = build_matcher_crowd(args)\n",
    "    criterion = SetCriterion_Crowd(num_classes, \\\n",
    "                                matcher=matcher, weight_dict=weight_dict, \\\n",
    "                                eos_coef=args.eos_coef, losses=losses)\n",
    "\n",
    "    return model, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2257ff8-7e06-4ae4-bd7b-e0100b193b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Set parameters for training P2PNet', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=16, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=4500, type=int)\n",
    "    parser.add_argument('--lr_drop', default=1500, type=int)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='vgg16_bn', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "\n",
    "    parser.add_argument('--set_cost_point', default=0.05, type=float,\n",
    "                        help=\"L1 point coefficient in the matching cost\")\n",
    "\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--point_loss_coef', default=0.0002, type=float)\n",
    "\n",
    "    parser.add_argument('--eos_coef', default=0.5, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "    parser.add_argument('--row', default=2, type=int,\n",
    "                        help=\"row number of anchor points\")\n",
    "    parser.add_argument('--line', default=2, type=int,\n",
    "                        help=\"line number of anchor points\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='SHHA')\n",
    "    parser.add_argument('--data_root', default='/home/ding/Datasets/ShanghaiTech_Crowd_Counting_Dataset/part_A_final',\n",
    "                        help='path where the dataset is')\n",
    "    \n",
    "    parser.add_argument('--output_dir', default='./logs',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--checkpoints_dir', default='./ckpt',\n",
    "                        help='path where to save checkpoints, empty for no saving')\n",
    "    parser.add_argument('--tensorboard_dir', default='./runs',\n",
    "                        help='path where to save, empty for no saving')\n",
    "\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=8, type=int)\n",
    "    parser.add_argument('--eval_freq', default=10, type=int,\n",
    "                        help='frequency of evaluation, default setting is evaluating in every 5 epoch')\n",
    "    parser.add_argument('--gpu_id', default=0, type=int, help='the gpu used for training')\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = argparse.ArgumentParser('P2PNet training and evaluation script', parents=[get_args_parser()])\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ccc9dc-1a76-4bf6-ad1b-effeaf007f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "# load image and gt pairs\n",
    "def load_data(img_gt_path, train):\n",
    "    img_path, gt_path = img_gt_path\n",
    "    # load the images\n",
    "    img = cv2.imread(img_path)\n",
    "    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    # load ground truth points\n",
    "    data = scipy.io.loadmat(gt_path)\n",
    "    points = data['image_info'][0][0][0][0][0]\n",
    "\n",
    "    return img, points\n",
    "\n",
    "# random crop augumentation\n",
    "def random_crop(img, den, num_patch=4):\n",
    "    half_h = 128\n",
    "    half_w = 128\n",
    "    result_img = np.zeros([num_patch, img.shape[0], half_h, half_w])\n",
    "    result_den = []\n",
    "    # crop num_patch for each image\n",
    "    for i in range(num_patch):\n",
    "        start_h = random.randint(0, img.size(1) - half_h)\n",
    "        start_w = random.randint(0, img.size(2) - half_w)\n",
    "        end_h = start_h + half_h\n",
    "        end_w = start_w + half_w\n",
    "        # copy the cropped rect\n",
    "        result_img[i] = img[:, start_h:end_h, start_w:end_w]\n",
    "        # copy the cropped points\n",
    "        idx = (den[:, 0] >= start_w) & (den[:, 0] <= end_w) & (den[:, 1] >= start_h) & (den[:, 1] <= end_h)\n",
    "        # shift the corrdinates\n",
    "        record_den = den[idx]\n",
    "        record_den[:, 0] -= start_w\n",
    "        record_den[:, 1] -= start_h\n",
    "\n",
    "        result_den.append(record_den)\n",
    "\n",
    "    return result_img, result_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8bbde7-9603-48eb-b6a7-162fcaa8c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# dataset load\n",
    "class SHHA(Dataset):\n",
    "    def __init__(self, data_root, transform=None, train=False, patch=False, flip=False):\n",
    "        self.root_path = data_root #/home/ding/Datasets/ShanghaiTech_Crowd_Counting_Dataset/part_A_final\n",
    "        img_dir = os.path.join(self.root_path, 'train_data' if train else 'test_data')\n",
    "        image_names = sorted(os.listdir(os.path.join(img_dir, 'images')))\n",
    "            \n",
    "        self.img_list = [os.path.join(img_dir, 'images', i) for i in image_names]\n",
    "        self.img_map = {}\n",
    "        for _, image_name in enumerate(image_names):\n",
    "            gt_name = 'GT_' + image_name.split('.')[0] + '.mat'\n",
    "            self.img_map[os.path.join(img_dir, 'images', image_name)] = os.path.join(img_dir, 'ground_truth', gt_name)\n",
    "        \n",
    "        '''\n",
    "        self.train_lists = \"shanghai_tech_part_a_train.list\"\n",
    "        self.eval_list = \"shanghai_tech_part_a_test.list\"\n",
    "        # there may exist multiple list files\n",
    "        self.img_list_file = self.train_lists.split(',')\n",
    "        if train:\n",
    "            self.img_list_file = self.train_lists.split(',')\n",
    "        else:\n",
    "            self.img_list_file = self.eval_list.split(',')\n",
    "\n",
    "        self.img_map = {}\n",
    "        self.img_list = []\n",
    "        # loads the image/gt pairs\n",
    "        for _, train_list in enumerate(self.img_list_file):\n",
    "            train_list = train_list.strip()\n",
    "            with open(os.path.join(self.root_path, train_list)) as fin:\n",
    "                for line in fin:\n",
    "                    if len(line) < 2: \n",
    "                        continue\n",
    "                    line = line.strip().split()\n",
    "                    self.img_map[os.path.join(self.root_path, line[0].strip())] = \\\n",
    "                                    os.path.join(self.root_path, line[1].strip())\n",
    "        self.img_list = sorted(list(self.img_map.keys()))\n",
    "        '''\n",
    "        # number of samples\n",
    "        self.nSamples = len(self.img_list)\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.patch = patch\n",
    "        self.flip = flip\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index <= len(self), 'index range error'\n",
    "\n",
    "        img_path = self.img_list[index]\n",
    "        gt_path = self.img_map[img_path]\n",
    "        # load image and ground truth\n",
    "        img, point = load_data((img_path, gt_path), self.train)\n",
    "        # applu augumentation\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.train:\n",
    "            # data augmentation -> random scale\n",
    "            scale_range = [0.7, 1.3]\n",
    "            min_size = min(img.shape[1:])\n",
    "            scale = random.uniform(*scale_range)\n",
    "            # scale the image and points\n",
    "            if scale * min_size > 128:\n",
    "                img = torch.nn.functional.upsample_bilinear(img.unsqueeze(0), scale_factor=scale).squeeze(0)\n",
    "                point *= scale\n",
    "        # random crop augumentaiton\n",
    "        if self.train and self.patch:\n",
    "            img, point = random_crop(img, point)\n",
    "            for i, _ in enumerate(point):\n",
    "                point[i] = torch.Tensor(point[i])\n",
    "        # random flipping\n",
    "        if random.random() > 0.5 and self.train and self.flip:\n",
    "            # random flip\n",
    "            img = torch.Tensor(img[:, :, :, ::-1].copy())\n",
    "            for i, _ in enumerate(point):\n",
    "                point[i][:, 0] = 128 - point[i][:, 0]\n",
    "\n",
    "        if not self.train:\n",
    "            point = [point]\n",
    "\n",
    "        img = torch.Tensor(img)\n",
    "        # pack up related infos\n",
    "        target = [{} for i in range(len(point))]\n",
    "        for i, _ in enumerate(point):\n",
    "            target[i]['point'] = torch.Tensor(point[i])\n",
    "            image_id = int(img_path.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "            image_id = torch.Tensor([image_id]).long()\n",
    "            target[i]['image_id'] = image_id\n",
    "            target[i]['labels'] = torch.ones([point[i].shape[0]]).long()\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "def loading_data(data_root):\n",
    "    # the pre-proccssing transform\n",
    "    transform = standard_transforms.Compose([\n",
    "        standard_transforms.ToTensor(), \n",
    "        standard_transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    # create the training dataset\n",
    "    train_set = SHHA(data_root, train=True, transform=transform, patch=True, flip=True)\n",
    "    # create the validation dataset\n",
    "    val_set = SHHA(data_root, train=False, transform=transform)\n",
    "\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d737ebf1-2e58-4f65-9f22-b3de8e7bebc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=16, weight_decay=0.0001, epochs=4500, lr_drop=1500, clip_max_norm=0.1, frozen_weights=None, backbone='vgg16_bn', set_cost_class=1, set_cost_point=0.05, point_loss_coef=0.0002, eos_coef=0.5, row=2, line=2, dataset_file='SHHA', data_root='/home/ding/Datasets/ShanghaiTech_Crowd_Counting_Dataset/part_A_final', output_dir='./logs', checkpoints_dir='./ckpt', tensorboard_dir='./runs', seed=42, resume='', start_epoch=0, eval=False, num_workers=8, eval_freq=10, gpu_id=0)\n",
      "number of params: 17802896\n",
      "Start training\n",
      "Averaged stats: lr: 0.000100  loss: 0.1777 (0.1916)  loss_ce: 0.1777 (0.1916)  loss_ce_unscaled: 0.1777 (0.1916)  loss_point_unscaled: 50.6025 (72.0744)\n",
      "[ep 0][lr 0.0001000][3.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1907 (0.1901)  loss_ce: 0.1907 (0.1901)  loss_ce_unscaled: 0.1907 (0.1901)  loss_point_unscaled: 51.5557 (70.4763)\n",
      "[ep 1][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1849 (0.1845)  loss_ce: 0.1849 (0.1845)  loss_ce_unscaled: 0.1849 (0.1845)  loss_point_unscaled: 50.6054 (88.6164)\n",
      "[ep 2][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1934 (0.1994)  loss_ce: 0.1934 (0.1994)  loss_ce_unscaled: 0.1934 (0.1994)  loss_point_unscaled: 51.7981 (99.6673)\n",
      "[ep 3][lr 0.0001000][2.73s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1924 (0.1961)  loss_ce: 0.1924 (0.1961)  loss_ce_unscaled: 0.1924 (0.1961)  loss_point_unscaled: 57.9721 (89.4553)\n",
      "[ep 4][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1867 (0.1838)  loss_ce: 0.1867 (0.1838)  loss_ce_unscaled: 0.1867 (0.1838)  loss_point_unscaled: 55.1820 (88.9181)\n",
      "[ep 5][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1881 (0.1960)  loss_ce: 0.1881 (0.1960)  loss_ce_unscaled: 0.1881 (0.1960)  loss_point_unscaled: 54.7196 (100.8963)\n",
      "[ep 6][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1951 (0.1949)  loss_ce: 0.1951 (0.1949)  loss_ce_unscaled: 0.1951 (0.1949)  loss_point_unscaled: 56.9163 (72.1657)\n",
      "[ep 7][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1918 (0.1937)  loss_ce: 0.1918 (0.1937)  loss_ce_unscaled: 0.1918 (0.1937)  loss_point_unscaled: 53.0168 (72.2620)\n",
      "[ep 8][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1921 (0.1949)  loss_ce: 0.1921 (0.1949)  loss_ce_unscaled: 0.1921 (0.1949)  loss_point_unscaled: 53.8151 (74.1254)\n",
      "[ep 9][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1947)  loss_ce: 0.1872 (0.1947)  loss_ce_unscaled: 0.1872 (0.1947)  loss_point_unscaled: 51.0777 (125.4922)\n",
      "[ep 10][lr 0.0001000][3.56s]\n",
      "=======================================test=======================================\n",
      "mae: 155.22527472527472 mse: 234.51721509480743 time: 2.331329107284546 best mae: 155.22527472527472\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1878 (0.1891)  loss_ce: 0.1878 (0.1891)  loss_ce_unscaled: 0.1878 (0.1891)  loss_point_unscaled: 56.2915 (65.0918)\n",
      "[ep 11][lr 0.0001000][2.93s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1783 (0.1867)  loss_ce: 0.1783 (0.1867)  loss_ce_unscaled: 0.1783 (0.1867)  loss_point_unscaled: 50.9929 (65.9959)\n",
      "[ep 12][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1996 (0.2034)  loss_ce: 0.1996 (0.2034)  loss_ce_unscaled: 0.1996 (0.2034)  loss_point_unscaled: 52.9856 (101.6301)\n",
      "[ep 13][lr 0.0001000][2.51s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1910)  loss_ce: 0.1823 (0.1910)  loss_ce_unscaled: 0.1823 (0.1910)  loss_point_unscaled: 50.4750 (56.6328)\n",
      "[ep 14][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1864 (0.1924)  loss_ce: 0.1864 (0.1924)  loss_ce_unscaled: 0.1864 (0.1924)  loss_point_unscaled: 57.0745 (81.5568)\n",
      "[ep 15][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1848 (0.1875)  loss_ce: 0.1848 (0.1875)  loss_ce_unscaled: 0.1848 (0.1875)  loss_point_unscaled: 51.0820 (68.4894)\n",
      "[ep 16][lr 0.0001000][2.86s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1780 (0.1931)  loss_ce: 0.1780 (0.1931)  loss_ce_unscaled: 0.1780 (0.1931)  loss_point_unscaled: 53.1934 (89.9836)\n",
      "[ep 17][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1893 (0.1904)  loss_ce: 0.1893 (0.1904)  loss_ce_unscaled: 0.1893 (0.1904)  loss_point_unscaled: 51.2911 (75.7300)\n",
      "[ep 18][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1850 (0.1900)  loss_ce: 0.1850 (0.1900)  loss_ce_unscaled: 0.1850 (0.1900)  loss_point_unscaled: 51.1803 (67.7635)\n",
      "[ep 19][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1904 (0.1861)  loss_ce: 0.1904 (0.1861)  loss_ce_unscaled: 0.1904 (0.1861)  loss_point_unscaled: 51.4560 (63.3032)\n",
      "[ep 20][lr 0.0001000][3.11s]\n",
      "=======================================test=======================================\n",
      "mae: 148.63186813186815 mse: 223.46687686309363 time: 2.33439040184021 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1798 (0.1889)  loss_ce: 0.1798 (0.1889)  loss_ce_unscaled: 0.1798 (0.1889)  loss_point_unscaled: 56.3090 (92.3033)\n",
      "[ep 21][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1923 (0.1974)  loss_ce: 0.1923 (0.1974)  loss_ce_unscaled: 0.1923 (0.1974)  loss_point_unscaled: 50.2254 (83.0517)\n",
      "[ep 22][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2005 (0.1952)  loss_ce: 0.2005 (0.1952)  loss_ce_unscaled: 0.2005 (0.1952)  loss_point_unscaled: 54.3210 (109.0681)\n",
      "[ep 23][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2004 (0.1989)  loss_ce: 0.2004 (0.1989)  loss_ce_unscaled: 0.2004 (0.1989)  loss_point_unscaled: 57.9177 (103.2045)\n",
      "[ep 24][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1892 (0.1898)  loss_ce: 0.1892 (0.1898)  loss_ce_unscaled: 0.1892 (0.1898)  loss_point_unscaled: 50.3018 (64.4894)\n",
      "[ep 25][lr 0.0001000][2.62s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1961)  loss_ce: 0.1808 (0.1961)  loss_ce_unscaled: 0.1808 (0.1961)  loss_point_unscaled: 70.3115 (114.7337)\n",
      "[ep 26][lr 0.0001000][2.99s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1903)  loss_ce: 0.1808 (0.1903)  loss_ce_unscaled: 0.1808 (0.1903)  loss_point_unscaled: 51.6062 (58.0001)\n",
      "[ep 27][lr 0.0001000][2.87s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1894 (0.2082)  loss_ce: 0.1894 (0.2082)  loss_ce_unscaled: 0.1894 (0.2082)  loss_point_unscaled: 52.7665 (107.8384)\n",
      "[ep 28][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1805 (0.1867)  loss_ce: 0.1805 (0.1867)  loss_ce_unscaled: 0.1805 (0.1867)  loss_point_unscaled: 49.5214 (95.8725)\n",
      "[ep 29][lr 0.0001000][2.55s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1782 (0.1882)  loss_ce: 0.1782 (0.1882)  loss_ce_unscaled: 0.1782 (0.1882)  loss_point_unscaled: 52.9858 (86.5774)\n",
      "[ep 30][lr 0.0001000][2.96s]\n",
      "=======================================test=======================================\n",
      "mae: 160.44505494505495 mse: 250.9622908497837 time: 4.10290002822876 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1961 (0.1994)  loss_ce: 0.1961 (0.1994)  loss_ce_unscaled: 0.1961 (0.1994)  loss_point_unscaled: 51.2473 (89.9929)\n",
      "[ep 31][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1833 (0.1890)  loss_ce: 0.1833 (0.1890)  loss_ce_unscaled: 0.1833 (0.1890)  loss_point_unscaled: 53.4354 (94.0416)\n",
      "[ep 32][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1850 (0.1886)  loss_ce: 0.1850 (0.1886)  loss_ce_unscaled: 0.1850 (0.1886)  loss_point_unscaled: 52.3526 (88.4915)\n",
      "[ep 33][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1906 (0.1936)  loss_ce: 0.1906 (0.1936)  loss_ce_unscaled: 0.1906 (0.1936)  loss_point_unscaled: 55.9573 (78.1153)\n",
      "[ep 34][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1830 (0.1890)  loss_ce: 0.1830 (0.1890)  loss_ce_unscaled: 0.1830 (0.1890)  loss_point_unscaled: 54.7851 (75.5732)\n",
      "[ep 35][lr 0.0001000][2.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1936)  loss_ce: 0.1823 (0.1936)  loss_ce_unscaled: 0.1823 (0.1936)  loss_point_unscaled: 51.4635 (64.1748)\n",
      "[ep 36][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1834 (0.1817)  loss_ce: 0.1834 (0.1817)  loss_ce_unscaled: 0.1834 (0.1817)  loss_point_unscaled: 52.0154 (77.5321)\n",
      "[ep 37][lr 0.0001000][2.73s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1873 (0.1970)  loss_ce: 0.1873 (0.1970)  loss_ce_unscaled: 0.1873 (0.1970)  loss_point_unscaled: 52.1674 (58.4315)\n",
      "[ep 38][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1752 (0.1835)  loss_ce: 0.1752 (0.1835)  loss_ce_unscaled: 0.1752 (0.1835)  loss_point_unscaled: 53.6860 (71.3889)\n",
      "[ep 39][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1907 (0.1920)  loss_ce: 0.1907 (0.1920)  loss_ce_unscaled: 0.1907 (0.1920)  loss_point_unscaled: 53.2927 (93.1831)\n",
      "[ep 40][lr 0.0001000][3.11s]\n",
      "=======================================test=======================================\n",
      "mae: 164.13736263736263 mse: 245.46895016377425 time: 2.960773468017578 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1858 (0.1956)  loss_ce: 0.1858 (0.1956)  loss_ce_unscaled: 0.1858 (0.1956)  loss_point_unscaled: 57.1080 (66.0855)\n",
      "[ep 41][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1727 (0.1947)  loss_ce: 0.1727 (0.1947)  loss_ce_unscaled: 0.1727 (0.1947)  loss_point_unscaled: 54.3477 (98.2470)\n",
      "[ep 42][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1814 (0.1877)  loss_ce: 0.1814 (0.1877)  loss_ce_unscaled: 0.1814 (0.1877)  loss_point_unscaled: 54.1192 (77.3648)\n",
      "[ep 43][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1844 (0.1902)  loss_ce: 0.1844 (0.1902)  loss_ce_unscaled: 0.1844 (0.1902)  loss_point_unscaled: 52.5662 (82.5020)\n",
      "[ep 44][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1793 (0.1991)  loss_ce: 0.1793 (0.1991)  loss_ce_unscaled: 0.1793 (0.1991)  loss_point_unscaled: 49.5478 (61.0660)\n",
      "[ep 45][lr 0.0001000][2.66s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1890 (0.1965)  loss_ce: 0.1890 (0.1965)  loss_ce_unscaled: 0.1890 (0.1965)  loss_point_unscaled: 52.1090 (63.8572)\n",
      "[ep 46][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1882 (0.1949)  loss_ce: 0.1882 (0.1949)  loss_ce_unscaled: 0.1882 (0.1949)  loss_point_unscaled: 53.0483 (83.7913)\n",
      "[ep 47][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1917 (0.1936)  loss_ce: 0.1917 (0.1936)  loss_ce_unscaled: 0.1917 (0.1936)  loss_point_unscaled: 52.5784 (90.0158)\n",
      "[ep 48][lr 0.0001000][2.59s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1944 (0.1941)  loss_ce: 0.1944 (0.1941)  loss_ce_unscaled: 0.1944 (0.1941)  loss_point_unscaled: 52.3811 (63.2716)\n",
      "[ep 49][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1893 (0.1946)  loss_ce: 0.1893 (0.1946)  loss_ce_unscaled: 0.1893 (0.1946)  loss_point_unscaled: 54.0759 (91.9561)\n",
      "[ep 50][lr 0.0001000][3.19s]\n",
      "=======================================test=======================================\n",
      "mae: 167.52747252747253 mse: 252.29657238963918 time: 4.182547569274902 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1968 (0.1963)  loss_ce: 0.1968 (0.1963)  loss_ce_unscaled: 0.1968 (0.1963)  loss_point_unscaled: 53.4763 (67.5076)\n",
      "[ep 51][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1785 (0.1921)  loss_ce: 0.1785 (0.1921)  loss_ce_unscaled: 0.1785 (0.1921)  loss_point_unscaled: 54.7051 (74.5349)\n",
      "[ep 52][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1890 (0.1926)  loss_ce: 0.1890 (0.1926)  loss_ce_unscaled: 0.1890 (0.1926)  loss_point_unscaled: 51.7042 (79.6517)\n",
      "[ep 53][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1840 (0.1917)  loss_ce: 0.1840 (0.1917)  loss_ce_unscaled: 0.1840 (0.1917)  loss_point_unscaled: 54.9087 (84.2004)\n",
      "[ep 54][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1915 (0.1913)  loss_ce: 0.1915 (0.1913)  loss_ce_unscaled: 0.1915 (0.1913)  loss_point_unscaled: 50.0957 (67.2296)\n",
      "[ep 55][lr 0.0001000][3.01s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1783 (0.1863)  loss_ce: 0.1783 (0.1863)  loss_ce_unscaled: 0.1783 (0.1863)  loss_point_unscaled: 51.8827 (72.9219)\n",
      "[ep 56][lr 0.0001000][2.82s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1739 (0.1860)  loss_ce: 0.1739 (0.1860)  loss_ce_unscaled: 0.1739 (0.1860)  loss_point_unscaled: 48.8604 (55.9473)\n",
      "[ep 57][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1974 (0.1963)  loss_ce: 0.1974 (0.1963)  loss_ce_unscaled: 0.1974 (0.1963)  loss_point_unscaled: 50.7621 (65.0264)\n",
      "[ep 58][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1928 (0.1984)  loss_ce: 0.1928 (0.1984)  loss_ce_unscaled: 0.1928 (0.1984)  loss_point_unscaled: 51.7459 (90.6951)\n",
      "[ep 59][lr 0.0001000][2.76s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1830 (0.1883)  loss_ce: 0.1830 (0.1883)  loss_ce_unscaled: 0.1830 (0.1883)  loss_point_unscaled: 54.7801 (93.3611)\n",
      "[ep 60][lr 0.0001000][2.51s]\n",
      "=======================================test=======================================\n",
      "mae: 178.76373626373626 mse: 272.40234665204053 time: 2.3388452529907227 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1939 (0.2028)  loss_ce: 0.1939 (0.2028)  loss_ce_unscaled: 0.1939 (0.2028)  loss_point_unscaled: 51.6799 (75.2400)\n",
      "[ep 61][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1915 (0.2014)  loss_ce: 0.1915 (0.2014)  loss_ce_unscaled: 0.1915 (0.2014)  loss_point_unscaled: 50.9719 (67.5037)\n",
      "[ep 62][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1904 (0.1927)  loss_ce: 0.1904 (0.1927)  loss_ce_unscaled: 0.1904 (0.1927)  loss_point_unscaled: 51.3566 (105.5725)\n",
      "[ep 63][lr 0.0001000][2.88s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1922)  loss_ce: 0.1865 (0.1922)  loss_ce_unscaled: 0.1865 (0.1922)  loss_point_unscaled: 53.4761 (84.5666)\n",
      "[ep 64][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1748 (0.1877)  loss_ce: 0.1748 (0.1877)  loss_ce_unscaled: 0.1748 (0.1877)  loss_point_unscaled: 56.9819 (81.8948)\n",
      "[ep 65][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1790 (0.1829)  loss_ce: 0.1790 (0.1829)  loss_ce_unscaled: 0.1790 (0.1829)  loss_point_unscaled: 63.6380 (95.9306)\n",
      "[ep 66][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1910 (0.1982)  loss_ce: 0.1910 (0.1982)  loss_ce_unscaled: 0.1910 (0.1982)  loss_point_unscaled: 55.0291 (74.0422)\n",
      "[ep 67][lr 0.0001000][2.84s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1891 (0.1904)  loss_ce: 0.1891 (0.1904)  loss_ce_unscaled: 0.1891 (0.1904)  loss_point_unscaled: 51.3980 (72.9818)\n",
      "[ep 68][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1943 (0.1924)  loss_ce: 0.1943 (0.1924)  loss_ce_unscaled: 0.1943 (0.1924)  loss_point_unscaled: 50.9938 (93.0755)\n",
      "[ep 69][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1849 (0.1874)  loss_ce: 0.1849 (0.1874)  loss_ce_unscaled: 0.1849 (0.1874)  loss_point_unscaled: 52.8705 (74.1405)\n",
      "[ep 70][lr 0.0001000][2.37s]\n",
      "=======================================test=======================================\n",
      "mae: 159.8186813186813 mse: 226.923802382897 time: 2.28848934173584 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1912 (0.1927)  loss_ce: 0.1912 (0.1927)  loss_ce_unscaled: 0.1912 (0.1927)  loss_point_unscaled: 47.8355 (60.9475)\n",
      "[ep 71][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1834 (0.1897)  loss_ce: 0.1834 (0.1897)  loss_ce_unscaled: 0.1834 (0.1897)  loss_point_unscaled: 54.7038 (97.9718)\n",
      "[ep 72][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1826 (0.1857)  loss_ce: 0.1826 (0.1857)  loss_ce_unscaled: 0.1826 (0.1857)  loss_point_unscaled: 50.2344 (94.0196)\n",
      "[ep 73][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1894 (0.1920)  loss_ce: 0.1894 (0.1920)  loss_ce_unscaled: 0.1894 (0.1920)  loss_point_unscaled: 50.2693 (63.0414)\n",
      "[ep 74][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1949 (0.1882)  loss_ce: 0.1949 (0.1882)  loss_ce_unscaled: 0.1949 (0.1882)  loss_point_unscaled: 52.4977 (57.0285)\n",
      "[ep 75][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1879 (0.1891)  loss_ce: 0.1879 (0.1891)  loss_ce_unscaled: 0.1879 (0.1891)  loss_point_unscaled: 55.7584 (75.8777)\n",
      "[ep 76][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1859 (0.1877)  loss_ce: 0.1859 (0.1877)  loss_ce_unscaled: 0.1859 (0.1877)  loss_point_unscaled: 56.4520 (69.1282)\n",
      "[ep 77][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1909 (0.1950)  loss_ce: 0.1909 (0.1950)  loss_ce_unscaled: 0.1909 (0.1950)  loss_point_unscaled: 50.8877 (61.9493)\n",
      "[ep 78][lr 0.0001000][3.04s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1786 (0.1877)  loss_ce: 0.1786 (0.1877)  loss_ce_unscaled: 0.1786 (0.1877)  loss_point_unscaled: 50.1135 (85.3037)\n",
      "[ep 79][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1928 (0.1909)  loss_ce: 0.1928 (0.1909)  loss_ce_unscaled: 0.1928 (0.1909)  loss_point_unscaled: 71.5286 (111.6658)\n",
      "[ep 80][lr 0.0001000][3.50s]\n",
      "=======================================test=======================================\n",
      "mae: 170.6868131868132 mse: 261.66022133402595 time: 2.311624765396118 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1893 (0.1894)  loss_ce: 0.1893 (0.1894)  loss_ce_unscaled: 0.1893 (0.1894)  loss_point_unscaled: 53.4027 (87.4012)\n",
      "[ep 81][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1965)  loss_ce: 0.1884 (0.1965)  loss_ce_unscaled: 0.1884 (0.1965)  loss_point_unscaled: 52.1831 (98.8396)\n",
      "[ep 82][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1966 (0.1958)  loss_ce: 0.1966 (0.1958)  loss_ce_unscaled: 0.1966 (0.1958)  loss_point_unscaled: 54.4845 (78.0161)\n",
      "[ep 83][lr 0.0001000][2.67s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1868 (0.1924)  loss_ce: 0.1868 (0.1924)  loss_ce_unscaled: 0.1868 (0.1924)  loss_point_unscaled: 51.9924 (95.4715)\n",
      "[ep 84][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1922 (0.1987)  loss_ce: 0.1922 (0.1987)  loss_ce_unscaled: 0.1922 (0.1987)  loss_point_unscaled: 68.0359 (140.2039)\n",
      "[ep 85][lr 0.0001000][2.91s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1883 (0.1889)  loss_ce: 0.1883 (0.1889)  loss_ce_unscaled: 0.1883 (0.1889)  loss_point_unscaled: 54.2395 (61.2213)\n",
      "[ep 86][lr 0.0001000][2.68s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1887 (0.1851)  loss_ce: 0.1887 (0.1851)  loss_ce_unscaled: 0.1887 (0.1851)  loss_point_unscaled: 50.9654 (57.8352)\n",
      "[ep 87][lr 0.0001000][3.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1948 (0.1955)  loss_ce: 0.1948 (0.1955)  loss_ce_unscaled: 0.1948 (0.1955)  loss_point_unscaled: 48.5623 (56.9571)\n",
      "[ep 88][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1963 (0.1957)  loss_ce: 0.1963 (0.1957)  loss_ce_unscaled: 0.1963 (0.1957)  loss_point_unscaled: 51.6819 (72.6023)\n",
      "[ep 89][lr 0.0001000][2.58s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1876 (0.1909)  loss_ce: 0.1876 (0.1909)  loss_ce_unscaled: 0.1876 (0.1909)  loss_point_unscaled: 57.0831 (78.6629)\n",
      "[ep 90][lr 0.0001000][3.19s]\n",
      "=======================================test=======================================\n",
      "mae: 182.64835164835165 mse: 262.90753816955043 time: 2.3386123180389404 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1857 (0.1862)  loss_ce: 0.1857 (0.1862)  loss_ce_unscaled: 0.1857 (0.1862)  loss_point_unscaled: 54.7868 (102.1796)\n",
      "[ep 91][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1875 (0.1904)  loss_ce: 0.1875 (0.1904)  loss_ce_unscaled: 0.1875 (0.1904)  loss_point_unscaled: 52.9463 (79.1999)\n",
      "[ep 92][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1825 (0.1913)  loss_ce: 0.1825 (0.1913)  loss_ce_unscaled: 0.1825 (0.1913)  loss_point_unscaled: 56.5938 (75.3671)\n",
      "[ep 93][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1988 (0.2002)  loss_ce: 0.1988 (0.2002)  loss_ce_unscaled: 0.1988 (0.2002)  loss_point_unscaled: 63.0650 (100.6605)\n",
      "[ep 94][lr 0.0001000][2.55s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1828 (0.1876)  loss_ce: 0.1828 (0.1876)  loss_ce_unscaled: 0.1828 (0.1876)  loss_point_unscaled: 52.9876 (104.4357)\n",
      "[ep 95][lr 0.0001000][3.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1905 (0.1917)  loss_ce: 0.1905 (0.1917)  loss_ce_unscaled: 0.1905 (0.1917)  loss_point_unscaled: 49.1028 (78.7672)\n",
      "[ep 96][lr 0.0001000][2.92s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1825 (0.1940)  loss_ce: 0.1825 (0.1940)  loss_ce_unscaled: 0.1825 (0.1940)  loss_point_unscaled: 52.2104 (71.2302)\n",
      "[ep 97][lr 0.0001000][3.04s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1896 (0.1920)  loss_ce: 0.1896 (0.1920)  loss_ce_unscaled: 0.1896 (0.1920)  loss_point_unscaled: 51.3736 (83.6505)\n",
      "[ep 98][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1862 (0.1897)  loss_ce: 0.1862 (0.1897)  loss_ce_unscaled: 0.1862 (0.1897)  loss_point_unscaled: 50.3898 (116.0819)\n",
      "[ep 99][lr 0.0001000][3.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2024 (0.1965)  loss_ce: 0.2024 (0.1965)  loss_ce_unscaled: 0.2024 (0.1965)  loss_point_unscaled: 51.7152 (98.9001)\n",
      "[ep 100][lr 0.0001000][3.33s]\n",
      "=======================================test=======================================\n",
      "mae: 156.15934065934067 mse: 241.70580837497016 time: 4.127677917480469 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1801 (0.1845)  loss_ce: 0.1801 (0.1845)  loss_ce_unscaled: 0.1801 (0.1845)  loss_point_unscaled: 53.3064 (91.6738)\n",
      "[ep 101][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1891 (0.1938)  loss_ce: 0.1891 (0.1938)  loss_ce_unscaled: 0.1891 (0.1938)  loss_point_unscaled: 55.0518 (103.8647)\n",
      "[ep 102][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1957 (0.1916)  loss_ce: 0.1957 (0.1916)  loss_ce_unscaled: 0.1957 (0.1916)  loss_point_unscaled: 56.9183 (78.5451)\n",
      "[ep 103][lr 0.0001000][3.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1866 (0.1871)  loss_ce: 0.1866 (0.1871)  loss_ce_unscaled: 0.1866 (0.1871)  loss_point_unscaled: 52.8192 (83.7799)\n",
      "[ep 104][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1854 (0.1865)  loss_ce: 0.1854 (0.1865)  loss_ce_unscaled: 0.1854 (0.1865)  loss_point_unscaled: 51.8514 (76.7717)\n",
      "[ep 105][lr 0.0001000][2.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1837 (0.1883)  loss_ce: 0.1837 (0.1883)  loss_ce_unscaled: 0.1837 (0.1883)  loss_point_unscaled: 50.8077 (87.4181)\n",
      "[ep 106][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1857)  loss_ce: 0.1839 (0.1857)  loss_ce_unscaled: 0.1839 (0.1857)  loss_point_unscaled: 51.4154 (76.5441)\n",
      "[ep 107][lr 0.0001000][2.81s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2069 (0.2008)  loss_ce: 0.2069 (0.2008)  loss_ce_unscaled: 0.2069 (0.2008)  loss_point_unscaled: 50.5720 (72.8365)\n",
      "[ep 108][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1913)  loss_ce: 0.1839 (0.1913)  loss_ce_unscaled: 0.1839 (0.1913)  loss_point_unscaled: 51.6809 (73.3045)\n",
      "[ep 109][lr 0.0001000][2.70s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1866 (0.1859)  loss_ce: 0.1866 (0.1859)  loss_ce_unscaled: 0.1866 (0.1859)  loss_point_unscaled: 51.6560 (86.5107)\n",
      "[ep 110][lr 0.0001000][2.97s]\n",
      "=======================================test=======================================\n",
      "mae: 161.46153846153845 mse: 253.48691363582338 time: 2.3161346912384033 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1862 (0.1899)  loss_ce: 0.1862 (0.1899)  loss_ce_unscaled: 0.1862 (0.1899)  loss_point_unscaled: 51.7644 (80.3372)\n",
      "[ep 111][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1986 (0.1908)  loss_ce: 0.1986 (0.1908)  loss_ce_unscaled: 0.1986 (0.1908)  loss_point_unscaled: 50.2014 (66.3395)\n",
      "[ep 112][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1947 (0.1991)  loss_ce: 0.1947 (0.1991)  loss_ce_unscaled: 0.1947 (0.1991)  loss_point_unscaled: 54.2034 (89.1064)\n",
      "[ep 113][lr 0.0001000][2.55s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1891 (0.1945)  loss_ce: 0.1891 (0.1945)  loss_ce_unscaled: 0.1891 (0.1945)  loss_point_unscaled: 53.0259 (62.9806)\n",
      "[ep 114][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1945 (0.1907)  loss_ce: 0.1945 (0.1907)  loss_ce_unscaled: 0.1945 (0.1907)  loss_point_unscaled: 75.2306 (121.4262)\n",
      "[ep 115][lr 0.0001000][2.51s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1778 (0.1864)  loss_ce: 0.1778 (0.1864)  loss_ce_unscaled: 0.1778 (0.1864)  loss_point_unscaled: 48.2776 (53.3014)\n",
      "[ep 116][lr 0.0001000][3.03s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1871 (0.1897)  loss_ce: 0.1871 (0.1897)  loss_ce_unscaled: 0.1871 (0.1897)  loss_point_unscaled: 53.0314 (67.9029)\n",
      "[ep 117][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1843 (0.1897)  loss_ce: 0.1843 (0.1897)  loss_ce_unscaled: 0.1843 (0.1897)  loss_point_unscaled: 50.9876 (85.9351)\n",
      "[ep 118][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1882 (0.1947)  loss_ce: 0.1882 (0.1947)  loss_ce_unscaled: 0.1882 (0.1947)  loss_point_unscaled: 53.0642 (81.1126)\n",
      "[ep 119][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1833 (0.1910)  loss_ce: 0.1833 (0.1910)  loss_ce_unscaled: 0.1833 (0.1910)  loss_point_unscaled: 52.0866 (66.4142)\n",
      "[ep 120][lr 0.0001000][2.60s]\n",
      "=======================================test=======================================\n",
      "mae: 171.3131868131868 mse: 248.88905909313036 time: 4.1161208152771 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1957 (0.1918)  loss_ce: 0.1957 (0.1918)  loss_ce_unscaled: 0.1957 (0.1918)  loss_point_unscaled: 50.8781 (63.7783)\n",
      "[ep 121][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1864 (0.1914)  loss_ce: 0.1864 (0.1914)  loss_ce_unscaled: 0.1864 (0.1914)  loss_point_unscaled: 54.4097 (102.4587)\n",
      "[ep 122][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1805 (0.1822)  loss_ce: 0.1805 (0.1822)  loss_ce_unscaled: 0.1805 (0.1822)  loss_point_unscaled: 55.1474 (87.0693)\n",
      "[ep 123][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1962 (0.1892)  loss_ce: 0.1962 (0.1892)  loss_ce_unscaled: 0.1962 (0.1892)  loss_point_unscaled: 54.0515 (76.8999)\n",
      "[ep 124][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2012 (0.2048)  loss_ce: 0.2012 (0.2048)  loss_ce_unscaled: 0.2012 (0.2048)  loss_point_unscaled: 53.5345 (75.4833)\n",
      "[ep 125][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1910)  loss_ce: 0.1851 (0.1910)  loss_ce_unscaled: 0.1851 (0.1910)  loss_point_unscaled: 52.6249 (77.7874)\n",
      "[ep 126][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1795 (0.1849)  loss_ce: 0.1795 (0.1849)  loss_ce_unscaled: 0.1795 (0.1849)  loss_point_unscaled: 53.6232 (65.1347)\n",
      "[ep 127][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1840 (0.1817)  loss_ce: 0.1840 (0.1817)  loss_ce_unscaled: 0.1840 (0.1817)  loss_point_unscaled: 52.9190 (78.7644)\n",
      "[ep 128][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1938 (0.2004)  loss_ce: 0.1938 (0.2004)  loss_ce_unscaled: 0.1938 (0.2004)  loss_point_unscaled: 50.6178 (95.4337)\n",
      "[ep 129][lr 0.0001000][3.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1778 (0.1856)  loss_ce: 0.1778 (0.1856)  loss_ce_unscaled: 0.1778 (0.1856)  loss_point_unscaled: 52.1390 (80.5417)\n",
      "[ep 130][lr 0.0001000][3.23s]\n",
      "=======================================test=======================================\n",
      "mae: 165.5164835164835 mse: 255.15125873717503 time: 4.120264053344727 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1829 (0.1894)  loss_ce: 0.1829 (0.1894)  loss_ce_unscaled: 0.1829 (0.1894)  loss_point_unscaled: 49.3186 (65.3509)\n",
      "[ep 131][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1913 (0.1920)  loss_ce: 0.1913 (0.1920)  loss_ce_unscaled: 0.1913 (0.1920)  loss_point_unscaled: 63.1255 (97.6755)\n",
      "[ep 132][lr 0.0001000][3.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1881 (0.1943)  loss_ce: 0.1881 (0.1943)  loss_ce_unscaled: 0.1881 (0.1943)  loss_point_unscaled: 55.9932 (74.5281)\n",
      "[ep 133][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1860 (0.1853)  loss_ce: 0.1860 (0.1853)  loss_ce_unscaled: 0.1860 (0.1853)  loss_point_unscaled: 53.3021 (65.7691)\n",
      "[ep 134][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1964 (0.1955)  loss_ce: 0.1964 (0.1955)  loss_ce_unscaled: 0.1964 (0.1955)  loss_point_unscaled: 49.6638 (62.6991)\n",
      "[ep 135][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1860 (0.1896)  loss_ce: 0.1860 (0.1896)  loss_ce_unscaled: 0.1860 (0.1896)  loss_point_unscaled: 52.8339 (101.8280)\n",
      "[ep 136][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1859 (0.1947)  loss_ce: 0.1859 (0.1947)  loss_ce_unscaled: 0.1859 (0.1947)  loss_point_unscaled: 49.4484 (63.0205)\n",
      "[ep 137][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1819 (0.1817)  loss_ce: 0.1819 (0.1817)  loss_ce_unscaled: 0.1819 (0.1817)  loss_point_unscaled: 54.9764 (63.0284)\n",
      "[ep 138][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1885 (0.1851)  loss_ce: 0.1885 (0.1851)  loss_ce_unscaled: 0.1885 (0.1851)  loss_point_unscaled: 52.1705 (82.2018)\n",
      "[ep 139][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1880 (0.1913)  loss_ce: 0.1880 (0.1913)  loss_ce_unscaled: 0.1880 (0.1913)  loss_point_unscaled: 51.4491 (77.7740)\n",
      "[ep 140][lr 0.0001000][2.35s]\n",
      "=======================================test=======================================\n",
      "mae: 155.5 mse: 245.30616662574013 time: 3.877135992050171 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1848)  loss_ce: 0.1839 (0.1848)  loss_ce_unscaled: 0.1839 (0.1848)  loss_point_unscaled: 54.0511 (73.1477)\n",
      "[ep 141][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1836 (0.1811)  loss_ce: 0.1836 (0.1811)  loss_ce_unscaled: 0.1836 (0.1811)  loss_point_unscaled: 56.6691 (76.9907)\n",
      "[ep 142][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1976 (0.1972)  loss_ce: 0.1976 (0.1972)  loss_ce_unscaled: 0.1976 (0.1972)  loss_point_unscaled: 52.8554 (68.9158)\n",
      "[ep 143][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1946 (0.1917)  loss_ce: 0.1946 (0.1917)  loss_ce_unscaled: 0.1946 (0.1917)  loss_point_unscaled: 54.6464 (83.7654)\n",
      "[ep 144][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1891 (0.1965)  loss_ce: 0.1891 (0.1965)  loss_ce_unscaled: 0.1891 (0.1965)  loss_point_unscaled: 55.1135 (87.3387)\n",
      "[ep 145][lr 0.0001000][2.54s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1837)  loss_ce: 0.1781 (0.1837)  loss_ce_unscaled: 0.1781 (0.1837)  loss_point_unscaled: 55.2969 (69.7483)\n",
      "[ep 146][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1736 (0.1860)  loss_ce: 0.1736 (0.1860)  loss_ce_unscaled: 0.1736 (0.1860)  loss_point_unscaled: 51.9350 (63.5465)\n",
      "[ep 147][lr 0.0001000][3.03s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1863 (0.1943)  loss_ce: 0.1863 (0.1943)  loss_ce_unscaled: 0.1863 (0.1943)  loss_point_unscaled: 51.4672 (66.0592)\n",
      "[ep 148][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1857 (0.1963)  loss_ce: 0.1857 (0.1963)  loss_ce_unscaled: 0.1857 (0.1963)  loss_point_unscaled: 48.9247 (99.9824)\n",
      "[ep 149][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1915)  loss_ce: 0.1865 (0.1915)  loss_ce_unscaled: 0.1865 (0.1915)  loss_point_unscaled: 49.4839 (82.9332)\n",
      "[ep 150][lr 0.0001000][3.29s]\n",
      "=======================================test=======================================\n",
      "mae: 167.96153846153845 mse: 242.5232400781871 time: 2.6180531978607178 best mae: 148.63186813186815\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1799 (0.1936)  loss_ce: 0.1799 (0.1936)  loss_ce_unscaled: 0.1799 (0.1936)  loss_point_unscaled: 54.0755 (77.5840)\n",
      "[ep 151][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1862 (0.1940)  loss_ce: 0.1862 (0.1940)  loss_ce_unscaled: 0.1862 (0.1940)  loss_point_unscaled: 52.8870 (66.4249)\n",
      "[ep 152][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1728 (0.1845)  loss_ce: 0.1728 (0.1845)  loss_ce_unscaled: 0.1728 (0.1845)  loss_point_unscaled: 54.1819 (63.8317)\n",
      "[ep 153][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1975 (0.1981)  loss_ce: 0.1975 (0.1981)  loss_ce_unscaled: 0.1975 (0.1981)  loss_point_unscaled: 47.8933 (74.4731)\n",
      "[ep 154][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1871 (0.1875)  loss_ce: 0.1871 (0.1875)  loss_ce_unscaled: 0.1871 (0.1875)  loss_point_unscaled: 57.3034 (88.4699)\n",
      "[ep 155][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1930 (0.1917)  loss_ce: 0.1930 (0.1917)  loss_ce_unscaled: 0.1930 (0.1917)  loss_point_unscaled: 54.7213 (91.8599)\n",
      "[ep 156][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1983 (0.2027)  loss_ce: 0.1983 (0.2027)  loss_ce_unscaled: 0.1983 (0.2027)  loss_point_unscaled: 57.5815 (69.4327)\n",
      "[ep 157][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1800 (0.1883)  loss_ce: 0.1800 (0.1883)  loss_ce_unscaled: 0.1800 (0.1883)  loss_point_unscaled: 55.0100 (81.2720)\n",
      "[ep 158][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1886 (0.1945)  loss_ce: 0.1886 (0.1945)  loss_ce_unscaled: 0.1886 (0.1945)  loss_point_unscaled: 54.5248 (86.3223)\n",
      "[ep 159][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1831 (0.1918)  loss_ce: 0.1831 (0.1918)  loss_ce_unscaled: 0.1831 (0.1918)  loss_point_unscaled: 50.4018 (71.8393)\n",
      "[ep 160][lr 0.0001000][3.17s]\n",
      "=======================================test=======================================\n",
      "mae: 139.3901098901099 mse: 217.94982535810018 time: 2.223914384841919 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1922 (0.1899)  loss_ce: 0.1922 (0.1899)  loss_ce_unscaled: 0.1922 (0.1899)  loss_point_unscaled: 55.8833 (79.9350)\n",
      "[ep 161][lr 0.0001000][2.74s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1772 (0.1852)  loss_ce: 0.1772 (0.1852)  loss_ce_unscaled: 0.1772 (0.1852)  loss_point_unscaled: 54.9889 (86.6256)\n",
      "[ep 162][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1890 (0.1910)  loss_ce: 0.1890 (0.1910)  loss_ce_unscaled: 0.1890 (0.1910)  loss_point_unscaled: 50.3868 (95.1286)\n",
      "[ep 163][lr 0.0001000][2.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1830 (0.1861)  loss_ce: 0.1830 (0.1861)  loss_ce_unscaled: 0.1830 (0.1861)  loss_point_unscaled: 50.3633 (74.4431)\n",
      "[ep 164][lr 0.0001000][3.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1998 (0.1895)  loss_ce: 0.1998 (0.1895)  loss_ce_unscaled: 0.1998 (0.1895)  loss_point_unscaled: 51.9654 (73.0248)\n",
      "[ep 165][lr 0.0001000][2.55s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1748 (0.1789)  loss_ce: 0.1748 (0.1789)  loss_ce_unscaled: 0.1748 (0.1789)  loss_point_unscaled: 52.1574 (83.9739)\n",
      "[ep 166][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1784 (0.1870)  loss_ce: 0.1784 (0.1870)  loss_ce_unscaled: 0.1784 (0.1870)  loss_point_unscaled: 52.4470 (67.7182)\n",
      "[ep 167][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2010 (0.1964)  loss_ce: 0.2010 (0.1964)  loss_ce_unscaled: 0.2010 (0.1964)  loss_point_unscaled: 51.7967 (92.2368)\n",
      "[ep 168][lr 0.0001000][3.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1929 (0.1889)  loss_ce: 0.1929 (0.1889)  loss_ce_unscaled: 0.1929 (0.1889)  loss_point_unscaled: 55.7921 (88.3316)\n",
      "[ep 169][lr 0.0001000][2.63s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1908 (0.1949)  loss_ce: 0.1908 (0.1949)  loss_ce_unscaled: 0.1908 (0.1949)  loss_point_unscaled: 51.2961 (75.5348)\n",
      "[ep 170][lr 0.0001000][3.28s]\n",
      "=======================================test=======================================\n",
      "mae: 158.37362637362637 mse: 231.4611732564983 time: 4.22616171836853 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1890 (0.1970)  loss_ce: 0.1890 (0.1970)  loss_ce_unscaled: 0.1890 (0.1970)  loss_point_unscaled: 49.8411 (79.3078)\n",
      "[ep 171][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1802 (0.1824)  loss_ce: 0.1802 (0.1824)  loss_ce_unscaled: 0.1802 (0.1824)  loss_point_unscaled: 53.6707 (70.7598)\n",
      "[ep 172][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1848 (0.1844)  loss_ce: 0.1848 (0.1844)  loss_ce_unscaled: 0.1848 (0.1844)  loss_point_unscaled: 53.5983 (73.3514)\n",
      "[ep 173][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1852 (0.1884)  loss_ce: 0.1852 (0.1884)  loss_ce_unscaled: 0.1852 (0.1884)  loss_point_unscaled: 49.5970 (81.0044)\n",
      "[ep 174][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1952 (0.1952)  loss_ce: 0.1952 (0.1952)  loss_ce_unscaled: 0.1952 (0.1952)  loss_point_unscaled: 52.6978 (83.7762)\n",
      "[ep 175][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1917 (0.1941)  loss_ce: 0.1917 (0.1941)  loss_ce_unscaled: 0.1917 (0.1941)  loss_point_unscaled: 55.6511 (84.9330)\n",
      "[ep 176][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1820 (0.1865)  loss_ce: 0.1820 (0.1865)  loss_ce_unscaled: 0.1820 (0.1865)  loss_point_unscaled: 62.2466 (78.8730)\n",
      "[ep 177][lr 0.0001000][3.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1977 (0.1931)  loss_ce: 0.1977 (0.1931)  loss_ce_unscaled: 0.1977 (0.1931)  loss_point_unscaled: 54.4461 (79.8617)\n",
      "[ep 178][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1959 (0.1983)  loss_ce: 0.1959 (0.1983)  loss_ce_unscaled: 0.1959 (0.1983)  loss_point_unscaled: 54.5299 (74.1977)\n",
      "[ep 179][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1826 (0.1923)  loss_ce: 0.1826 (0.1923)  loss_ce_unscaled: 0.1826 (0.1923)  loss_point_unscaled: 53.2579 (86.0621)\n",
      "[ep 180][lr 0.0001000][3.17s]\n",
      "=======================================test=======================================\n",
      "mae: 154.35164835164835 mse: 238.2360773926329 time: 2.317500352859497 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1849 (0.1934)  loss_ce: 0.1849 (0.1934)  loss_ce_unscaled: 0.1849 (0.1934)  loss_point_unscaled: 58.1693 (92.4295)\n",
      "[ep 181][lr 0.0001000][2.97s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1780 (0.1851)  loss_ce: 0.1780 (0.1851)  loss_ce_unscaled: 0.1780 (0.1851)  loss_point_unscaled: 53.5082 (72.0015)\n",
      "[ep 182][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1908 (0.1896)  loss_ce: 0.1908 (0.1896)  loss_ce_unscaled: 0.1908 (0.1896)  loss_point_unscaled: 52.7228 (82.6375)\n",
      "[ep 183][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1789 (0.1885)  loss_ce: 0.1789 (0.1885)  loss_ce_unscaled: 0.1789 (0.1885)  loss_point_unscaled: 53.3367 (107.8724)\n",
      "[ep 184][lr 0.0001000][3.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1862 (0.1874)  loss_ce: 0.1862 (0.1874)  loss_ce_unscaled: 0.1862 (0.1874)  loss_point_unscaled: 53.0993 (72.7133)\n",
      "[ep 185][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1902 (0.1937)  loss_ce: 0.1902 (0.1937)  loss_ce_unscaled: 0.1902 (0.1937)  loss_point_unscaled: 54.9012 (80.5621)\n",
      "[ep 186][lr 0.0001000][2.51s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1946 (0.1937)  loss_ce: 0.1946 (0.1937)  loss_ce_unscaled: 0.1946 (0.1937)  loss_point_unscaled: 64.4519 (78.3775)\n",
      "[ep 187][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1863)  loss_ce: 0.1851 (0.1863)  loss_ce_unscaled: 0.1851 (0.1863)  loss_point_unscaled: 55.3574 (81.3109)\n",
      "[ep 188][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1835 (0.1902)  loss_ce: 0.1835 (0.1902)  loss_ce_unscaled: 0.1835 (0.1902)  loss_point_unscaled: 52.1387 (86.4091)\n",
      "[ep 189][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1909 (0.1945)  loss_ce: 0.1909 (0.1945)  loss_ce_unscaled: 0.1909 (0.1945)  loss_point_unscaled: 52.8430 (106.4037)\n",
      "[ep 190][lr 0.0001000][2.89s]\n",
      "=======================================test=======================================\n",
      "mae: 146.53846153846155 mse: 222.64479175741636 time: 2.3058221340179443 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1785 (0.1788)  loss_ce: 0.1785 (0.1788)  loss_ce_unscaled: 0.1785 (0.1788)  loss_point_unscaled: 51.3681 (67.0857)\n",
      "[ep 191][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1726 (0.1814)  loss_ce: 0.1726 (0.1814)  loss_ce_unscaled: 0.1726 (0.1814)  loss_point_unscaled: 51.1943 (68.4242)\n",
      "[ep 192][lr 0.0001000][2.57s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1953 (0.1921)  loss_ce: 0.1953 (0.1921)  loss_ce_unscaled: 0.1953 (0.1921)  loss_point_unscaled: 51.3536 (87.7477)\n",
      "[ep 193][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1828 (0.1892)  loss_ce: 0.1828 (0.1892)  loss_ce_unscaled: 0.1828 (0.1892)  loss_point_unscaled: 53.9472 (109.7550)\n",
      "[ep 194][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1877 (0.1877)  loss_ce: 0.1877 (0.1877)  loss_ce_unscaled: 0.1877 (0.1877)  loss_point_unscaled: 53.6434 (69.5733)\n",
      "[ep 195][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1872)  loss_ce: 0.1851 (0.1872)  loss_ce_unscaled: 0.1851 (0.1872)  loss_point_unscaled: 52.2987 (58.3681)\n",
      "[ep 196][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1828 (0.1842)  loss_ce: 0.1828 (0.1842)  loss_ce_unscaled: 0.1828 (0.1842)  loss_point_unscaled: 54.6628 (71.2193)\n",
      "[ep 197][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1774 (0.1843)  loss_ce: 0.1774 (0.1843)  loss_ce_unscaled: 0.1774 (0.1843)  loss_point_unscaled: 51.9055 (83.0835)\n",
      "[ep 198][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1899)  loss_ce: 0.1755 (0.1899)  loss_ce_unscaled: 0.1755 (0.1899)  loss_point_unscaled: 59.2216 (81.4240)\n",
      "[ep 199][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1908 (0.1917)  loss_ce: 0.1908 (0.1917)  loss_ce_unscaled: 0.1908 (0.1917)  loss_point_unscaled: 58.4694 (78.9135)\n",
      "[ep 200][lr 0.0001000][3.10s]\n",
      "=======================================test=======================================\n",
      "mae: 180.14285714285714 mse: 262.88509168184385 time: 2.4332082271575928 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1794 (0.1854)  loss_ce: 0.1794 (0.1854)  loss_ce_unscaled: 0.1794 (0.1854)  loss_point_unscaled: 51.3794 (88.0038)\n",
      "[ep 201][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1911)  loss_ce: 0.1872 (0.1911)  loss_ce_unscaled: 0.1872 (0.1911)  loss_point_unscaled: 52.3259 (103.5969)\n",
      "[ep 202][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1892)  loss_ce: 0.1865 (0.1892)  loss_ce_unscaled: 0.1865 (0.1892)  loss_point_unscaled: 68.3326 (102.5351)\n",
      "[ep 203][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1847 (0.1914)  loss_ce: 0.1847 (0.1914)  loss_ce_unscaled: 0.1847 (0.1914)  loss_point_unscaled: 51.6167 (58.1745)\n",
      "[ep 204][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1943)  loss_ce: 0.1872 (0.1943)  loss_ce_unscaled: 0.1872 (0.1943)  loss_point_unscaled: 52.9051 (93.4826)\n",
      "[ep 205][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1984 (0.2010)  loss_ce: 0.1984 (0.2010)  loss_ce_unscaled: 0.1984 (0.2010)  loss_point_unscaled: 49.4952 (90.8813)\n",
      "[ep 206][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1942 (0.1932)  loss_ce: 0.1942 (0.1932)  loss_ce_unscaled: 0.1942 (0.1932)  loss_point_unscaled: 55.2870 (86.7809)\n",
      "[ep 207][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1849 (0.1889)  loss_ce: 0.1849 (0.1889)  loss_ce_unscaled: 0.1849 (0.1889)  loss_point_unscaled: 52.3898 (107.0827)\n",
      "[ep 208][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1900 (0.1949)  loss_ce: 0.1900 (0.1949)  loss_ce_unscaled: 0.1900 (0.1949)  loss_point_unscaled: 51.3427 (70.7879)\n",
      "[ep 209][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1948 (0.2013)  loss_ce: 0.1948 (0.2013)  loss_ce_unscaled: 0.1948 (0.2013)  loss_point_unscaled: 51.7587 (69.5708)\n",
      "[ep 210][lr 0.0001000][3.23s]\n",
      "=======================================test=======================================\n",
      "mae: 185.2032967032967 mse: 294.70496514907325 time: 2.334848165512085 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1818 (0.1893)  loss_ce: 0.1818 (0.1893)  loss_ce_unscaled: 0.1818 (0.1893)  loss_point_unscaled: 51.2385 (73.8730)\n",
      "[ep 211][lr 0.0001000][2.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1834 (0.1925)  loss_ce: 0.1834 (0.1925)  loss_ce_unscaled: 0.1834 (0.1925)  loss_point_unscaled: 52.7916 (79.8753)\n",
      "[ep 212][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1797 (0.1910)  loss_ce: 0.1797 (0.1910)  loss_ce_unscaled: 0.1797 (0.1910)  loss_point_unscaled: 53.6238 (85.8161)\n",
      "[ep 213][lr 0.0001000][3.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1916 (0.1945)  loss_ce: 0.1916 (0.1945)  loss_ce_unscaled: 0.1916 (0.1945)  loss_point_unscaled: 52.5529 (61.5127)\n",
      "[ep 214][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1854 (0.1877)  loss_ce: 0.1854 (0.1877)  loss_ce_unscaled: 0.1854 (0.1877)  loss_point_unscaled: 52.0490 (58.9650)\n",
      "[ep 215][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1799 (0.1815)  loss_ce: 0.1799 (0.1815)  loss_ce_unscaled: 0.1799 (0.1815)  loss_point_unscaled: 50.2285 (79.0072)\n",
      "[ep 216][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1867 (0.1877)  loss_ce: 0.1867 (0.1877)  loss_ce_unscaled: 0.1867 (0.1877)  loss_point_unscaled: 51.8858 (66.2620)\n",
      "[ep 217][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1753 (0.1872)  loss_ce: 0.1753 (0.1872)  loss_ce_unscaled: 0.1753 (0.1872)  loss_point_unscaled: 49.9519 (74.6279)\n",
      "[ep 218][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1858 (0.1866)  loss_ce: 0.1858 (0.1866)  loss_ce_unscaled: 0.1858 (0.1866)  loss_point_unscaled: 55.7292 (98.5015)\n",
      "[ep 219][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1826 (0.1882)  loss_ce: 0.1826 (0.1882)  loss_ce_unscaled: 0.1826 (0.1882)  loss_point_unscaled: 53.0642 (72.9053)\n",
      "[ep 220][lr 0.0001000][2.41s]\n",
      "=======================================test=======================================\n",
      "mae: 160.0989010989011 mse: 239.57996669079083 time: 2.323305130004883 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1859)  loss_ce: 0.1832 (0.1859)  loss_ce_unscaled: 0.1832 (0.1859)  loss_point_unscaled: 51.8728 (73.3213)\n",
      "[ep 221][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1813 (0.1853)  loss_ce: 0.1813 (0.1853)  loss_ce_unscaled: 0.1813 (0.1853)  loss_point_unscaled: 69.6313 (138.8920)\n",
      "[ep 222][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1776 (0.1870)  loss_ce: 0.1776 (0.1870)  loss_ce_unscaled: 0.1776 (0.1870)  loss_point_unscaled: 52.5191 (84.8190)\n",
      "[ep 223][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1855 (0.1938)  loss_ce: 0.1855 (0.1938)  loss_ce_unscaled: 0.1855 (0.1938)  loss_point_unscaled: 52.2039 (75.6653)\n",
      "[ep 224][lr 0.0001000][3.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1833 (0.1871)  loss_ce: 0.1833 (0.1871)  loss_ce_unscaled: 0.1833 (0.1871)  loss_point_unscaled: 51.7998 (83.7111)\n",
      "[ep 225][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1875 (0.1942)  loss_ce: 0.1875 (0.1942)  loss_ce_unscaled: 0.1875 (0.1942)  loss_point_unscaled: 51.6377 (56.4500)\n",
      "[ep 226][lr 0.0001000][2.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1850 (0.1983)  loss_ce: 0.1850 (0.1983)  loss_ce_unscaled: 0.1850 (0.1983)  loss_point_unscaled: 47.2518 (72.0365)\n",
      "[ep 227][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1947 (0.1904)  loss_ce: 0.1947 (0.1904)  loss_ce_unscaled: 0.1947 (0.1904)  loss_point_unscaled: 52.6198 (104.2960)\n",
      "[ep 228][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1727 (0.1871)  loss_ce: 0.1727 (0.1871)  loss_ce_unscaled: 0.1727 (0.1871)  loss_point_unscaled: 49.8842 (62.2252)\n",
      "[ep 229][lr 0.0001000][2.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1812 (0.1943)  loss_ce: 0.1812 (0.1943)  loss_ce_unscaled: 0.1812 (0.1943)  loss_point_unscaled: 51.9629 (83.6928)\n",
      "[ep 230][lr 0.0001000][3.06s]\n",
      "=======================================test=======================================\n",
      "mae: 148.4945054945055 mse: 217.47797665435075 time: 2.701542615890503 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.2001 (0.2049)  loss_ce: 0.2001 (0.2049)  loss_ce_unscaled: 0.2001 (0.2049)  loss_point_unscaled: 51.9356 (60.8380)\n",
      "[ep 231][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1905 (0.1873)  loss_ce: 0.1905 (0.1873)  loss_ce_unscaled: 0.1905 (0.1873)  loss_point_unscaled: 51.2450 (100.5514)\n",
      "[ep 232][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1935 (0.1915)  loss_ce: 0.1935 (0.1915)  loss_ce_unscaled: 0.1935 (0.1915)  loss_point_unscaled: 56.5455 (85.8890)\n",
      "[ep 233][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1948 (0.1951)  loss_ce: 0.1948 (0.1951)  loss_ce_unscaled: 0.1948 (0.1951)  loss_point_unscaled: 52.2072 (88.9056)\n",
      "[ep 234][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1824 (0.1904)  loss_ce: 0.1824 (0.1904)  loss_ce_unscaled: 0.1824 (0.1904)  loss_point_unscaled: 57.0712 (105.0233)\n",
      "[ep 235][lr 0.0001000][3.01s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1934 (0.1917)  loss_ce: 0.1934 (0.1917)  loss_ce_unscaled: 0.1934 (0.1917)  loss_point_unscaled: 50.2389 (81.3033)\n",
      "[ep 236][lr 0.0001000][2.71s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1924)  loss_ce: 0.1884 (0.1924)  loss_ce_unscaled: 0.1884 (0.1924)  loss_point_unscaled: 51.1936 (55.9104)\n",
      "[ep 237][lr 0.0001000][2.70s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1843 (0.1933)  loss_ce: 0.1843 (0.1933)  loss_ce_unscaled: 0.1843 (0.1933)  loss_point_unscaled: 51.1409 (92.6272)\n",
      "[ep 238][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1796 (0.1833)  loss_ce: 0.1796 (0.1833)  loss_ce_unscaled: 0.1796 (0.1833)  loss_point_unscaled: 58.8536 (100.5622)\n",
      "[ep 239][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1820 (0.1910)  loss_ce: 0.1820 (0.1910)  loss_ce_unscaled: 0.1820 (0.1910)  loss_point_unscaled: 51.8658 (82.6955)\n",
      "[ep 240][lr 0.0001000][3.17s]\n",
      "=======================================test=======================================\n",
      "mae: 165.43956043956044 mse: 249.5797346661863 time: 2.3965282440185547 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1883 (0.1939)  loss_ce: 0.1883 (0.1939)  loss_ce_unscaled: 0.1883 (0.1939)  loss_point_unscaled: 53.3975 (84.6451)\n",
      "[ep 241][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1846 (0.1857)  loss_ce: 0.1846 (0.1857)  loss_ce_unscaled: 0.1846 (0.1857)  loss_point_unscaled: 59.2407 (84.5641)\n",
      "[ep 242][lr 0.0001000][2.76s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1847 (0.1869)  loss_ce: 0.1847 (0.1869)  loss_ce_unscaled: 0.1847 (0.1869)  loss_point_unscaled: 48.3568 (85.4751)\n",
      "[ep 243][lr 0.0001000][2.73s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1871)  loss_ce: 0.1839 (0.1871)  loss_ce_unscaled: 0.1839 (0.1871)  loss_point_unscaled: 55.9865 (70.6938)\n",
      "[ep 244][lr 0.0001000][2.64s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1908)  loss_ce: 0.1832 (0.1908)  loss_ce_unscaled: 0.1832 (0.1908)  loss_point_unscaled: 51.8780 (88.9989)\n",
      "[ep 245][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1976 (0.1984)  loss_ce: 0.1976 (0.1984)  loss_ce_unscaled: 0.1976 (0.1984)  loss_point_unscaled: 55.2238 (98.5141)\n",
      "[ep 246][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1888 (0.1948)  loss_ce: 0.1888 (0.1948)  loss_ce_unscaled: 0.1888 (0.1948)  loss_point_unscaled: 52.1300 (90.8027)\n",
      "[ep 247][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1889 (0.1911)  loss_ce: 0.1889 (0.1911)  loss_ce_unscaled: 0.1889 (0.1911)  loss_point_unscaled: 52.1989 (107.8127)\n",
      "[ep 248][lr 0.0001000][2.60s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1928 (0.1966)  loss_ce: 0.1928 (0.1966)  loss_ce_unscaled: 0.1928 (0.1966)  loss_point_unscaled: 54.8268 (90.7674)\n",
      "[ep 249][lr 0.0001000][2.60s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1769 (0.1908)  loss_ce: 0.1769 (0.1908)  loss_ce_unscaled: 0.1769 (0.1908)  loss_point_unscaled: 51.2414 (81.7239)\n",
      "[ep 250][lr 0.0001000][3.30s]\n",
      "=======================================test=======================================\n",
      "mae: 182.3901098901099 mse: 303.9564836552166 time: 4.269006252288818 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1856 (0.1852)  loss_ce: 0.1856 (0.1852)  loss_ce_unscaled: 0.1856 (0.1852)  loss_point_unscaled: 49.9056 (62.5624)\n",
      "[ep 251][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1881 (0.1877)  loss_ce: 0.1881 (0.1877)  loss_ce_unscaled: 0.1881 (0.1877)  loss_point_unscaled: 56.4915 (79.2160)\n",
      "[ep 252][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1922 (0.1968)  loss_ce: 0.1922 (0.1968)  loss_ce_unscaled: 0.1922 (0.1968)  loss_point_unscaled: 52.2952 (97.5540)\n",
      "[ep 253][lr 0.0001000][3.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1860 (0.1819)  loss_ce: 0.1860 (0.1819)  loss_ce_unscaled: 0.1860 (0.1819)  loss_point_unscaled: 53.9181 (80.3317)\n",
      "[ep 254][lr 0.0001000][2.86s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1944 (0.1911)  loss_ce: 0.1944 (0.1911)  loss_ce_unscaled: 0.1944 (0.1911)  loss_point_unscaled: 52.9190 (90.9835)\n",
      "[ep 255][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1812 (0.1971)  loss_ce: 0.1812 (0.1971)  loss_ce_unscaled: 0.1812 (0.1971)  loss_point_unscaled: 56.7965 (65.5530)\n",
      "[ep 256][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1958 (0.1951)  loss_ce: 0.1958 (0.1951)  loss_ce_unscaled: 0.1958 (0.1951)  loss_point_unscaled: 52.9873 (64.5296)\n",
      "[ep 257][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1702 (0.1890)  loss_ce: 0.1702 (0.1890)  loss_ce_unscaled: 0.1702 (0.1890)  loss_point_unscaled: 53.6344 (64.4920)\n",
      "[ep 258][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1985 (0.1972)  loss_ce: 0.1985 (0.1972)  loss_ce_unscaled: 0.1985 (0.1972)  loss_point_unscaled: 57.2872 (79.0071)\n",
      "[ep 259][lr 0.0001000][2.87s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1896 (0.1935)  loss_ce: 0.1896 (0.1935)  loss_ce_unscaled: 0.1896 (0.1935)  loss_point_unscaled: 60.7200 (117.0763)\n",
      "[ep 260][lr 0.0001000][2.61s]\n",
      "=======================================test=======================================\n",
      "mae: 172.63736263736263 mse: 260.3579741918321 time: 4.280986070632935 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1796 (0.1790)  loss_ce: 0.1796 (0.1790)  loss_ce_unscaled: 0.1796 (0.1790)  loss_point_unscaled: 53.4706 (66.5066)\n",
      "[ep 261][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1902)  loss_ce: 0.1865 (0.1902)  loss_ce_unscaled: 0.1865 (0.1902)  loss_point_unscaled: 49.4038 (98.8131)\n",
      "[ep 262][lr 0.0001000][2.72s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1738 (0.1886)  loss_ce: 0.1738 (0.1886)  loss_ce_unscaled: 0.1738 (0.1886)  loss_point_unscaled: 54.9296 (98.9589)\n",
      "[ep 263][lr 0.0001000][2.59s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1927 (0.1972)  loss_ce: 0.1927 (0.1972)  loss_ce_unscaled: 0.1927 (0.1972)  loss_point_unscaled: 47.1311 (87.5679)\n",
      "[ep 264][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1847 (0.1814)  loss_ce: 0.1847 (0.1814)  loss_ce_unscaled: 0.1847 (0.1814)  loss_point_unscaled: 52.4298 (117.5052)\n",
      "[ep 265][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1867 (0.1865)  loss_ce: 0.1867 (0.1865)  loss_ce_unscaled: 0.1867 (0.1865)  loss_point_unscaled: 54.7729 (88.8117)\n",
      "[ep 266][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1892 (0.1903)  loss_ce: 0.1892 (0.1903)  loss_ce_unscaled: 0.1892 (0.1903)  loss_point_unscaled: 51.7538 (60.4422)\n",
      "[ep 267][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1828 (0.1933)  loss_ce: 0.1828 (0.1933)  loss_ce_unscaled: 0.1828 (0.1933)  loss_point_unscaled: 57.1251 (84.1852)\n",
      "[ep 268][lr 0.0001000][2.76s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1866 (0.1937)  loss_ce: 0.1866 (0.1937)  loss_ce_unscaled: 0.1866 (0.1937)  loss_point_unscaled: 54.8454 (62.2091)\n",
      "[ep 269][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1854 (0.1878)  loss_ce: 0.1854 (0.1878)  loss_ce_unscaled: 0.1854 (0.1878)  loss_point_unscaled: 49.6229 (77.0882)\n",
      "[ep 270][lr 0.0001000][3.20s]\n",
      "=======================================test=======================================\n",
      "mae: 153.52197802197801 mse: 228.89499247999635 time: 4.172321319580078 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1883 (0.1899)  loss_ce: 0.1883 (0.1899)  loss_ce_unscaled: 0.1883 (0.1899)  loss_point_unscaled: 58.1343 (106.4675)\n",
      "[ep 271][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1843 (0.1933)  loss_ce: 0.1843 (0.1933)  loss_ce_unscaled: 0.1843 (0.1933)  loss_point_unscaled: 50.3783 (77.2462)\n",
      "[ep 272][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1875)  loss_ce: 0.1808 (0.1875)  loss_ce_unscaled: 0.1808 (0.1875)  loss_point_unscaled: 52.2631 (63.8039)\n",
      "[ep 273][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1776 (0.1845)  loss_ce: 0.1776 (0.1845)  loss_ce_unscaled: 0.1776 (0.1845)  loss_point_unscaled: 68.1610 (102.1781)\n",
      "[ep 274][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1966 (0.1993)  loss_ce: 0.1966 (0.1993)  loss_ce_unscaled: 0.1966 (0.1993)  loss_point_unscaled: 53.6480 (72.4265)\n",
      "[ep 275][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1706 (0.1825)  loss_ce: 0.1706 (0.1825)  loss_ce_unscaled: 0.1706 (0.1825)  loss_point_unscaled: 51.6077 (93.9187)\n",
      "[ep 276][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1883 (0.1975)  loss_ce: 0.1883 (0.1975)  loss_ce_unscaled: 0.1883 (0.1975)  loss_point_unscaled: 53.3901 (70.9115)\n",
      "[ep 277][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1919 (0.1900)  loss_ce: 0.1919 (0.1900)  loss_ce_unscaled: 0.1919 (0.1900)  loss_point_unscaled: 53.1691 (102.3229)\n",
      "[ep 278][lr 0.0001000][2.76s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1817 (0.1936)  loss_ce: 0.1817 (0.1936)  loss_ce_unscaled: 0.1817 (0.1936)  loss_point_unscaled: 50.3452 (94.9769)\n",
      "[ep 279][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1821 (0.1910)  loss_ce: 0.1821 (0.1910)  loss_ce_unscaled: 0.1821 (0.1910)  loss_point_unscaled: 53.6725 (83.7816)\n",
      "[ep 280][lr 0.0001000][2.78s]\n",
      "=======================================test=======================================\n",
      "mae: 194.9835164835165 mse: 296.8480297176532 time: 4.189162254333496 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1938 (0.1894)  loss_ce: 0.1938 (0.1894)  loss_ce_unscaled: 0.1938 (0.1894)  loss_point_unscaled: 52.8634 (108.6240)\n",
      "[ep 281][lr 0.0001000][3.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2010 (0.2010)  loss_ce: 0.2010 (0.2010)  loss_ce_unscaled: 0.2010 (0.2010)  loss_point_unscaled: 53.8591 (88.1075)\n",
      "[ep 282][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1897 (0.1969)  loss_ce: 0.1897 (0.1969)  loss_ce_unscaled: 0.1897 (0.1969)  loss_point_unscaled: 61.8623 (73.7931)\n",
      "[ep 283][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1761 (0.1888)  loss_ce: 0.1761 (0.1888)  loss_ce_unscaled: 0.1761 (0.1888)  loss_point_unscaled: 51.2304 (74.2624)\n",
      "[ep 284][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1899 (0.1963)  loss_ce: 0.1899 (0.1963)  loss_ce_unscaled: 0.1899 (0.1963)  loss_point_unscaled: 54.5703 (93.7017)\n",
      "[ep 285][lr 0.0001000][2.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1835 (0.1899)  loss_ce: 0.1835 (0.1899)  loss_ce_unscaled: 0.1835 (0.1899)  loss_point_unscaled: 51.0782 (71.5331)\n",
      "[ep 286][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1800 (0.1896)  loss_ce: 0.1800 (0.1896)  loss_ce_unscaled: 0.1800 (0.1896)  loss_point_unscaled: 50.6785 (102.5841)\n",
      "[ep 287][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1880)  loss_ce: 0.1884 (0.1880)  loss_ce_unscaled: 0.1884 (0.1880)  loss_point_unscaled: 50.1861 (83.3402)\n",
      "[ep 288][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1818)  loss_ce: 0.1808 (0.1818)  loss_ce_unscaled: 0.1808 (0.1818)  loss_point_unscaled: 50.8325 (71.9377)\n",
      "[ep 289][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1817 (0.1940)  loss_ce: 0.1817 (0.1940)  loss_ce_unscaled: 0.1817 (0.1940)  loss_point_unscaled: 52.6985 (82.9675)\n",
      "[ep 290][lr 0.0001000][2.56s]\n",
      "=======================================test=======================================\n",
      "mae: 168.9065934065934 mse: 259.24826872163453 time: 2.2628886699676514 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1850 (0.1888)  loss_ce: 0.1850 (0.1888)  loss_ce_unscaled: 0.1850 (0.1888)  loss_point_unscaled: 50.2135 (84.6018)\n",
      "[ep 291][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1967 (0.1923)  loss_ce: 0.1967 (0.1923)  loss_ce_unscaled: 0.1967 (0.1923)  loss_point_unscaled: 51.6074 (68.5736)\n",
      "[ep 292][lr 0.0001000][2.94s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1914)  loss_ce: 0.1823 (0.1914)  loss_ce_unscaled: 0.1823 (0.1914)  loss_point_unscaled: 52.0819 (85.0804)\n",
      "[ep 293][lr 0.0001000][2.55s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1876)  loss_ce: 0.1781 (0.1876)  loss_ce_unscaled: 0.1781 (0.1876)  loss_point_unscaled: 53.6240 (63.9076)\n",
      "[ep 294][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1822 (0.1902)  loss_ce: 0.1822 (0.1902)  loss_ce_unscaled: 0.1822 (0.1902)  loss_point_unscaled: 51.0127 (76.6947)\n",
      "[ep 295][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1889)  loss_ce: 0.1872 (0.1889)  loss_ce_unscaled: 0.1872 (0.1889)  loss_point_unscaled: 52.7082 (85.7277)\n",
      "[ep 296][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1844 (0.1861)  loss_ce: 0.1844 (0.1861)  loss_ce_unscaled: 0.1844 (0.1861)  loss_point_unscaled: 54.7161 (89.7849)\n",
      "[ep 297][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1840 (0.1865)  loss_ce: 0.1840 (0.1865)  loss_ce_unscaled: 0.1840 (0.1865)  loss_point_unscaled: 54.8075 (66.9592)\n",
      "[ep 298][lr 0.0001000][3.55s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1845 (0.1920)  loss_ce: 0.1845 (0.1920)  loss_ce_unscaled: 0.1845 (0.1920)  loss_point_unscaled: 53.3528 (65.8917)\n",
      "[ep 299][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1833 (0.1935)  loss_ce: 0.1833 (0.1935)  loss_ce_unscaled: 0.1833 (0.1935)  loss_point_unscaled: 50.0505 (75.5118)\n",
      "[ep 300][lr 0.0001000][3.34s]\n",
      "=======================================test=======================================\n",
      "mae: 168.56043956043956 mse: 262.2529606784552 time: 3.5494577884674072 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1852 (0.1884)  loss_ce: 0.1852 (0.1884)  loss_ce_unscaled: 0.1852 (0.1884)  loss_point_unscaled: 51.7919 (83.5073)\n",
      "[ep 301][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1875 (0.1841)  loss_ce: 0.1875 (0.1841)  loss_ce_unscaled: 0.1875 (0.1841)  loss_point_unscaled: 56.8404 (74.7872)\n",
      "[ep 302][lr 0.0001000][3.04s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1861 (0.1980)  loss_ce: 0.1861 (0.1980)  loss_ce_unscaled: 0.1861 (0.1980)  loss_point_unscaled: 50.8529 (82.6107)\n",
      "[ep 303][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1709 (0.1871)  loss_ce: 0.1709 (0.1871)  loss_ce_unscaled: 0.1709 (0.1871)  loss_point_unscaled: 50.2285 (70.8278)\n",
      "[ep 304][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2029 (0.2014)  loss_ce: 0.2029 (0.2014)  loss_ce_unscaled: 0.2029 (0.2014)  loss_point_unscaled: 51.1707 (85.6957)\n",
      "[ep 305][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1742 (0.1879)  loss_ce: 0.1742 (0.1879)  loss_ce_unscaled: 0.1742 (0.1879)  loss_point_unscaled: 49.4607 (84.5582)\n",
      "[ep 306][lr 0.0001000][2.64s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1835 (0.1947)  loss_ce: 0.1835 (0.1947)  loss_ce_unscaled: 0.1835 (0.1947)  loss_point_unscaled: 53.3140 (80.1634)\n",
      "[ep 307][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1803 (0.1903)  loss_ce: 0.1803 (0.1903)  loss_ce_unscaled: 0.1803 (0.1903)  loss_point_unscaled: 49.3914 (60.3580)\n",
      "[ep 308][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1866 (0.1861)  loss_ce: 0.1866 (0.1861)  loss_ce_unscaled: 0.1866 (0.1861)  loss_point_unscaled: 73.8547 (103.1475)\n",
      "[ep 309][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1794 (0.1899)  loss_ce: 0.1794 (0.1899)  loss_ce_unscaled: 0.1794 (0.1899)  loss_point_unscaled: 52.3305 (80.9727)\n",
      "[ep 310][lr 0.0001000][2.48s]\n",
      "=======================================test=======================================\n",
      "mae: 145.7032967032967 mse: 231.19667190273543 time: 2.265483856201172 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1836 (0.1812)  loss_ce: 0.1836 (0.1812)  loss_ce_unscaled: 0.1836 (0.1812)  loss_point_unscaled: 49.7659 (70.8338)\n",
      "[ep 311][lr 0.0001000][3.00s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1728 (0.1817)  loss_ce: 0.1728 (0.1817)  loss_ce_unscaled: 0.1728 (0.1817)  loss_point_unscaled: 54.2779 (85.4260)\n",
      "[ep 312][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1900 (0.1964)  loss_ce: 0.1900 (0.1964)  loss_ce_unscaled: 0.1900 (0.1964)  loss_point_unscaled: 49.1789 (70.5968)\n",
      "[ep 313][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1858 (0.1847)  loss_ce: 0.1858 (0.1847)  loss_ce_unscaled: 0.1858 (0.1847)  loss_point_unscaled: 56.4126 (79.3260)\n",
      "[ep 314][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1796 (0.1925)  loss_ce: 0.1796 (0.1925)  loss_ce_unscaled: 0.1796 (0.1925)  loss_point_unscaled: 52.0908 (65.1797)\n",
      "[ep 315][lr 0.0001000][3.00s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1923 (0.1887)  loss_ce: 0.1923 (0.1887)  loss_ce_unscaled: 0.1923 (0.1887)  loss_point_unscaled: 51.1756 (73.0728)\n",
      "[ep 316][lr 0.0001000][2.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1997 (0.1998)  loss_ce: 0.1997 (0.1998)  loss_ce_unscaled: 0.1997 (0.1998)  loss_point_unscaled: 49.2296 (88.6836)\n",
      "[ep 317][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1695 (0.1810)  loss_ce: 0.1695 (0.1810)  loss_ce_unscaled: 0.1695 (0.1810)  loss_point_unscaled: 53.4831 (92.3943)\n",
      "[ep 318][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1838 (0.1903)  loss_ce: 0.1838 (0.1903)  loss_ce_unscaled: 0.1838 (0.1903)  loss_point_unscaled: 54.5697 (87.0650)\n",
      "[ep 319][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1930 (0.1997)  loss_ce: 0.1930 (0.1997)  loss_ce_unscaled: 0.1930 (0.1997)  loss_point_unscaled: 53.9368 (91.5590)\n",
      "[ep 320][lr 0.0001000][3.26s]\n",
      "=======================================test=======================================\n",
      "mae: 151.3901098901099 mse: 222.1206854417186 time: 2.2731919288635254 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1830 (0.1874)  loss_ce: 0.1830 (0.1874)  loss_ce_unscaled: 0.1830 (0.1874)  loss_point_unscaled: 50.3260 (61.5138)\n",
      "[ep 321][lr 0.0001000][2.81s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1754 (0.1863)  loss_ce: 0.1754 (0.1863)  loss_ce_unscaled: 0.1754 (0.1863)  loss_point_unscaled: 56.9106 (64.9980)\n",
      "[ep 322][lr 0.0001000][2.54s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1813 (0.1869)  loss_ce: 0.1813 (0.1869)  loss_ce_unscaled: 0.1813 (0.1869)  loss_point_unscaled: 51.0063 (92.2836)\n",
      "[ep 323][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1710 (0.1813)  loss_ce: 0.1710 (0.1813)  loss_ce_unscaled: 0.1710 (0.1813)  loss_point_unscaled: 53.5517 (89.3777)\n",
      "[ep 324][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1891)  loss_ce: 0.1755 (0.1891)  loss_ce_unscaled: 0.1755 (0.1891)  loss_point_unscaled: 54.3011 (63.1958)\n",
      "[ep 325][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1892)  loss_ce: 0.1755 (0.1892)  loss_ce_unscaled: 0.1755 (0.1892)  loss_point_unscaled: 53.1026 (61.8074)\n",
      "[ep 326][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1983 (0.1946)  loss_ce: 0.1983 (0.1946)  loss_ce_unscaled: 0.1983 (0.1946)  loss_point_unscaled: 52.0357 (78.4813)\n",
      "[ep 327][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1959 (0.1931)  loss_ce: 0.1959 (0.1931)  loss_ce_unscaled: 0.1959 (0.1931)  loss_point_unscaled: 51.5577 (76.9063)\n",
      "[ep 328][lr 0.0001000][2.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1811 (0.1874)  loss_ce: 0.1811 (0.1874)  loss_ce_unscaled: 0.1811 (0.1874)  loss_point_unscaled: 61.1261 (82.0523)\n",
      "[ep 329][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1754 (0.1793)  loss_ce: 0.1754 (0.1793)  loss_ce_unscaled: 0.1754 (0.1793)  loss_point_unscaled: 55.9346 (78.0939)\n",
      "[ep 330][lr 0.0001000][3.31s]\n",
      "=======================================test=======================================\n",
      "mae: 177.5054945054945 mse: 271.2367482963428 time: 4.172861099243164 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1694 (0.1871)  loss_ce: 0.1694 (0.1871)  loss_ce_unscaled: 0.1694 (0.1871)  loss_point_unscaled: 52.2059 (65.8622)\n",
      "[ep 331][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1948 (0.1924)  loss_ce: 0.1948 (0.1924)  loss_ce_unscaled: 0.1948 (0.1924)  loss_point_unscaled: 50.3249 (96.8432)\n",
      "[ep 332][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1856 (0.1919)  loss_ce: 0.1856 (0.1919)  loss_ce_unscaled: 0.1856 (0.1919)  loss_point_unscaled: 57.2652 (101.1750)\n",
      "[ep 333][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1925 (0.1927)  loss_ce: 0.1925 (0.1927)  loss_ce_unscaled: 0.1925 (0.1927)  loss_point_unscaled: 50.0467 (78.3298)\n",
      "[ep 334][lr 0.0001000][2.57s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1816 (0.1916)  loss_ce: 0.1816 (0.1916)  loss_ce_unscaled: 0.1816 (0.1916)  loss_point_unscaled: 55.3837 (107.2809)\n",
      "[ep 335][lr 0.0001000][3.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1875 (0.1902)  loss_ce: 0.1875 (0.1902)  loss_ce_unscaled: 0.1875 (0.1902)  loss_point_unscaled: 52.9819 (85.6085)\n",
      "[ep 336][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1848)  loss_ce: 0.1851 (0.1848)  loss_ce_unscaled: 0.1851 (0.1848)  loss_point_unscaled: 58.5282 (97.4946)\n",
      "[ep 337][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1908)  loss_ce: 0.1839 (0.1908)  loss_ce_unscaled: 0.1839 (0.1908)  loss_point_unscaled: 53.2350 (86.5051)\n",
      "[ep 338][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1879 (0.1876)  loss_ce: 0.1879 (0.1876)  loss_ce_unscaled: 0.1879 (0.1876)  loss_point_unscaled: 58.4122 (99.3608)\n",
      "[ep 339][lr 0.0001000][2.99s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2037 (0.1966)  loss_ce: 0.2037 (0.1966)  loss_ce_unscaled: 0.2037 (0.1966)  loss_point_unscaled: 58.1138 (83.4050)\n",
      "[ep 340][lr 0.0001000][2.40s]\n",
      "=======================================test=======================================\n",
      "mae: 188.35164835164835 mse: 285.15343854928966 time: 2.3177425861358643 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1796 (0.1903)  loss_ce: 0.1796 (0.1903)  loss_ce_unscaled: 0.1796 (0.1903)  loss_point_unscaled: 53.8330 (100.8734)\n",
      "[ep 341][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1806 (0.1845)  loss_ce: 0.1806 (0.1845)  loss_ce_unscaled: 0.1806 (0.1845)  loss_point_unscaled: 53.1042 (77.8522)\n",
      "[ep 342][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1713 (0.1772)  loss_ce: 0.1713 (0.1772)  loss_ce_unscaled: 0.1713 (0.1772)  loss_point_unscaled: 52.8245 (63.8560)\n",
      "[ep 343][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1918 (0.1857)  loss_ce: 0.1918 (0.1857)  loss_ce_unscaled: 0.1918 (0.1857)  loss_point_unscaled: 54.2316 (65.6354)\n",
      "[ep 344][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1783 (0.1883)  loss_ce: 0.1783 (0.1883)  loss_ce_unscaled: 0.1783 (0.1883)  loss_point_unscaled: 52.6033 (105.4542)\n",
      "[ep 345][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1754 (0.1917)  loss_ce: 0.1754 (0.1917)  loss_ce_unscaled: 0.1754 (0.1917)  loss_point_unscaled: 55.8186 (114.2301)\n",
      "[ep 346][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1859 (0.1848)  loss_ce: 0.1859 (0.1848)  loss_ce_unscaled: 0.1859 (0.1848)  loss_point_unscaled: 55.9812 (66.1794)\n",
      "[ep 347][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2022 (0.1960)  loss_ce: 0.2022 (0.1960)  loss_ce_unscaled: 0.2022 (0.1960)  loss_point_unscaled: 57.0406 (76.9450)\n",
      "[ep 348][lr 0.0001000][2.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1855 (0.1872)  loss_ce: 0.1855 (0.1872)  loss_ce_unscaled: 0.1855 (0.1872)  loss_point_unscaled: 50.9908 (67.9943)\n",
      "[ep 349][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1812 (0.1855)  loss_ce: 0.1812 (0.1855)  loss_ce_unscaled: 0.1812 (0.1855)  loss_point_unscaled: 52.5372 (69.2935)\n",
      "[ep 350][lr 0.0001000][2.46s]\n",
      "=======================================test=======================================\n",
      "mae: 154.9945054945055 mse: 230.1073925103496 time: 4.198636770248413 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1773 (0.1850)  loss_ce: 0.1773 (0.1850)  loss_ce_unscaled: 0.1773 (0.1850)  loss_point_unscaled: 51.9520 (83.6248)\n",
      "[ep 351][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1813 (0.1901)  loss_ce: 0.1813 (0.1901)  loss_ce_unscaled: 0.1813 (0.1901)  loss_point_unscaled: 54.6115 (92.1209)\n",
      "[ep 352][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1886 (0.1886)  loss_ce: 0.1886 (0.1886)  loss_ce_unscaled: 0.1886 (0.1886)  loss_point_unscaled: 51.9012 (76.7497)\n",
      "[ep 353][lr 0.0001000][2.60s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1825 (0.1904)  loss_ce: 0.1825 (0.1904)  loss_ce_unscaled: 0.1825 (0.1904)  loss_point_unscaled: 54.2075 (68.0350)\n",
      "[ep 354][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1840 (0.1902)  loss_ce: 0.1840 (0.1902)  loss_ce_unscaled: 0.1840 (0.1902)  loss_point_unscaled: 55.0161 (99.5279)\n",
      "[ep 355][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1895 (0.1911)  loss_ce: 0.1895 (0.1911)  loss_ce_unscaled: 0.1895 (0.1911)  loss_point_unscaled: 57.4942 (88.0358)\n",
      "[ep 356][lr 0.0001000][3.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1929 (0.1924)  loss_ce: 0.1929 (0.1924)  loss_ce_unscaled: 0.1929 (0.1924)  loss_point_unscaled: 51.9372 (107.4514)\n",
      "[ep 357][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1919)  loss_ce: 0.1872 (0.1919)  loss_ce_unscaled: 0.1872 (0.1919)  loss_point_unscaled: 51.7774 (86.9297)\n",
      "[ep 358][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1887)  loss_ce: 0.1781 (0.1887)  loss_ce_unscaled: 0.1781 (0.1887)  loss_point_unscaled: 51.4400 (69.2674)\n",
      "[ep 359][lr 0.0001000][3.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1873 (0.1880)  loss_ce: 0.1873 (0.1880)  loss_ce_unscaled: 0.1873 (0.1880)  loss_point_unscaled: 54.2179 (80.2811)\n",
      "[ep 360][lr 0.0001000][2.40s]\n",
      "=======================================test=======================================\n",
      "mae: 170.30769230769232 mse: 262.9808625469429 time: 2.7929775714874268 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1803 (0.1867)  loss_ce: 0.1803 (0.1867)  loss_ce_unscaled: 0.1803 (0.1867)  loss_point_unscaled: 51.4395 (74.0599)\n",
      "[ep 361][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1834 (0.1881)  loss_ce: 0.1834 (0.1881)  loss_ce_unscaled: 0.1834 (0.1881)  loss_point_unscaled: 50.8258 (67.6103)\n",
      "[ep 362][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1875 (0.1907)  loss_ce: 0.1875 (0.1907)  loss_ce_unscaled: 0.1875 (0.1907)  loss_point_unscaled: 49.9776 (101.2619)\n",
      "[ep 363][lr 0.0001000][2.87s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1889 (0.1912)  loss_ce: 0.1889 (0.1912)  loss_ce_unscaled: 0.1889 (0.1912)  loss_point_unscaled: 52.6096 (75.5616)\n",
      "[ep 364][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1821)  loss_ce: 0.1851 (0.1821)  loss_ce_unscaled: 0.1851 (0.1821)  loss_point_unscaled: 52.3744 (84.4682)\n",
      "[ep 365][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1891 (0.1929)  loss_ce: 0.1891 (0.1929)  loss_ce_unscaled: 0.1891 (0.1929)  loss_point_unscaled: 57.9639 (87.8244)\n",
      "[ep 366][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1853)  loss_ce: 0.1872 (0.1853)  loss_ce_unscaled: 0.1872 (0.1853)  loss_point_unscaled: 54.1767 (73.1555)\n",
      "[ep 367][lr 0.0001000][2.80s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1869 (0.1928)  loss_ce: 0.1869 (0.1928)  loss_ce_unscaled: 0.1869 (0.1928)  loss_point_unscaled: 53.5334 (74.0576)\n",
      "[ep 368][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1862 (0.1948)  loss_ce: 0.1862 (0.1948)  loss_ce_unscaled: 0.1862 (0.1948)  loss_point_unscaled: 51.7586 (82.8905)\n",
      "[ep 369][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1879 (0.1844)  loss_ce: 0.1879 (0.1844)  loss_ce_unscaled: 0.1879 (0.1844)  loss_point_unscaled: 53.5420 (99.3671)\n",
      "[ep 370][lr 0.0001000][3.27s]\n",
      "=======================================test=======================================\n",
      "mae: 170.8131868131868 mse: 268.72644287248903 time: 3.817500352859497 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1831 (0.1843)  loss_ce: 0.1831 (0.1843)  loss_ce_unscaled: 0.1831 (0.1843)  loss_point_unscaled: 53.2483 (97.3769)\n",
      "[ep 371][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1784 (0.1959)  loss_ce: 0.1784 (0.1959)  loss_ce_unscaled: 0.1784 (0.1959)  loss_point_unscaled: 53.5269 (97.7380)\n",
      "[ep 372][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1841 (0.1846)  loss_ce: 0.1841 (0.1846)  loss_ce_unscaled: 0.1841 (0.1846)  loss_point_unscaled: 51.1809 (78.8401)\n",
      "[ep 373][lr 0.0001000][2.92s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1957 (0.1930)  loss_ce: 0.1957 (0.1930)  loss_ce_unscaled: 0.1957 (0.1930)  loss_point_unscaled: 52.9365 (74.9962)\n",
      "[ep 374][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1841 (0.1855)  loss_ce: 0.1841 (0.1855)  loss_ce_unscaled: 0.1841 (0.1855)  loss_point_unscaled: 53.9169 (85.8329)\n",
      "[ep 375][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1870 (0.1991)  loss_ce: 0.1870 (0.1991)  loss_ce_unscaled: 0.1870 (0.1991)  loss_point_unscaled: 53.1658 (78.7091)\n",
      "[ep 376][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1835 (0.1884)  loss_ce: 0.1835 (0.1884)  loss_ce_unscaled: 0.1835 (0.1884)  loss_point_unscaled: 52.1906 (72.7330)\n",
      "[ep 377][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1845)  loss_ce: 0.1808 (0.1845)  loss_ce_unscaled: 0.1808 (0.1845)  loss_point_unscaled: 51.7884 (68.1811)\n",
      "[ep 378][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1869 (0.1921)  loss_ce: 0.1869 (0.1921)  loss_ce_unscaled: 0.1869 (0.1921)  loss_point_unscaled: 49.4566 (92.6447)\n",
      "[ep 379][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1902 (0.1895)  loss_ce: 0.1902 (0.1895)  loss_ce_unscaled: 0.1902 (0.1895)  loss_point_unscaled: 57.4725 (111.3002)\n",
      "[ep 380][lr 0.0001000][3.35s]\n",
      "=======================================test=======================================\n",
      "mae: 158.95054945054946 mse: 239.09889213415485 time: 3.233353614807129 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1838 (0.1877)  loss_ce: 0.1838 (0.1877)  loss_ce_unscaled: 0.1838 (0.1877)  loss_point_unscaled: 52.8041 (77.6180)\n",
      "[ep 381][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1975 (0.1953)  loss_ce: 0.1975 (0.1953)  loss_ce_unscaled: 0.1975 (0.1953)  loss_point_unscaled: 50.4625 (72.2298)\n",
      "[ep 382][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1791 (0.1832)  loss_ce: 0.1791 (0.1832)  loss_ce_unscaled: 0.1791 (0.1832)  loss_point_unscaled: 55.4259 (58.8931)\n",
      "[ep 383][lr 0.0001000][2.57s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1873 (0.1940)  loss_ce: 0.1873 (0.1940)  loss_ce_unscaled: 0.1873 (0.1940)  loss_point_unscaled: 50.7339 (73.5522)\n",
      "[ep 384][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1858)  loss_ce: 0.1839 (0.1858)  loss_ce_unscaled: 0.1839 (0.1858)  loss_point_unscaled: 53.4768 (72.9257)\n",
      "[ep 385][lr 0.0001000][2.91s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1894 (0.1894)  loss_ce: 0.1894 (0.1894)  loss_ce_unscaled: 0.1894 (0.1894)  loss_point_unscaled: 51.1822 (76.8904)\n",
      "[ep 386][lr 0.0001000][3.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1860)  loss_ce: 0.1832 (0.1860)  loss_ce_unscaled: 0.1832 (0.1860)  loss_point_unscaled: 53.2909 (62.4786)\n",
      "[ep 387][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1928 (0.1931)  loss_ce: 0.1928 (0.1931)  loss_ce_unscaled: 0.1928 (0.1931)  loss_point_unscaled: 48.7549 (68.8745)\n",
      "[ep 388][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1843 (0.1974)  loss_ce: 0.1843 (0.1974)  loss_ce_unscaled: 0.1843 (0.1974)  loss_point_unscaled: 54.3218 (85.0763)\n",
      "[ep 389][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1902 (0.1877)  loss_ce: 0.1902 (0.1877)  loss_ce_unscaled: 0.1902 (0.1877)  loss_point_unscaled: 56.8048 (90.4750)\n",
      "[ep 390][lr 0.0001000][3.29s]\n",
      "=======================================test=======================================\n",
      "mae: 149.14835164835165 mse: 230.62991857228616 time: 2.6220645904541016 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1762 (0.1864)  loss_ce: 0.1762 (0.1864)  loss_ce_unscaled: 0.1762 (0.1864)  loss_point_unscaled: 54.5597 (99.7791)\n",
      "[ep 391][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1770 (0.1825)  loss_ce: 0.1770 (0.1825)  loss_ce_unscaled: 0.1770 (0.1825)  loss_point_unscaled: 57.6934 (96.7565)\n",
      "[ep 392][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1867 (0.1891)  loss_ce: 0.1867 (0.1891)  loss_ce_unscaled: 0.1867 (0.1891)  loss_point_unscaled: 52.2986 (70.2784)\n",
      "[ep 393][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1860 (0.1909)  loss_ce: 0.1860 (0.1909)  loss_ce_unscaled: 0.1860 (0.1909)  loss_point_unscaled: 52.6240 (77.1972)\n",
      "[ep 394][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1862 (0.1870)  loss_ce: 0.1862 (0.1870)  loss_ce_unscaled: 0.1862 (0.1870)  loss_point_unscaled: 48.4376 (88.7859)\n",
      "[ep 395][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1974 (0.1972)  loss_ce: 0.1974 (0.1972)  loss_ce_unscaled: 0.1974 (0.1972)  loss_point_unscaled: 50.3646 (76.7060)\n",
      "[ep 396][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1797 (0.1866)  loss_ce: 0.1797 (0.1866)  loss_ce_unscaled: 0.1797 (0.1866)  loss_point_unscaled: 50.2303 (75.4863)\n",
      "[ep 397][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1878 (0.1899)  loss_ce: 0.1878 (0.1899)  loss_ce_unscaled: 0.1878 (0.1899)  loss_point_unscaled: 56.1758 (89.0140)\n",
      "[ep 398][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1867 (0.1839)  loss_ce: 0.1867 (0.1839)  loss_ce_unscaled: 0.1867 (0.1839)  loss_point_unscaled: 55.0546 (87.2078)\n",
      "[ep 399][lr 0.0001000][2.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1886 (0.1845)  loss_ce: 0.1886 (0.1845)  loss_ce_unscaled: 0.1886 (0.1845)  loss_point_unscaled: 55.1097 (113.2854)\n",
      "[ep 400][lr 0.0001000][2.97s]\n",
      "=======================================test=======================================\n",
      "mae: 181.13186813186815 mse: 262.9901180611207 time: 2.270110845565796 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1856 (0.1908)  loss_ce: 0.1856 (0.1908)  loss_ce_unscaled: 0.1856 (0.1908)  loss_point_unscaled: 50.2105 (86.8828)\n",
      "[ep 401][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1907 (0.1914)  loss_ce: 0.1907 (0.1914)  loss_ce_unscaled: 0.1907 (0.1914)  loss_point_unscaled: 51.2952 (81.3703)\n",
      "[ep 402][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1866 (0.1904)  loss_ce: 0.1866 (0.1904)  loss_ce_unscaled: 0.1866 (0.1904)  loss_point_unscaled: 54.0076 (95.6392)\n",
      "[ep 403][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1922 (0.1951)  loss_ce: 0.1922 (0.1951)  loss_ce_unscaled: 0.1922 (0.1951)  loss_point_unscaled: 50.3845 (60.1940)\n",
      "[ep 404][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1891 (0.1926)  loss_ce: 0.1891 (0.1926)  loss_ce_unscaled: 0.1891 (0.1926)  loss_point_unscaled: 60.8894 (117.6173)\n",
      "[ep 405][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1766 (0.1839)  loss_ce: 0.1766 (0.1839)  loss_ce_unscaled: 0.1766 (0.1839)  loss_point_unscaled: 51.1561 (80.5903)\n",
      "[ep 406][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1855 (0.1944)  loss_ce: 0.1855 (0.1944)  loss_ce_unscaled: 0.1855 (0.1944)  loss_point_unscaled: 52.0397 (74.2267)\n",
      "[ep 407][lr 0.0001000][2.70s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1904 (0.1884)  loss_ce: 0.1904 (0.1884)  loss_ce_unscaled: 0.1904 (0.1884)  loss_point_unscaled: 52.4436 (90.0939)\n",
      "[ep 408][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1914 (0.1915)  loss_ce: 0.1914 (0.1915)  loss_ce_unscaled: 0.1914 (0.1915)  loss_point_unscaled: 59.9944 (84.5007)\n",
      "[ep 409][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1748 (0.1798)  loss_ce: 0.1748 (0.1798)  loss_ce_unscaled: 0.1748 (0.1798)  loss_point_unscaled: 49.3800 (71.5726)\n",
      "[ep 410][lr 0.0001000][3.16s]\n",
      "=======================================test=======================================\n",
      "mae: 175.5879120879121 mse: 257.2729661679966 time: 2.3429782390594482 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1739 (0.1900)  loss_ce: 0.1739 (0.1900)  loss_ce_unscaled: 0.1739 (0.1900)  loss_point_unscaled: 54.2514 (82.7444)\n",
      "[ep 411][lr 0.0001000][2.54s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1903)  loss_ce: 0.1851 (0.1903)  loss_ce_unscaled: 0.1851 (0.1903)  loss_point_unscaled: 49.7329 (86.4567)\n",
      "[ep 412][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1742 (0.1775)  loss_ce: 0.1742 (0.1775)  loss_ce_unscaled: 0.1742 (0.1775)  loss_point_unscaled: 54.8975 (73.0329)\n",
      "[ep 413][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1887 (0.1909)  loss_ce: 0.1887 (0.1909)  loss_ce_unscaled: 0.1887 (0.1909)  loss_point_unscaled: 61.1589 (82.0116)\n",
      "[ep 414][lr 0.0001000][3.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1854 (0.1867)  loss_ce: 0.1854 (0.1867)  loss_ce_unscaled: 0.1854 (0.1867)  loss_point_unscaled: 51.4875 (96.2904)\n",
      "[ep 415][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1824 (0.1891)  loss_ce: 0.1824 (0.1891)  loss_ce_unscaled: 0.1824 (0.1891)  loss_point_unscaled: 72.2677 (92.4622)\n",
      "[ep 416][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1896 (0.1897)  loss_ce: 0.1896 (0.1897)  loss_ce_unscaled: 0.1896 (0.1897)  loss_point_unscaled: 49.4231 (65.8250)\n",
      "[ep 417][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1779 (0.1922)  loss_ce: 0.1779 (0.1922)  loss_ce_unscaled: 0.1779 (0.1922)  loss_point_unscaled: 53.7274 (70.6473)\n",
      "[ep 418][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1925 (0.1875)  loss_ce: 0.1925 (0.1875)  loss_ce_unscaled: 0.1925 (0.1875)  loss_point_unscaled: 50.3237 (56.5513)\n",
      "[ep 419][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1916 (0.1918)  loss_ce: 0.1916 (0.1918)  loss_ce_unscaled: 0.1916 (0.1918)  loss_point_unscaled: 48.7650 (72.0957)\n",
      "[ep 420][lr 0.0001000][2.91s]\n",
      "=======================================test=======================================\n",
      "mae: 153.43406593406593 mse: 227.61219911686015 time: 4.259712219238281 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1705 (0.1779)  loss_ce: 0.1705 (0.1779)  loss_ce_unscaled: 0.1705 (0.1779)  loss_point_unscaled: 56.5278 (87.7173)\n",
      "[ep 421][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1856 (0.1935)  loss_ce: 0.1856 (0.1935)  loss_ce_unscaled: 0.1856 (0.1935)  loss_point_unscaled: 51.4798 (75.3140)\n",
      "[ep 422][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1790)  loss_ce: 0.1755 (0.1790)  loss_ce_unscaled: 0.1755 (0.1790)  loss_point_unscaled: 53.0763 (69.6568)\n",
      "[ep 423][lr 0.0001000][2.95s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1837)  loss_ce: 0.1823 (0.1837)  loss_ce_unscaled: 0.1823 (0.1837)  loss_point_unscaled: 52.3316 (110.3986)\n",
      "[ep 424][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1875 (0.1892)  loss_ce: 0.1875 (0.1892)  loss_ce_unscaled: 0.1875 (0.1892)  loss_point_unscaled: 49.0849 (77.5230)\n",
      "[ep 425][lr 0.0001000][3.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1908 (0.1898)  loss_ce: 0.1908 (0.1898)  loss_ce_unscaled: 0.1908 (0.1898)  loss_point_unscaled: 53.0458 (86.3398)\n",
      "[ep 426][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1859 (0.1836)  loss_ce: 0.1859 (0.1836)  loss_ce_unscaled: 0.1859 (0.1836)  loss_point_unscaled: 59.3065 (82.5038)\n",
      "[ep 427][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1853 (0.1923)  loss_ce: 0.1853 (0.1923)  loss_ce_unscaled: 0.1853 (0.1923)  loss_point_unscaled: 52.7450 (77.3522)\n",
      "[ep 428][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1799 (0.1907)  loss_ce: 0.1799 (0.1907)  loss_ce_unscaled: 0.1799 (0.1907)  loss_point_unscaled: 50.2892 (54.4777)\n",
      "[ep 429][lr 0.0001000][2.55s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1769 (0.1877)  loss_ce: 0.1769 (0.1877)  loss_ce_unscaled: 0.1769 (0.1877)  loss_point_unscaled: 54.8583 (76.9894)\n",
      "[ep 430][lr 0.0001000][3.23s]\n",
      "=======================================test=======================================\n",
      "mae: 160.15934065934067 mse: 241.90794952617006 time: 2.2618408203125 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1869 (0.1873)  loss_ce: 0.1869 (0.1873)  loss_ce_unscaled: 0.1869 (0.1873)  loss_point_unscaled: 50.6996 (72.5531)\n",
      "[ep 431][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1822 (0.1873)  loss_ce: 0.1822 (0.1873)  loss_ce_unscaled: 0.1822 (0.1873)  loss_point_unscaled: 53.1097 (76.2221)\n",
      "[ep 432][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1785 (0.1932)  loss_ce: 0.1785 (0.1932)  loss_ce_unscaled: 0.1785 (0.1932)  loss_point_unscaled: 53.6517 (78.7599)\n",
      "[ep 433][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1726 (0.1884)  loss_ce: 0.1726 (0.1884)  loss_ce_unscaled: 0.1726 (0.1884)  loss_point_unscaled: 50.4416 (84.7698)\n",
      "[ep 434][lr 0.0001000][2.85s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1900)  loss_ce: 0.1865 (0.1900)  loss_ce_unscaled: 0.1865 (0.1900)  loss_point_unscaled: 51.2841 (65.4433)\n",
      "[ep 435][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1761 (0.1838)  loss_ce: 0.1761 (0.1838)  loss_ce_unscaled: 0.1761 (0.1838)  loss_point_unscaled: 52.2087 (80.0663)\n",
      "[ep 436][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1862 (0.1930)  loss_ce: 0.1862 (0.1930)  loss_ce_unscaled: 0.1862 (0.1930)  loss_point_unscaled: 49.2123 (76.0018)\n",
      "[ep 437][lr 0.0001000][2.72s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1716 (0.1819)  loss_ce: 0.1716 (0.1819)  loss_ce_unscaled: 0.1716 (0.1819)  loss_point_unscaled: 55.0331 (114.0840)\n",
      "[ep 438][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1882 (0.1912)  loss_ce: 0.1882 (0.1912)  loss_ce_unscaled: 0.1882 (0.1912)  loss_point_unscaled: 52.1422 (70.5751)\n",
      "[ep 439][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1957 (0.1904)  loss_ce: 0.1957 (0.1904)  loss_ce_unscaled: 0.1957 (0.1904)  loss_point_unscaled: 50.4173 (90.7741)\n",
      "[ep 440][lr 0.0001000][3.32s]\n",
      "=======================================test=======================================\n",
      "mae: 194.83516483516485 mse: 306.69243999037985 time: 2.3199424743652344 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1879)  loss_ce: 0.1808 (0.1879)  loss_ce_unscaled: 0.1808 (0.1879)  loss_point_unscaled: 49.9156 (95.5579)\n",
      "[ep 441][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1738 (0.1830)  loss_ce: 0.1738 (0.1830)  loss_ce_unscaled: 0.1738 (0.1830)  loss_point_unscaled: 52.0467 (67.0496)\n",
      "[ep 442][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1798 (0.1845)  loss_ce: 0.1798 (0.1845)  loss_ce_unscaled: 0.1798 (0.1845)  loss_point_unscaled: 50.3305 (60.5041)\n",
      "[ep 443][lr 0.0001000][2.94s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1867 (0.1922)  loss_ce: 0.1867 (0.1922)  loss_ce_unscaled: 0.1867 (0.1922)  loss_point_unscaled: 50.7619 (120.1538)\n",
      "[ep 444][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1789 (0.1846)  loss_ce: 0.1789 (0.1846)  loss_ce_unscaled: 0.1789 (0.1846)  loss_point_unscaled: 52.5461 (58.8781)\n",
      "[ep 445][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1805 (0.1913)  loss_ce: 0.1805 (0.1913)  loss_ce_unscaled: 0.1805 (0.1913)  loss_point_unscaled: 53.3923 (97.3830)\n",
      "[ep 446][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1940 (0.1944)  loss_ce: 0.1940 (0.1944)  loss_ce_unscaled: 0.1940 (0.1944)  loss_point_unscaled: 50.0958 (77.5923)\n",
      "[ep 447][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1833 (0.1883)  loss_ce: 0.1833 (0.1883)  loss_ce_unscaled: 0.1833 (0.1883)  loss_point_unscaled: 50.7734 (85.3330)\n",
      "[ep 448][lr 0.0001000][2.89s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1905 (0.1897)  loss_ce: 0.1905 (0.1897)  loss_ce_unscaled: 0.1905 (0.1897)  loss_point_unscaled: 52.5596 (67.7306)\n",
      "[ep 449][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1734 (0.1930)  loss_ce: 0.1734 (0.1930)  loss_ce_unscaled: 0.1734 (0.1930)  loss_point_unscaled: 61.6280 (116.1862)\n",
      "[ep 450][lr 0.0001000][3.37s]\n",
      "=======================================test=======================================\n",
      "mae: 175.42307692307693 mse: 245.91657434679254 time: 3.05308198928833 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1918 (0.1884)  loss_ce: 0.1918 (0.1884)  loss_ce_unscaled: 0.1918 (0.1884)  loss_point_unscaled: 51.1541 (74.2401)\n",
      "[ep 451][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1816 (0.1845)  loss_ce: 0.1816 (0.1845)  loss_ce_unscaled: 0.1816 (0.1845)  loss_point_unscaled: 50.0814 (65.1623)\n",
      "[ep 452][lr 0.0001000][3.00s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1715 (0.1880)  loss_ce: 0.1715 (0.1880)  loss_ce_unscaled: 0.1715 (0.1880)  loss_point_unscaled: 51.1054 (70.5796)\n",
      "[ep 453][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1847 (0.1940)  loss_ce: 0.1847 (0.1940)  loss_ce_unscaled: 0.1847 (0.1940)  loss_point_unscaled: 50.1896 (79.7491)\n",
      "[ep 454][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1920 (0.1908)  loss_ce: 0.1920 (0.1908)  loss_ce_unscaled: 0.1920 (0.1908)  loss_point_unscaled: 55.6224 (115.8427)\n",
      "[ep 455][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1831 (0.1858)  loss_ce: 0.1831 (0.1858)  loss_ce_unscaled: 0.1831 (0.1858)  loss_point_unscaled: 52.5266 (93.5342)\n",
      "[ep 456][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1825 (0.1884)  loss_ce: 0.1825 (0.1884)  loss_ce_unscaled: 0.1825 (0.1884)  loss_point_unscaled: 52.9078 (76.7731)\n",
      "[ep 457][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1937 (0.1907)  loss_ce: 0.1937 (0.1907)  loss_ce_unscaled: 0.1937 (0.1907)  loss_point_unscaled: 50.4894 (101.1703)\n",
      "[ep 458][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1826 (0.1941)  loss_ce: 0.1826 (0.1941)  loss_ce_unscaled: 0.1826 (0.1941)  loss_point_unscaled: 56.9095 (77.0202)\n",
      "[ep 459][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1792 (0.1844)  loss_ce: 0.1792 (0.1844)  loss_ce_unscaled: 0.1792 (0.1844)  loss_point_unscaled: 52.2725 (94.5808)\n",
      "[ep 460][lr 0.0001000][3.07s]\n",
      "=======================================test=======================================\n",
      "mae: 152.95604395604394 mse: 221.72364246145162 time: 2.2395923137664795 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1914)  loss_ce: 0.1832 (0.1914)  loss_ce_unscaled: 0.1832 (0.1914)  loss_point_unscaled: 53.2654 (73.7275)\n",
      "[ep 461][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1782 (0.1826)  loss_ce: 0.1782 (0.1826)  loss_ce_unscaled: 0.1782 (0.1826)  loss_point_unscaled: 55.6083 (100.6491)\n",
      "[ep 462][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1881 (0.1936)  loss_ce: 0.1881 (0.1936)  loss_ce_unscaled: 0.1881 (0.1936)  loss_point_unscaled: 54.8926 (77.1726)\n",
      "[ep 463][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1918 (0.1972)  loss_ce: 0.1918 (0.1972)  loss_ce_unscaled: 0.1918 (0.1972)  loss_point_unscaled: 52.8479 (116.5639)\n",
      "[ep 464][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1950 (0.1920)  loss_ce: 0.1950 (0.1920)  loss_ce_unscaled: 0.1950 (0.1920)  loss_point_unscaled: 52.1694 (99.6281)\n",
      "[ep 465][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1848 (0.1985)  loss_ce: 0.1848 (0.1985)  loss_ce_unscaled: 0.1848 (0.1985)  loss_point_unscaled: 50.4399 (72.8263)\n",
      "[ep 466][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1878 (0.1811)  loss_ce: 0.1878 (0.1811)  loss_ce_unscaled: 0.1878 (0.1811)  loss_point_unscaled: 52.1846 (83.4192)\n",
      "[ep 467][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1844 (0.1874)  loss_ce: 0.1844 (0.1874)  loss_ce_unscaled: 0.1844 (0.1874)  loss_point_unscaled: 50.3406 (66.0991)\n",
      "[ep 468][lr 0.0001000][3.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1927 (0.1840)  loss_ce: 0.1927 (0.1840)  loss_ce_unscaled: 0.1927 (0.1840)  loss_point_unscaled: 51.3516 (85.2769)\n",
      "[ep 469][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1911)  loss_ce: 0.1823 (0.1911)  loss_ce_unscaled: 0.1823 (0.1911)  loss_point_unscaled: 52.3278 (68.0497)\n",
      "[ep 470][lr 0.0001000][2.73s]\n",
      "=======================================test=======================================\n",
      "mae: 185.32417582417582 mse: 282.3528402081806 time: 2.2686893939971924 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1815 (0.1863)  loss_ce: 0.1815 (0.1863)  loss_ce_unscaled: 0.1815 (0.1863)  loss_point_unscaled: 51.2483 (112.3446)\n",
      "[ep 471][lr 0.0001000][2.58s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1829 (0.1856)  loss_ce: 0.1829 (0.1856)  loss_ce_unscaled: 0.1829 (0.1856)  loss_point_unscaled: 63.0070 (104.4120)\n",
      "[ep 472][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1726 (0.1785)  loss_ce: 0.1726 (0.1785)  loss_ce_unscaled: 0.1726 (0.1785)  loss_point_unscaled: 53.8971 (83.3704)\n",
      "[ep 473][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1834)  loss_ce: 0.1781 (0.1834)  loss_ce_unscaled: 0.1781 (0.1834)  loss_point_unscaled: 51.3402 (70.6828)\n",
      "[ep 474][lr 0.0001000][2.70s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1802 (0.1868)  loss_ce: 0.1802 (0.1868)  loss_ce_unscaled: 0.1802 (0.1868)  loss_point_unscaled: 52.6929 (71.3300)\n",
      "[ep 475][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1735 (0.1880)  loss_ce: 0.1735 (0.1880)  loss_ce_unscaled: 0.1735 (0.1880)  loss_point_unscaled: 51.6233 (69.8314)\n",
      "[ep 476][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1720 (0.1788)  loss_ce: 0.1720 (0.1788)  loss_ce_unscaled: 0.1720 (0.1788)  loss_point_unscaled: 52.1628 (66.6784)\n",
      "[ep 477][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1899 (0.1970)  loss_ce: 0.1899 (0.1970)  loss_ce_unscaled: 0.1899 (0.1970)  loss_point_unscaled: 54.0881 (90.2157)\n",
      "[ep 478][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1862 (0.1938)  loss_ce: 0.1862 (0.1938)  loss_ce_unscaled: 0.1862 (0.1938)  loss_point_unscaled: 53.0700 (97.4878)\n",
      "[ep 479][lr 0.0001000][2.54s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1850)  loss_ce: 0.1832 (0.1850)  loss_ce_unscaled: 0.1832 (0.1850)  loss_point_unscaled: 50.4778 (100.7375)\n",
      "[ep 480][lr 0.0001000][3.26s]\n",
      "=======================================test=======================================\n",
      "mae: 162.5989010989011 mse: 246.04615175045674 time: 4.219931125640869 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1748 (0.1869)  loss_ce: 0.1748 (0.1869)  loss_ce_unscaled: 0.1748 (0.1869)  loss_point_unscaled: 52.4561 (100.2041)\n",
      "[ep 481][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1981 (0.2049)  loss_ce: 0.1981 (0.2049)  loss_ce_unscaled: 0.1981 (0.2049)  loss_point_unscaled: 54.2551 (96.2365)\n",
      "[ep 482][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1866 (0.1898)  loss_ce: 0.1866 (0.1898)  loss_ce_unscaled: 0.1866 (0.1898)  loss_point_unscaled: 52.4768 (83.8818)\n",
      "[ep 483][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1758 (0.1783)  loss_ce: 0.1758 (0.1783)  loss_ce_unscaled: 0.1758 (0.1783)  loss_point_unscaled: 52.4168 (89.1495)\n",
      "[ep 484][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1870 (0.1869)  loss_ce: 0.1870 (0.1869)  loss_ce_unscaled: 0.1870 (0.1869)  loss_point_unscaled: 51.5803 (91.8308)\n",
      "[ep 485][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1818 (0.1846)  loss_ce: 0.1818 (0.1846)  loss_ce_unscaled: 0.1818 (0.1846)  loss_point_unscaled: 54.8650 (63.0729)\n",
      "[ep 486][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1901 (0.1944)  loss_ce: 0.1901 (0.1944)  loss_ce_unscaled: 0.1901 (0.1944)  loss_point_unscaled: 50.6606 (79.6703)\n",
      "[ep 487][lr 0.0001000][2.51s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1874 (0.1938)  loss_ce: 0.1874 (0.1938)  loss_ce_unscaled: 0.1874 (0.1938)  loss_point_unscaled: 49.9614 (71.3347)\n",
      "[ep 488][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1844 (0.1848)  loss_ce: 0.1844 (0.1848)  loss_ce_unscaled: 0.1844 (0.1848)  loss_point_unscaled: 55.4898 (113.2052)\n",
      "[ep 489][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1918 (0.1928)  loss_ce: 0.1918 (0.1928)  loss_ce_unscaled: 0.1918 (0.1928)  loss_point_unscaled: 51.5122 (60.5541)\n",
      "[ep 490][lr 0.0001000][2.34s]\n",
      "=======================================test=======================================\n",
      "mae: 186.43956043956044 mse: 303.17968777100333 time: 2.3395776748657227 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1930 (0.1947)  loss_ce: 0.1930 (0.1947)  loss_ce_unscaled: 0.1930 (0.1947)  loss_point_unscaled: 53.2233 (89.4972)\n",
      "[ep 491][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1818 (0.1848)  loss_ce: 0.1818 (0.1848)  loss_ce_unscaled: 0.1818 (0.1848)  loss_point_unscaled: 54.9047 (86.2244)\n",
      "[ep 492][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1853 (0.1903)  loss_ce: 0.1853 (0.1903)  loss_ce_unscaled: 0.1853 (0.1903)  loss_point_unscaled: 51.0740 (81.2062)\n",
      "[ep 493][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1973 (0.1937)  loss_ce: 0.1973 (0.1937)  loss_ce_unscaled: 0.1973 (0.1937)  loss_point_unscaled: 56.3790 (83.3310)\n",
      "[ep 494][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2003 (0.1971)  loss_ce: 0.2003 (0.1971)  loss_ce_unscaled: 0.2003 (0.1971)  loss_point_unscaled: 50.7285 (77.1859)\n",
      "[ep 495][lr 0.0001000][3.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1811 (0.1844)  loss_ce: 0.1811 (0.1844)  loss_ce_unscaled: 0.1811 (0.1844)  loss_point_unscaled: 51.7204 (77.4301)\n",
      "[ep 496][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1777 (0.1865)  loss_ce: 0.1777 (0.1865)  loss_ce_unscaled: 0.1777 (0.1865)  loss_point_unscaled: 51.6051 (100.2353)\n",
      "[ep 497][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1872)  loss_ce: 0.1872 (0.1872)  loss_ce_unscaled: 0.1872 (0.1872)  loss_point_unscaled: 51.0989 (68.5157)\n",
      "[ep 498][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1876 (0.1969)  loss_ce: 0.1876 (0.1969)  loss_ce_unscaled: 0.1876 (0.1969)  loss_point_unscaled: 54.2139 (89.2845)\n",
      "[ep 499][lr 0.0001000][2.69s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1760 (0.1878)  loss_ce: 0.1760 (0.1878)  loss_ce_unscaled: 0.1760 (0.1878)  loss_point_unscaled: 50.8680 (77.8121)\n",
      "[ep 500][lr 0.0001000][3.30s]\n",
      "=======================================test=======================================\n",
      "mae: 164.9065934065934 mse: 223.97491764986484 time: 2.2884609699249268 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1782 (0.1869)  loss_ce: 0.1782 (0.1869)  loss_ce_unscaled: 0.1782 (0.1869)  loss_point_unscaled: 51.9886 (77.0093)\n",
      "[ep 501][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1715 (0.1828)  loss_ce: 0.1715 (0.1828)  loss_ce_unscaled: 0.1715 (0.1828)  loss_point_unscaled: 53.1796 (84.5649)\n",
      "[ep 502][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1882)  loss_ce: 0.1839 (0.1882)  loss_ce_unscaled: 0.1839 (0.1882)  loss_point_unscaled: 57.5659 (81.9528)\n",
      "[ep 503][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1881 (0.1930)  loss_ce: 0.1881 (0.1930)  loss_ce_unscaled: 0.1881 (0.1930)  loss_point_unscaled: 55.1298 (72.8096)\n",
      "[ep 504][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1984 (0.1922)  loss_ce: 0.1984 (0.1922)  loss_ce_unscaled: 0.1984 (0.1922)  loss_point_unscaled: 50.5087 (103.7660)\n",
      "[ep 505][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1856 (0.1921)  loss_ce: 0.1856 (0.1921)  loss_ce_unscaled: 0.1856 (0.1921)  loss_point_unscaled: 53.9339 (89.4730)\n",
      "[ep 506][lr 0.0001000][3.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1855 (0.1846)  loss_ce: 0.1855 (0.1846)  loss_ce_unscaled: 0.1855 (0.1846)  loss_point_unscaled: 52.2876 (107.1237)\n",
      "[ep 507][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1998 (0.1919)  loss_ce: 0.1998 (0.1919)  loss_ce_unscaled: 0.1998 (0.1919)  loss_point_unscaled: 52.8938 (59.7227)\n",
      "[ep 508][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1680 (0.1823)  loss_ce: 0.1680 (0.1823)  loss_ce_unscaled: 0.1680 (0.1823)  loss_point_unscaled: 53.3666 (58.4175)\n",
      "[ep 509][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1766 (0.1866)  loss_ce: 0.1766 (0.1866)  loss_ce_unscaled: 0.1766 (0.1866)  loss_point_unscaled: 56.8403 (75.8899)\n",
      "[ep 510][lr 0.0001000][3.14s]\n",
      "=======================================test=======================================\n",
      "mae: 184.92307692307693 mse: 284.80969161803546 time: 2.2842612266540527 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1893 (0.1855)  loss_ce: 0.1893 (0.1855)  loss_ce_unscaled: 0.1893 (0.1855)  loss_point_unscaled: 53.7964 (70.2922)\n",
      "[ep 511][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1926 (0.1934)  loss_ce: 0.1926 (0.1934)  loss_ce_unscaled: 0.1926 (0.1934)  loss_point_unscaled: 50.3688 (62.3395)\n",
      "[ep 512][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1721 (0.1801)  loss_ce: 0.1721 (0.1801)  loss_ce_unscaled: 0.1721 (0.1801)  loss_point_unscaled: 52.7220 (71.1689)\n",
      "[ep 513][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1787 (0.1843)  loss_ce: 0.1787 (0.1843)  loss_ce_unscaled: 0.1787 (0.1843)  loss_point_unscaled: 51.4003 (71.6606)\n",
      "[ep 514][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1871 (0.1865)  loss_ce: 0.1871 (0.1865)  loss_ce_unscaled: 0.1871 (0.1865)  loss_point_unscaled: 52.5105 (90.8800)\n",
      "[ep 515][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1866 (0.1865)  loss_ce: 0.1866 (0.1865)  loss_ce_unscaled: 0.1866 (0.1865)  loss_point_unscaled: 50.9091 (65.6601)\n",
      "[ep 516][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1905 (0.1874)  loss_ce: 0.1905 (0.1874)  loss_ce_unscaled: 0.1905 (0.1874)  loss_point_unscaled: 54.9656 (76.5569)\n",
      "[ep 517][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1890 (0.1899)  loss_ce: 0.1890 (0.1899)  loss_ce_unscaled: 0.1890 (0.1899)  loss_point_unscaled: 52.4316 (86.5823)\n",
      "[ep 518][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1875 (0.1898)  loss_ce: 0.1875 (0.1898)  loss_ce_unscaled: 0.1875 (0.1898)  loss_point_unscaled: 54.5637 (79.2231)\n",
      "[ep 519][lr 0.0001000][3.01s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1813 (0.1810)  loss_ce: 0.1813 (0.1810)  loss_ce_unscaled: 0.1813 (0.1810)  loss_point_unscaled: 50.4243 (67.7411)\n",
      "[ep 520][lr 0.0001000][2.42s]\n",
      "=======================================test=======================================\n",
      "mae: 155.64285714285714 mse: 228.837230410565 time: 2.2936458587646484 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1803 (0.1803)  loss_ce: 0.1803 (0.1803)  loss_ce_unscaled: 0.1803 (0.1803)  loss_point_unscaled: 51.5254 (78.6813)\n",
      "[ep 521][lr 0.0001000][2.99s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1768 (0.1810)  loss_ce: 0.1768 (0.1810)  loss_ce_unscaled: 0.1768 (0.1810)  loss_point_unscaled: 53.1182 (78.9331)\n",
      "[ep 522][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1802 (0.1902)  loss_ce: 0.1802 (0.1902)  loss_ce_unscaled: 0.1802 (0.1902)  loss_point_unscaled: 50.3803 (76.2723)\n",
      "[ep 523][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1784 (0.1883)  loss_ce: 0.1784 (0.1883)  loss_ce_unscaled: 0.1784 (0.1883)  loss_point_unscaled: 58.9478 (104.0560)\n",
      "[ep 524][lr 0.0001000][3.72s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1861 (0.1889)  loss_ce: 0.1861 (0.1889)  loss_ce_unscaled: 0.1861 (0.1889)  loss_point_unscaled: 57.1393 (79.4577)\n",
      "[ep 525][lr 0.0001000][2.57s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1941 (0.1967)  loss_ce: 0.1941 (0.1967)  loss_ce_unscaled: 0.1941 (0.1967)  loss_point_unscaled: 55.9978 (75.6433)\n",
      "[ep 526][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1813 (0.1756)  loss_ce: 0.1813 (0.1756)  loss_ce_unscaled: 0.1813 (0.1756)  loss_point_unscaled: 52.9057 (68.8071)\n",
      "[ep 527][lr 0.0001000][2.84s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1810 (0.1863)  loss_ce: 0.1810 (0.1863)  loss_ce_unscaled: 0.1810 (0.1863)  loss_point_unscaled: 53.0793 (66.3972)\n",
      "[ep 528][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1759 (0.1801)  loss_ce: 0.1759 (0.1801)  loss_ce_unscaled: 0.1759 (0.1801)  loss_point_unscaled: 49.4824 (89.2373)\n",
      "[ep 529][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1870 (0.1974)  loss_ce: 0.1870 (0.1974)  loss_ce_unscaled: 0.1870 (0.1974)  loss_point_unscaled: 53.7486 (76.5286)\n",
      "[ep 530][lr 0.0001000][3.00s]\n",
      "=======================================test=======================================\n",
      "mae: 164.45604395604394 mse: 241.3465544412419 time: 2.258140802383423 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1876 (0.1934)  loss_ce: 0.1876 (0.1934)  loss_ce_unscaled: 0.1876 (0.1934)  loss_point_unscaled: 50.5392 (96.2781)\n",
      "[ep 531][lr 0.0001000][2.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1777 (0.1838)  loss_ce: 0.1777 (0.1838)  loss_ce_unscaled: 0.1777 (0.1838)  loss_point_unscaled: 50.2765 (76.2551)\n",
      "[ep 532][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1825 (0.1931)  loss_ce: 0.1825 (0.1931)  loss_ce_unscaled: 0.1825 (0.1931)  loss_point_unscaled: 54.8991 (80.7983)\n",
      "[ep 533][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1840 (0.1870)  loss_ce: 0.1840 (0.1870)  loss_ce_unscaled: 0.1840 (0.1870)  loss_point_unscaled: 51.6229 (72.5531)\n",
      "[ep 534][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1847 (0.1885)  loss_ce: 0.1847 (0.1885)  loss_ce_unscaled: 0.1847 (0.1885)  loss_point_unscaled: 51.0833 (62.9695)\n",
      "[ep 535][lr 0.0001000][3.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1896 (0.1967)  loss_ce: 0.1896 (0.1967)  loss_ce_unscaled: 0.1896 (0.1967)  loss_point_unscaled: 55.4300 (93.4227)\n",
      "[ep 536][lr 0.0001000][2.51s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1867 (0.1907)  loss_ce: 0.1867 (0.1907)  loss_ce_unscaled: 0.1867 (0.1907)  loss_point_unscaled: 53.3037 (70.1070)\n",
      "[ep 537][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1807 (0.1883)  loss_ce: 0.1807 (0.1883)  loss_ce_unscaled: 0.1807 (0.1883)  loss_point_unscaled: 53.3179 (100.2844)\n",
      "[ep 538][lr 0.0001000][2.51s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1905 (0.1932)  loss_ce: 0.1905 (0.1932)  loss_ce_unscaled: 0.1905 (0.1932)  loss_point_unscaled: 51.5073 (68.5280)\n",
      "[ep 539][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1725 (0.1870)  loss_ce: 0.1725 (0.1870)  loss_ce_unscaled: 0.1725 (0.1870)  loss_point_unscaled: 52.8544 (100.4822)\n",
      "[ep 540][lr 0.0001000][2.42s]\n",
      "=======================================test=======================================\n",
      "mae: 163.73626373626374 mse: 225.0736143312491 time: 4.177691221237183 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1804 (0.1853)  loss_ce: 0.1804 (0.1853)  loss_ce_unscaled: 0.1804 (0.1853)  loss_point_unscaled: 50.1281 (71.9550)\n",
      "[ep 541][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1734 (0.1894)  loss_ce: 0.1734 (0.1894)  loss_ce_unscaled: 0.1734 (0.1894)  loss_point_unscaled: 51.7711 (64.7219)\n",
      "[ep 542][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1927 (0.1938)  loss_ce: 0.1927 (0.1938)  loss_ce_unscaled: 0.1927 (0.1938)  loss_point_unscaled: 54.5969 (78.3068)\n",
      "[ep 543][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1893 (0.1920)  loss_ce: 0.1893 (0.1920)  loss_ce_unscaled: 0.1893 (0.1920)  loss_point_unscaled: 53.8946 (83.7126)\n",
      "[ep 544][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1740 (0.1777)  loss_ce: 0.1740 (0.1777)  loss_ce_unscaled: 0.1740 (0.1777)  loss_point_unscaled: 50.3324 (85.1298)\n",
      "[ep 545][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1898 (0.1922)  loss_ce: 0.1898 (0.1922)  loss_ce_unscaled: 0.1898 (0.1922)  loss_point_unscaled: 52.1911 (93.4512)\n",
      "[ep 546][lr 0.0001000][2.90s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1822 (0.1849)  loss_ce: 0.1822 (0.1849)  loss_ce_unscaled: 0.1822 (0.1849)  loss_point_unscaled: 60.1123 (94.6078)\n",
      "[ep 547][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1825 (0.1866)  loss_ce: 0.1825 (0.1866)  loss_ce_unscaled: 0.1825 (0.1866)  loss_point_unscaled: 51.1657 (69.8783)\n",
      "[ep 548][lr 0.0001000][3.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1774 (0.1792)  loss_ce: 0.1774 (0.1792)  loss_ce_unscaled: 0.1774 (0.1792)  loss_point_unscaled: 58.4980 (74.9958)\n",
      "[ep 549][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1811 (0.1889)  loss_ce: 0.1811 (0.1889)  loss_ce_unscaled: 0.1811 (0.1889)  loss_point_unscaled: 52.6147 (69.1777)\n",
      "[ep 550][lr 0.0001000][3.18s]\n",
      "=======================================test=======================================\n",
      "mae: 166.26923076923077 mse: 252.75341977664954 time: 4.123102426528931 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1807 (0.1836)  loss_ce: 0.1807 (0.1836)  loss_ce_unscaled: 0.1807 (0.1836)  loss_point_unscaled: 51.2427 (74.1731)\n",
      "[ep 551][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1815 (0.1809)  loss_ce: 0.1815 (0.1809)  loss_ce_unscaled: 0.1815 (0.1809)  loss_point_unscaled: 55.0083 (86.5210)\n",
      "[ep 552][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1850 (0.1926)  loss_ce: 0.1850 (0.1926)  loss_ce_unscaled: 0.1850 (0.1926)  loss_point_unscaled: 50.5697 (84.7051)\n",
      "[ep 553][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1860 (0.1901)  loss_ce: 0.1860 (0.1901)  loss_ce_unscaled: 0.1860 (0.1901)  loss_point_unscaled: 52.3740 (78.3133)\n",
      "[ep 554][lr 0.0001000][2.78s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1842 (0.1900)  loss_ce: 0.1842 (0.1900)  loss_ce_unscaled: 0.1842 (0.1900)  loss_point_unscaled: 58.9679 (116.7318)\n",
      "[ep 555][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1973 (0.1949)  loss_ce: 0.1973 (0.1949)  loss_ce_unscaled: 0.1973 (0.1949)  loss_point_unscaled: 51.8608 (68.5809)\n",
      "[ep 556][lr 0.0001000][2.53s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1795 (0.1930)  loss_ce: 0.1795 (0.1930)  loss_ce_unscaled: 0.1795 (0.1930)  loss_point_unscaled: 56.2506 (87.0535)\n",
      "[ep 557][lr 0.0001000][2.54s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1803 (0.1822)  loss_ce: 0.1803 (0.1822)  loss_ce_unscaled: 0.1803 (0.1822)  loss_point_unscaled: 54.7812 (86.4345)\n",
      "[ep 558][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1761 (0.1813)  loss_ce: 0.1761 (0.1813)  loss_ce_unscaled: 0.1761 (0.1813)  loss_point_unscaled: 52.3059 (65.7557)\n",
      "[ep 559][lr 0.0001000][2.87s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1803 (0.1847)  loss_ce: 0.1803 (0.1847)  loss_ce_unscaled: 0.1803 (0.1847)  loss_point_unscaled: 52.6592 (60.7472)\n",
      "[ep 560][lr 0.0001000][2.49s]\n",
      "=======================================test=======================================\n",
      "mae: 172.86263736263737 mse: 272.2511263965235 time: 2.3889687061309814 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1892 (0.1897)  loss_ce: 0.1892 (0.1897)  loss_ce_unscaled: 0.1892 (0.1897)  loss_point_unscaled: 60.2444 (91.7233)\n",
      "[ep 561][lr 0.0001000][3.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1889)  loss_ce: 0.1809 (0.1889)  loss_ce_unscaled: 0.1809 (0.1889)  loss_point_unscaled: 49.9969 (81.8961)\n",
      "[ep 562][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1820 (0.1845)  loss_ce: 0.1820 (0.1845)  loss_ce_unscaled: 0.1820 (0.1845)  loss_point_unscaled: 55.3862 (113.7237)\n",
      "[ep 563][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1856 (0.1921)  loss_ce: 0.1856 (0.1921)  loss_ce_unscaled: 0.1856 (0.1921)  loss_point_unscaled: 49.2720 (101.4593)\n",
      "[ep 564][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1737 (0.1815)  loss_ce: 0.1737 (0.1815)  loss_ce_unscaled: 0.1737 (0.1815)  loss_point_unscaled: 51.9930 (78.7136)\n",
      "[ep 565][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1859 (0.1884)  loss_ce: 0.1859 (0.1884)  loss_ce_unscaled: 0.1859 (0.1884)  loss_point_unscaled: 53.8277 (119.7150)\n",
      "[ep 566][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1883 (0.1858)  loss_ce: 0.1883 (0.1858)  loss_ce_unscaled: 0.1883 (0.1858)  loss_point_unscaled: 51.9530 (64.6129)\n",
      "[ep 567][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1890 (0.1941)  loss_ce: 0.1890 (0.1941)  loss_ce_unscaled: 0.1890 (0.1941)  loss_point_unscaled: 51.3275 (75.7069)\n",
      "[ep 568][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1782 (0.1810)  loss_ce: 0.1782 (0.1810)  loss_ce_unscaled: 0.1782 (0.1810)  loss_point_unscaled: 48.2562 (60.5927)\n",
      "[ep 569][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1819 (0.1894)  loss_ce: 0.1819 (0.1894)  loss_ce_unscaled: 0.1819 (0.1894)  loss_point_unscaled: 54.0599 (93.8735)\n",
      "[ep 570][lr 0.0001000][3.23s]\n",
      "=======================================test=======================================\n",
      "mae: 152.34615384615384 mse: 220.10385910112706 time: 2.716367721557617 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1817 (0.1843)  loss_ce: 0.1817 (0.1843)  loss_ce_unscaled: 0.1817 (0.1843)  loss_point_unscaled: 50.9197 (56.5306)\n",
      "[ep 571][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1871 (0.1867)  loss_ce: 0.1871 (0.1867)  loss_ce_unscaled: 0.1871 (0.1867)  loss_point_unscaled: 52.5346 (74.6971)\n",
      "[ep 572][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1799 (0.1836)  loss_ce: 0.1799 (0.1836)  loss_ce_unscaled: 0.1799 (0.1836)  loss_point_unscaled: 59.1935 (100.9306)\n",
      "[ep 573][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1761 (0.1825)  loss_ce: 0.1761 (0.1825)  loss_ce_unscaled: 0.1761 (0.1825)  loss_point_unscaled: 60.1621 (83.0201)\n",
      "[ep 574][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1946 (0.1901)  loss_ce: 0.1946 (0.1901)  loss_ce_unscaled: 0.1946 (0.1901)  loss_point_unscaled: 50.1664 (70.4697)\n",
      "[ep 575][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1880 (0.1888)  loss_ce: 0.1880 (0.1888)  loss_ce_unscaled: 0.1880 (0.1888)  loss_point_unscaled: 54.7565 (85.4802)\n",
      "[ep 576][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1852 (0.1836)  loss_ce: 0.1852 (0.1836)  loss_ce_unscaled: 0.1852 (0.1836)  loss_point_unscaled: 56.7424 (64.9357)\n",
      "[ep 577][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1822 (0.1903)  loss_ce: 0.1822 (0.1903)  loss_ce_unscaled: 0.1822 (0.1903)  loss_point_unscaled: 51.6259 (66.1319)\n",
      "[ep 578][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1883 (0.1845)  loss_ce: 0.1883 (0.1845)  loss_ce_unscaled: 0.1883 (0.1845)  loss_point_unscaled: 51.0767 (69.0526)\n",
      "[ep 579][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1828 (0.1862)  loss_ce: 0.1828 (0.1862)  loss_ce_unscaled: 0.1828 (0.1862)  loss_point_unscaled: 51.2700 (83.8080)\n",
      "[ep 580][lr 0.0001000][2.44s]\n",
      "=======================================test=======================================\n",
      "mae: 152.5934065934066 mse: 232.16262299621684 time: 2.356649160385132 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1956 (0.1897)  loss_ce: 0.1956 (0.1897)  loss_ce_unscaled: 0.1956 (0.1897)  loss_point_unscaled: 51.2403 (78.4195)\n",
      "[ep 581][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1905 (0.1919)  loss_ce: 0.1905 (0.1919)  loss_ce_unscaled: 0.1905 (0.1919)  loss_point_unscaled: 53.5789 (82.8668)\n",
      "[ep 582][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1818 (0.1806)  loss_ce: 0.1818 (0.1806)  loss_ce_unscaled: 0.1818 (0.1806)  loss_point_unscaled: 49.2851 (87.4880)\n",
      "[ep 583][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1947 (0.1969)  loss_ce: 0.1947 (0.1969)  loss_ce_unscaled: 0.1947 (0.1969)  loss_point_unscaled: 54.4128 (96.3210)\n",
      "[ep 584][lr 0.0001000][2.51s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1855 (0.1910)  loss_ce: 0.1855 (0.1910)  loss_ce_unscaled: 0.1855 (0.1910)  loss_point_unscaled: 51.4279 (65.6323)\n",
      "[ep 585][lr 0.0001000][2.57s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1890 (0.1904)  loss_ce: 0.1890 (0.1904)  loss_ce_unscaled: 0.1890 (0.1904)  loss_point_unscaled: 52.6548 (86.8591)\n",
      "[ep 586][lr 0.0001000][3.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1875)  loss_ce: 0.1839 (0.1875)  loss_ce_unscaled: 0.1839 (0.1875)  loss_point_unscaled: 51.0622 (104.7579)\n",
      "[ep 587][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1790 (0.1875)  loss_ce: 0.1790 (0.1875)  loss_ce_unscaled: 0.1790 (0.1875)  loss_point_unscaled: 50.9641 (69.5060)\n",
      "[ep 588][lr 0.0001000][3.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1925 (0.1893)  loss_ce: 0.1925 (0.1893)  loss_ce_unscaled: 0.1925 (0.1893)  loss_point_unscaled: 49.9191 (116.6859)\n",
      "[ep 589][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1768 (0.1894)  loss_ce: 0.1768 (0.1894)  loss_ce_unscaled: 0.1768 (0.1894)  loss_point_unscaled: 50.0914 (108.0005)\n",
      "[ep 590][lr 0.0001000][2.43s]\n",
      "=======================================test=======================================\n",
      "mae: 179.46153846153845 mse: 268.2250981856532 time: 2.2395501136779785 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1768 (0.1805)  loss_ce: 0.1768 (0.1805)  loss_ce_unscaled: 0.1768 (0.1805)  loss_point_unscaled: 53.4202 (78.2977)\n",
      "[ep 591][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1838 (0.1908)  loss_ce: 0.1838 (0.1908)  loss_ce_unscaled: 0.1838 (0.1908)  loss_point_unscaled: 54.3433 (93.6570)\n",
      "[ep 592][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1931)  loss_ce: 0.1884 (0.1931)  loss_ce_unscaled: 0.1884 (0.1931)  loss_point_unscaled: 50.4534 (82.4828)\n",
      "[ep 593][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1827 (0.1858)  loss_ce: 0.1827 (0.1858)  loss_ce_unscaled: 0.1827 (0.1858)  loss_point_unscaled: 50.7147 (80.7987)\n",
      "[ep 594][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1830 (0.1935)  loss_ce: 0.1830 (0.1935)  loss_ce_unscaled: 0.1830 (0.1935)  loss_point_unscaled: 49.1098 (80.4910)\n",
      "[ep 595][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1843)  loss_ce: 0.1809 (0.1843)  loss_ce_unscaled: 0.1809 (0.1843)  loss_point_unscaled: 47.6651 (55.4038)\n",
      "[ep 596][lr 0.0001000][2.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1937 (0.1887)  loss_ce: 0.1937 (0.1887)  loss_ce_unscaled: 0.1937 (0.1887)  loss_point_unscaled: 55.3735 (89.2449)\n",
      "[ep 597][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1772 (0.1862)  loss_ce: 0.1772 (0.1862)  loss_ce_unscaled: 0.1772 (0.1862)  loss_point_unscaled: 50.0984 (72.6460)\n",
      "[ep 598][lr 0.0001000][2.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1903)  loss_ce: 0.1872 (0.1903)  loss_ce_unscaled: 0.1872 (0.1903)  loss_point_unscaled: 50.7028 (55.6772)\n",
      "[ep 599][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1869 (0.1855)  loss_ce: 0.1869 (0.1855)  loss_ce_unscaled: 0.1869 (0.1855)  loss_point_unscaled: 50.9382 (76.3964)\n",
      "[ep 600][lr 0.0001000][2.42s]\n",
      "=======================================test=======================================\n",
      "mae: 175.91758241758242 mse: 261.22964719079755 time: 2.232313394546509 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1787 (0.1911)  loss_ce: 0.1787 (0.1911)  loss_ce_unscaled: 0.1787 (0.1911)  loss_point_unscaled: 51.2192 (81.8686)\n",
      "[ep 601][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1835 (0.1947)  loss_ce: 0.1835 (0.1947)  loss_ce_unscaled: 0.1835 (0.1947)  loss_point_unscaled: 51.1816 (103.7665)\n",
      "[ep 602][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1841 (0.1894)  loss_ce: 0.1841 (0.1894)  loss_ce_unscaled: 0.1841 (0.1894)  loss_point_unscaled: 50.6394 (71.0374)\n",
      "[ep 603][lr 0.0001000][2.73s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1925 (0.1860)  loss_ce: 0.1925 (0.1860)  loss_ce_unscaled: 0.1925 (0.1860)  loss_point_unscaled: 52.6401 (87.3758)\n",
      "[ep 604][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1864 (0.1948)  loss_ce: 0.1864 (0.1948)  loss_ce_unscaled: 0.1864 (0.1948)  loss_point_unscaled: 49.1537 (70.1523)\n",
      "[ep 605][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1935)  loss_ce: 0.1884 (0.1935)  loss_ce_unscaled: 0.1884 (0.1935)  loss_point_unscaled: 51.6667 (111.1991)\n",
      "[ep 606][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1782 (0.1928)  loss_ce: 0.1782 (0.1928)  loss_ce_unscaled: 0.1782 (0.1928)  loss_point_unscaled: 52.4169 (127.9627)\n",
      "[ep 607][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1837 (0.1916)  loss_ce: 0.1837 (0.1916)  loss_ce_unscaled: 0.1837 (0.1916)  loss_point_unscaled: 51.9943 (65.5541)\n",
      "[ep 608][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1802 (0.1955)  loss_ce: 0.1802 (0.1955)  loss_ce_unscaled: 0.1802 (0.1955)  loss_point_unscaled: 52.5520 (89.7421)\n",
      "[ep 609][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1936)  loss_ce: 0.1823 (0.1936)  loss_ce_unscaled: 0.1823 (0.1936)  loss_point_unscaled: 49.6662 (100.0058)\n",
      "[ep 610][lr 0.0001000][3.31s]\n",
      "=======================================test=======================================\n",
      "mae: 173.25824175824175 mse: 277.47603599130963 time: 2.271028995513916 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1795 (0.1837)  loss_ce: 0.1795 (0.1837)  loss_ce_unscaled: 0.1795 (0.1837)  loss_point_unscaled: 49.0604 (62.5263)\n",
      "[ep 611][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1795 (0.1851)  loss_ce: 0.1795 (0.1851)  loss_ce_unscaled: 0.1795 (0.1851)  loss_point_unscaled: 51.1256 (92.2216)\n",
      "[ep 612][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1747 (0.1827)  loss_ce: 0.1747 (0.1827)  loss_ce_unscaled: 0.1747 (0.1827)  loss_point_unscaled: 52.8851 (108.2786)\n",
      "[ep 613][lr 0.0001000][2.98s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1713 (0.1806)  loss_ce: 0.1713 (0.1806)  loss_ce_unscaled: 0.1713 (0.1806)  loss_point_unscaled: 56.7015 (85.2286)\n",
      "[ep 614][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1807 (0.1798)  loss_ce: 0.1807 (0.1798)  loss_ce_unscaled: 0.1807 (0.1798)  loss_point_unscaled: 49.8940 (59.0764)\n",
      "[ep 615][lr 0.0001000][2.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1848 (0.1881)  loss_ce: 0.1848 (0.1881)  loss_ce_unscaled: 0.1848 (0.1881)  loss_point_unscaled: 53.1395 (78.3644)\n",
      "[ep 616][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1768 (0.1823)  loss_ce: 0.1768 (0.1823)  loss_ce_unscaled: 0.1768 (0.1823)  loss_point_unscaled: 48.2702 (66.3939)\n",
      "[ep 617][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1928 (0.1906)  loss_ce: 0.1928 (0.1906)  loss_ce_unscaled: 0.1928 (0.1906)  loss_point_unscaled: 51.1024 (85.2978)\n",
      "[ep 618][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1815 (0.1914)  loss_ce: 0.1815 (0.1914)  loss_ce_unscaled: 0.1815 (0.1914)  loss_point_unscaled: 58.8765 (88.9514)\n",
      "[ep 619][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1878 (0.1910)  loss_ce: 0.1878 (0.1910)  loss_ce_unscaled: 0.1878 (0.1910)  loss_point_unscaled: 49.8425 (63.7081)\n",
      "[ep 620][lr 0.0001000][2.66s]\n",
      "=======================================test=======================================\n",
      "mae: 144.32967032967034 mse: 228.61758462550512 time: 4.224788665771484 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1857 (0.1840)  loss_ce: 0.1857 (0.1840)  loss_ce_unscaled: 0.1857 (0.1840)  loss_point_unscaled: 54.5470 (105.6447)\n",
      "[ep 621][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1903 (0.1884)  loss_ce: 0.1903 (0.1884)  loss_ce_unscaled: 0.1903 (0.1884)  loss_point_unscaled: 52.6788 (75.5970)\n",
      "[ep 622][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1915 (0.1931)  loss_ce: 0.1915 (0.1931)  loss_ce_unscaled: 0.1915 (0.1931)  loss_point_unscaled: 54.4830 (70.0268)\n",
      "[ep 623][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1751 (0.1776)  loss_ce: 0.1751 (0.1776)  loss_ce_unscaled: 0.1751 (0.1776)  loss_point_unscaled: 52.5128 (66.8765)\n",
      "[ep 624][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1888 (0.1962)  loss_ce: 0.1888 (0.1962)  loss_ce_unscaled: 0.1888 (0.1962)  loss_point_unscaled: 50.8047 (70.6781)\n",
      "[ep 625][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1894 (0.1916)  loss_ce: 0.1894 (0.1916)  loss_ce_unscaled: 0.1894 (0.1916)  loss_point_unscaled: 51.3464 (67.4486)\n",
      "[ep 626][lr 0.0001000][2.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2037 (0.1996)  loss_ce: 0.2037 (0.1996)  loss_ce_unscaled: 0.2037 (0.1996)  loss_point_unscaled: 61.5479 (93.6023)\n",
      "[ep 627][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1915 (0.1970)  loss_ce: 0.1915 (0.1970)  loss_ce_unscaled: 0.1915 (0.1970)  loss_point_unscaled: 50.3827 (87.3596)\n",
      "[ep 628][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1867 (0.1915)  loss_ce: 0.1867 (0.1915)  loss_ce_unscaled: 0.1867 (0.1915)  loss_point_unscaled: 55.1906 (106.9919)\n",
      "[ep 629][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2018 (0.2036)  loss_ce: 0.2018 (0.2036)  loss_ce_unscaled: 0.2018 (0.2036)  loss_point_unscaled: 51.1012 (79.1163)\n",
      "[ep 630][lr 0.0001000][3.16s]\n",
      "=======================================test=======================================\n",
      "mae: 163.25824175824175 mse: 231.04139536807332 time: 4.14321494102478 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1827 (0.1852)  loss_ce: 0.1827 (0.1852)  loss_ce_unscaled: 0.1827 (0.1852)  loss_point_unscaled: 52.7654 (80.1163)\n",
      "[ep 631][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1814 (0.1876)  loss_ce: 0.1814 (0.1876)  loss_ce_unscaled: 0.1814 (0.1876)  loss_point_unscaled: 54.7277 (82.9772)\n",
      "[ep 632][lr 0.0001000][3.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1874 (0.1886)  loss_ce: 0.1874 (0.1886)  loss_ce_unscaled: 0.1874 (0.1886)  loss_point_unscaled: 52.4102 (82.8183)\n",
      "[ep 633][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1740 (0.1903)  loss_ce: 0.1740 (0.1903)  loss_ce_unscaled: 0.1740 (0.1903)  loss_point_unscaled: 48.5051 (111.8812)\n",
      "[ep 634][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1737 (0.1847)  loss_ce: 0.1737 (0.1847)  loss_ce_unscaled: 0.1737 (0.1847)  loss_point_unscaled: 50.9160 (81.6687)\n",
      "[ep 635][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1828)  loss_ce: 0.1809 (0.1828)  loss_ce_unscaled: 0.1809 (0.1828)  loss_point_unscaled: 52.1917 (64.9306)\n",
      "[ep 636][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1943 (0.1963)  loss_ce: 0.1943 (0.1963)  loss_ce_unscaled: 0.1943 (0.1963)  loss_point_unscaled: 49.9868 (57.1588)\n",
      "[ep 637][lr 0.0001000][2.90s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1764 (0.1839)  loss_ce: 0.1764 (0.1839)  loss_ce_unscaled: 0.1764 (0.1839)  loss_point_unscaled: 53.9912 (72.2581)\n",
      "[ep 638][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1735 (0.1790)  loss_ce: 0.1735 (0.1790)  loss_ce_unscaled: 0.1735 (0.1790)  loss_point_unscaled: 50.1861 (57.4145)\n",
      "[ep 639][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1868 (0.1853)  loss_ce: 0.1868 (0.1853)  loss_ce_unscaled: 0.1868 (0.1853)  loss_point_unscaled: 48.9445 (87.4223)\n",
      "[ep 640][lr 0.0001000][3.34s]\n",
      "=======================================test=======================================\n",
      "mae: 158.82417582417582 mse: 241.21517000752306 time: 4.145149230957031 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1864 (0.1839)  loss_ce: 0.1864 (0.1839)  loss_ce_unscaled: 0.1864 (0.1839)  loss_point_unscaled: 51.7992 (59.6315)\n",
      "[ep 641][lr 0.0001000][2.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1682 (0.1825)  loss_ce: 0.1682 (0.1825)  loss_ce_unscaled: 0.1682 (0.1825)  loss_point_unscaled: 52.2380 (87.2123)\n",
      "[ep 642][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2001 (0.1906)  loss_ce: 0.2001 (0.1906)  loss_ce_unscaled: 0.2001 (0.1906)  loss_point_unscaled: 66.1578 (87.3689)\n",
      "[ep 643][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1902 (0.1905)  loss_ce: 0.1902 (0.1905)  loss_ce_unscaled: 0.1902 (0.1905)  loss_point_unscaled: 55.8841 (109.1431)\n",
      "[ep 644][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1794 (0.1873)  loss_ce: 0.1794 (0.1873)  loss_ce_unscaled: 0.1794 (0.1873)  loss_point_unscaled: 54.5641 (68.8686)\n",
      "[ep 645][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1866 (0.1883)  loss_ce: 0.1866 (0.1883)  loss_ce_unscaled: 0.1866 (0.1883)  loss_point_unscaled: 55.0636 (86.5988)\n",
      "[ep 646][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1821 (0.1839)  loss_ce: 0.1821 (0.1839)  loss_ce_unscaled: 0.1821 (0.1839)  loss_point_unscaled: 51.4173 (70.1675)\n",
      "[ep 647][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1850 (0.1866)  loss_ce: 0.1850 (0.1866)  loss_ce_unscaled: 0.1850 (0.1866)  loss_point_unscaled: 49.8890 (78.8102)\n",
      "[ep 648][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1814 (0.1800)  loss_ce: 0.1814 (0.1800)  loss_ce_unscaled: 0.1814 (0.1800)  loss_point_unscaled: 53.7525 (79.7280)\n",
      "[ep 649][lr 0.0001000][2.65s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1827 (0.1878)  loss_ce: 0.1827 (0.1878)  loss_ce_unscaled: 0.1827 (0.1878)  loss_point_unscaled: 57.6133 (97.0621)\n",
      "[ep 650][lr 0.0001000][2.52s]\n",
      "=======================================test=======================================\n",
      "mae: 165.28021978021977 mse: 245.8630571215262 time: 4.164076328277588 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1787 (0.1848)  loss_ce: 0.1787 (0.1848)  loss_ce_unscaled: 0.1787 (0.1848)  loss_point_unscaled: 53.5355 (89.7048)\n",
      "[ep 651][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2069 (0.2056)  loss_ce: 0.2069 (0.2056)  loss_ce_unscaled: 0.2069 (0.2056)  loss_point_unscaled: 48.3581 (60.1866)\n",
      "[ep 652][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1771 (0.1797)  loss_ce: 0.1771 (0.1797)  loss_ce_unscaled: 0.1771 (0.1797)  loss_point_unscaled: 53.2871 (76.5760)\n",
      "[ep 653][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1849 (0.1850)  loss_ce: 0.1849 (0.1850)  loss_ce_unscaled: 0.1849 (0.1850)  loss_point_unscaled: 52.0578 (78.6761)\n",
      "[ep 654][lr 0.0001000][2.79s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1862 (0.1845)  loss_ce: 0.1862 (0.1845)  loss_ce_unscaled: 0.1862 (0.1845)  loss_point_unscaled: 54.4627 (83.3141)\n",
      "[ep 655][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1745 (0.1857)  loss_ce: 0.1745 (0.1857)  loss_ce_unscaled: 0.1745 (0.1857)  loss_point_unscaled: 53.0374 (77.3280)\n",
      "[ep 656][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1848 (0.1859)  loss_ce: 0.1848 (0.1859)  loss_ce_unscaled: 0.1848 (0.1859)  loss_point_unscaled: 59.0126 (76.8510)\n",
      "[ep 657][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1894 (0.1917)  loss_ce: 0.1894 (0.1917)  loss_ce_unscaled: 0.1894 (0.1917)  loss_point_unscaled: 50.7592 (115.3640)\n",
      "[ep 658][lr 0.0001000][2.60s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1892 (0.1897)  loss_ce: 0.1892 (0.1897)  loss_ce_unscaled: 0.1892 (0.1897)  loss_point_unscaled: 52.4718 (98.4377)\n",
      "[ep 659][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1730 (0.1862)  loss_ce: 0.1730 (0.1862)  loss_ce_unscaled: 0.1730 (0.1862)  loss_point_unscaled: 50.7134 (94.2890)\n",
      "[ep 660][lr 0.0001000][3.37s]\n",
      "=======================================test=======================================\n",
      "mae: 146.2087912087912 mse: 218.946528672145 time: 4.157858371734619 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1789 (0.1807)  loss_ce: 0.1789 (0.1807)  loss_ce_unscaled: 0.1789 (0.1807)  loss_point_unscaled: 53.3708 (65.9176)\n",
      "[ep 661][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1833 (0.1915)  loss_ce: 0.1833 (0.1915)  loss_ce_unscaled: 0.1833 (0.1915)  loss_point_unscaled: 52.3774 (98.2039)\n",
      "[ep 662][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1854 (0.1896)  loss_ce: 0.1854 (0.1896)  loss_ce_unscaled: 0.1854 (0.1896)  loss_point_unscaled: 51.8259 (86.8950)\n",
      "[ep 663][lr 0.0001000][2.63s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1709 (0.1849)  loss_ce: 0.1709 (0.1849)  loss_ce_unscaled: 0.1709 (0.1849)  loss_point_unscaled: 48.6775 (88.7743)\n",
      "[ep 664][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1916 (0.1893)  loss_ce: 0.1916 (0.1893)  loss_ce_unscaled: 0.1916 (0.1893)  loss_point_unscaled: 50.0262 (55.8549)\n",
      "[ep 665][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1789 (0.1796)  loss_ce: 0.1789 (0.1796)  loss_ce_unscaled: 0.1789 (0.1796)  loss_point_unscaled: 51.5914 (58.4616)\n",
      "[ep 666][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1837 (0.1851)  loss_ce: 0.1837 (0.1851)  loss_ce_unscaled: 0.1837 (0.1851)  loss_point_unscaled: 51.2272 (81.4732)\n",
      "[ep 667][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1862 (0.1940)  loss_ce: 0.1862 (0.1940)  loss_ce_unscaled: 0.1862 (0.1940)  loss_point_unscaled: 51.8795 (64.5631)\n",
      "[ep 668][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1940 (0.1933)  loss_ce: 0.1940 (0.1933)  loss_ce_unscaled: 0.1940 (0.1933)  loss_point_unscaled: 50.8688 (62.0503)\n",
      "[ep 669][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1942 (0.1921)  loss_ce: 0.1942 (0.1921)  loss_ce_unscaled: 0.1942 (0.1921)  loss_point_unscaled: 50.6727 (65.8442)\n",
      "[ep 670][lr 0.0001000][3.17s]\n",
      "=======================================test=======================================\n",
      "mae: 163.65384615384616 mse: 243.9436185084353 time: 4.097045183181763 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1704 (0.1824)  loss_ce: 0.1704 (0.1824)  loss_ce_unscaled: 0.1704 (0.1824)  loss_point_unscaled: 52.3389 (79.9683)\n",
      "[ep 671][lr 0.0001000][2.97s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1890)  loss_ce: 0.1884 (0.1890)  loss_ce_unscaled: 0.1884 (0.1890)  loss_point_unscaled: 55.6989 (90.3744)\n",
      "[ep 672][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1746 (0.1837)  loss_ce: 0.1746 (0.1837)  loss_ce_unscaled: 0.1746 (0.1837)  loss_point_unscaled: 53.7209 (129.4757)\n",
      "[ep 673][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1876)  loss_ce: 0.1865 (0.1876)  loss_ce_unscaled: 0.1865 (0.1876)  loss_point_unscaled: 54.0477 (96.4553)\n",
      "[ep 674][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1670 (0.1851)  loss_ce: 0.1670 (0.1851)  loss_ce_unscaled: 0.1670 (0.1851)  loss_point_unscaled: 72.8095 (101.8727)\n",
      "[ep 675][lr 0.0001000][2.81s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1871 (0.1921)  loss_ce: 0.1871 (0.1921)  loss_ce_unscaled: 0.1871 (0.1921)  loss_point_unscaled: 50.9051 (82.6874)\n",
      "[ep 676][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1779 (0.1881)  loss_ce: 0.1779 (0.1881)  loss_ce_unscaled: 0.1779 (0.1881)  loss_point_unscaled: 50.3209 (83.7559)\n",
      "[ep 677][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1759 (0.1821)  loss_ce: 0.1759 (0.1821)  loss_ce_unscaled: 0.1759 (0.1821)  loss_point_unscaled: 52.5211 (79.0854)\n",
      "[ep 678][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1852)  loss_ce: 0.1755 (0.1852)  loss_ce_unscaled: 0.1755 (0.1852)  loss_point_unscaled: 53.9399 (75.9827)\n",
      "[ep 679][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1767 (0.1786)  loss_ce: 0.1767 (0.1786)  loss_ce_unscaled: 0.1767 (0.1786)  loss_point_unscaled: 51.5354 (82.3736)\n",
      "[ep 680][lr 0.0001000][3.09s]\n",
      "=======================================test=======================================\n",
      "mae: 167.9945054945055 mse: 236.49641634717204 time: 4.151321887969971 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1798 (0.1893)  loss_ce: 0.1798 (0.1893)  loss_ce_unscaled: 0.1798 (0.1893)  loss_point_unscaled: 51.3669 (91.6927)\n",
      "[ep 681][lr 0.0001000][2.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1790 (0.1874)  loss_ce: 0.1790 (0.1874)  loss_ce_unscaled: 0.1790 (0.1874)  loss_point_unscaled: 53.0403 (97.7950)\n",
      "[ep 682][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1887 (0.1903)  loss_ce: 0.1887 (0.1903)  loss_ce_unscaled: 0.1887 (0.1903)  loss_point_unscaled: 51.8958 (77.1161)\n",
      "[ep 683][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1682 (0.1818)  loss_ce: 0.1682 (0.1818)  loss_ce_unscaled: 0.1682 (0.1818)  loss_point_unscaled: 49.6435 (88.2342)\n",
      "[ep 684][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1842 (0.1858)  loss_ce: 0.1842 (0.1858)  loss_ce_unscaled: 0.1842 (0.1858)  loss_point_unscaled: 51.5901 (90.7517)\n",
      "[ep 685][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1850 (0.1872)  loss_ce: 0.1850 (0.1872)  loss_ce_unscaled: 0.1850 (0.1872)  loss_point_unscaled: 57.5068 (136.3644)\n",
      "[ep 686][lr 0.0001000][3.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1804 (0.1862)  loss_ce: 0.1804 (0.1862)  loss_ce_unscaled: 0.1804 (0.1862)  loss_point_unscaled: 52.3918 (69.5149)\n",
      "[ep 687][lr 0.0001000][2.87s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1719 (0.1783)  loss_ce: 0.1719 (0.1783)  loss_ce_unscaled: 0.1719 (0.1783)  loss_point_unscaled: 68.7659 (113.8640)\n",
      "[ep 688][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1745 (0.1773)  loss_ce: 0.1745 (0.1773)  loss_ce_unscaled: 0.1745 (0.1773)  loss_point_unscaled: 49.1690 (57.8381)\n",
      "[ep 689][lr 0.0001000][2.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1862 (0.1917)  loss_ce: 0.1862 (0.1917)  loss_ce_unscaled: 0.1862 (0.1917)  loss_point_unscaled: 49.7676 (66.5227)\n",
      "[ep 690][lr 0.0001000][2.54s]\n",
      "=======================================test=======================================\n",
      "mae: 160.78021978021977 mse: 231.22521251173907 time: 2.278547525405884 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1786 (0.1815)  loss_ce: 0.1786 (0.1815)  loss_ce_unscaled: 0.1786 (0.1815)  loss_point_unscaled: 53.1608 (67.6943)\n",
      "[ep 691][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1837 (0.1860)  loss_ce: 0.1837 (0.1860)  loss_ce_unscaled: 0.1837 (0.1860)  loss_point_unscaled: 49.6058 (70.4167)\n",
      "[ep 692][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1957 (0.2029)  loss_ce: 0.1957 (0.2029)  loss_ce_unscaled: 0.1957 (0.2029)  loss_point_unscaled: 52.9840 (130.6570)\n",
      "[ep 693][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1852 (0.1896)  loss_ce: 0.1852 (0.1896)  loss_ce_unscaled: 0.1852 (0.1896)  loss_point_unscaled: 53.8214 (86.8292)\n",
      "[ep 694][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1762 (0.1871)  loss_ce: 0.1762 (0.1871)  loss_ce_unscaled: 0.1762 (0.1871)  loss_point_unscaled: 49.2562 (68.4627)\n",
      "[ep 695][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1893 (0.1883)  loss_ce: 0.1893 (0.1883)  loss_ce_unscaled: 0.1893 (0.1883)  loss_point_unscaled: 55.0638 (101.1116)\n",
      "[ep 696][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1841 (0.1888)  loss_ce: 0.1841 (0.1888)  loss_ce_unscaled: 0.1841 (0.1888)  loss_point_unscaled: 48.5377 (78.4306)\n",
      "[ep 697][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1797 (0.1856)  loss_ce: 0.1797 (0.1856)  loss_ce_unscaled: 0.1797 (0.1856)  loss_point_unscaled: 49.6908 (68.2020)\n",
      "[ep 698][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1849 (0.1990)  loss_ce: 0.1849 (0.1990)  loss_ce_unscaled: 0.1849 (0.1990)  loss_point_unscaled: 51.3676 (77.9137)\n",
      "[ep 699][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1899 (0.1962)  loss_ce: 0.1899 (0.1962)  loss_ce_unscaled: 0.1899 (0.1962)  loss_point_unscaled: 51.5633 (70.2757)\n",
      "[ep 700][lr 0.0001000][3.40s]\n",
      "=======================================test=======================================\n",
      "mae: 144.62087912087912 mse: 223.74780986097366 time: 2.2557554244995117 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1861)  loss_ce: 0.1839 (0.1861)  loss_ce_unscaled: 0.1839 (0.1861)  loss_point_unscaled: 61.0290 (102.5328)\n",
      "[ep 701][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1800 (0.1851)  loss_ce: 0.1800 (0.1851)  loss_ce_unscaled: 0.1800 (0.1851)  loss_point_unscaled: 51.8266 (77.8767)\n",
      "[ep 702][lr 0.0001000][3.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1828 (0.1857)  loss_ce: 0.1828 (0.1857)  loss_ce_unscaled: 0.1828 (0.1857)  loss_point_unscaled: 50.6163 (60.4398)\n",
      "[ep 703][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1819 (0.1894)  loss_ce: 0.1819 (0.1894)  loss_ce_unscaled: 0.1819 (0.1894)  loss_point_unscaled: 51.6509 (81.6911)\n",
      "[ep 704][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1804 (0.1879)  loss_ce: 0.1804 (0.1879)  loss_ce_unscaled: 0.1804 (0.1879)  loss_point_unscaled: 50.6079 (71.2991)\n",
      "[ep 705][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1854 (0.1903)  loss_ce: 0.1854 (0.1903)  loss_ce_unscaled: 0.1854 (0.1903)  loss_point_unscaled: 55.1509 (70.4200)\n",
      "[ep 706][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1848 (0.1881)  loss_ce: 0.1848 (0.1881)  loss_ce_unscaled: 0.1848 (0.1881)  loss_point_unscaled: 54.0568 (79.5678)\n",
      "[ep 707][lr 0.0001000][3.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1749 (0.1870)  loss_ce: 0.1749 (0.1870)  loss_ce_unscaled: 0.1749 (0.1870)  loss_point_unscaled: 51.3346 (63.9694)\n",
      "[ep 708][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1920 (0.1936)  loss_ce: 0.1920 (0.1936)  loss_ce_unscaled: 0.1920 (0.1936)  loss_point_unscaled: 52.4853 (93.6536)\n",
      "[ep 709][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1924 (0.1909)  loss_ce: 0.1924 (0.1909)  loss_ce_unscaled: 0.1924 (0.1909)  loss_point_unscaled: 50.7508 (84.4544)\n",
      "[ep 710][lr 0.0001000][2.45s]\n",
      "=======================================test=======================================\n",
      "mae: 140.23626373626374 mse: 216.86921921231294 time: 3.912161111831665 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1920 (0.1869)  loss_ce: 0.1920 (0.1869)  loss_ce_unscaled: 0.1920 (0.1869)  loss_point_unscaled: 49.4218 (65.9612)\n",
      "[ep 711][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1873 (0.1910)  loss_ce: 0.1873 (0.1910)  loss_ce_unscaled: 0.1873 (0.1910)  loss_point_unscaled: 56.5025 (81.6217)\n",
      "[ep 712][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1692 (0.1787)  loss_ce: 0.1692 (0.1787)  loss_ce_unscaled: 0.1692 (0.1787)  loss_point_unscaled: 51.6780 (86.6814)\n",
      "[ep 713][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1834 (0.1857)  loss_ce: 0.1834 (0.1857)  loss_ce_unscaled: 0.1834 (0.1857)  loss_point_unscaled: 54.2687 (94.5739)\n",
      "[ep 714][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1842 (0.1896)  loss_ce: 0.1842 (0.1896)  loss_ce_unscaled: 0.1842 (0.1896)  loss_point_unscaled: 56.3620 (60.6344)\n",
      "[ep 715][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1863 (0.1825)  loss_ce: 0.1863 (0.1825)  loss_ce_unscaled: 0.1863 (0.1825)  loss_point_unscaled: 53.4000 (104.0614)\n",
      "[ep 716][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1842 (0.1795)  loss_ce: 0.1842 (0.1795)  loss_ce_unscaled: 0.1842 (0.1795)  loss_point_unscaled: 50.7690 (63.5553)\n",
      "[ep 717][lr 0.0001000][3.03s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1819 (0.1943)  loss_ce: 0.1819 (0.1943)  loss_ce_unscaled: 0.1819 (0.1943)  loss_point_unscaled: 64.0875 (104.3435)\n",
      "[ep 718][lr 0.0001000][2.98s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1916)  loss_ce: 0.1865 (0.1916)  loss_ce_unscaled: 0.1865 (0.1916)  loss_point_unscaled: 52.1405 (83.8644)\n",
      "[ep 719][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1749 (0.1899)  loss_ce: 0.1749 (0.1899)  loss_ce_unscaled: 0.1749 (0.1899)  loss_point_unscaled: 57.2205 (118.7865)\n",
      "[ep 720][lr 0.0001000][3.40s]\n",
      "=======================================test=======================================\n",
      "mae: 162.17032967032966 mse: 237.27411467436875 time: 2.303630828857422 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1858 (0.1869)  loss_ce: 0.1858 (0.1869)  loss_ce_unscaled: 0.1858 (0.1869)  loss_point_unscaled: 51.8040 (73.2664)\n",
      "[ep 721][lr 0.0001000][2.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1764 (0.1797)  loss_ce: 0.1764 (0.1797)  loss_ce_unscaled: 0.1764 (0.1797)  loss_point_unscaled: 55.9906 (81.5402)\n",
      "[ep 722][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1843 (0.1936)  loss_ce: 0.1843 (0.1936)  loss_ce_unscaled: 0.1843 (0.1936)  loss_point_unscaled: 49.3284 (57.6387)\n",
      "[ep 723][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1837 (0.1824)  loss_ce: 0.1837 (0.1824)  loss_ce_unscaled: 0.1837 (0.1824)  loss_point_unscaled: 51.1049 (59.7949)\n",
      "[ep 724][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1890 (0.1926)  loss_ce: 0.1890 (0.1926)  loss_ce_unscaled: 0.1890 (0.1926)  loss_point_unscaled: 50.8249 (59.4537)\n",
      "[ep 725][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1966 (0.1942)  loss_ce: 0.1966 (0.1942)  loss_ce_unscaled: 0.1966 (0.1942)  loss_point_unscaled: 48.8299 (65.0574)\n",
      "[ep 726][lr 0.0001000][2.81s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1870 (0.1921)  loss_ce: 0.1870 (0.1921)  loss_ce_unscaled: 0.1870 (0.1921)  loss_point_unscaled: 49.8893 (75.5986)\n",
      "[ep 727][lr 0.0001000][2.72s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1749 (0.1846)  loss_ce: 0.1749 (0.1846)  loss_ce_unscaled: 0.1749 (0.1846)  loss_point_unscaled: 54.2254 (80.3747)\n",
      "[ep 728][lr 0.0001000][3.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1795 (0.1832)  loss_ce: 0.1795 (0.1832)  loss_ce_unscaled: 0.1795 (0.1832)  loss_point_unscaled: 55.1244 (79.7281)\n",
      "[ep 729][lr 0.0001000][2.73s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1876 (0.1854)  loss_ce: 0.1876 (0.1854)  loss_ce_unscaled: 0.1876 (0.1854)  loss_point_unscaled: 48.5753 (69.3466)\n",
      "[ep 730][lr 0.0001000][3.26s]\n",
      "=======================================test=======================================\n",
      "mae: 169.92857142857142 mse: 257.2850537849 time: 2.7333686351776123 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1799 (0.1835)  loss_ce: 0.1799 (0.1835)  loss_ce_unscaled: 0.1799 (0.1835)  loss_point_unscaled: 52.1751 (72.1472)\n",
      "[ep 731][lr 0.0001000][2.61s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1831)  loss_ce: 0.1832 (0.1831)  loss_ce_unscaled: 0.1832 (0.1831)  loss_point_unscaled: 52.0154 (79.1301)\n",
      "[ep 732][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1664 (0.1841)  loss_ce: 0.1664 (0.1841)  loss_ce_unscaled: 0.1664 (0.1841)  loss_point_unscaled: 53.2272 (66.4952)\n",
      "[ep 733][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1916 (0.1945)  loss_ce: 0.1916 (0.1945)  loss_ce_unscaled: 0.1916 (0.1945)  loss_point_unscaled: 48.3405 (67.2646)\n",
      "[ep 734][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1900 (0.1936)  loss_ce: 0.1900 (0.1936)  loss_ce_unscaled: 0.1900 (0.1936)  loss_point_unscaled: 52.5059 (73.9563)\n",
      "[ep 735][lr 0.0001000][3.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1817 (0.1795)  loss_ce: 0.1817 (0.1795)  loss_ce_unscaled: 0.1817 (0.1795)  loss_point_unscaled: 51.3883 (72.0613)\n",
      "[ep 736][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1848 (0.1896)  loss_ce: 0.1848 (0.1896)  loss_ce_unscaled: 0.1848 (0.1896)  loss_point_unscaled: 50.5263 (81.0624)\n",
      "[ep 737][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1759 (0.1819)  loss_ce: 0.1759 (0.1819)  loss_ce_unscaled: 0.1759 (0.1819)  loss_point_unscaled: 52.9146 (94.8868)\n",
      "[ep 738][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1904)  loss_ce: 0.1832 (0.1904)  loss_ce_unscaled: 0.1832 (0.1904)  loss_point_unscaled: 51.6272 (58.9335)\n",
      "[ep 739][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1859 (0.1856)  loss_ce: 0.1859 (0.1856)  loss_ce_unscaled: 0.1859 (0.1856)  loss_point_unscaled: 53.5274 (71.1629)\n",
      "[ep 740][lr 0.0001000][3.22s]\n",
      "=======================================test=======================================\n",
      "mae: 144.1868131868132 mse: 229.7525185503301 time: 4.18324875831604 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1867 (0.1863)  loss_ce: 0.1867 (0.1863)  loss_ce_unscaled: 0.1867 (0.1863)  loss_point_unscaled: 49.9729 (99.7756)\n",
      "[ep 741][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1879 (0.1966)  loss_ce: 0.1879 (0.1966)  loss_ce_unscaled: 0.1879 (0.1966)  loss_point_unscaled: 51.9083 (84.9235)\n",
      "[ep 742][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1780 (0.1828)  loss_ce: 0.1780 (0.1828)  loss_ce_unscaled: 0.1780 (0.1828)  loss_point_unscaled: 54.7924 (133.8604)\n",
      "[ep 743][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1840 (0.1922)  loss_ce: 0.1840 (0.1922)  loss_ce_unscaled: 0.1840 (0.1922)  loss_point_unscaled: 52.3722 (95.4313)\n",
      "[ep 744][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1842 (0.1879)  loss_ce: 0.1842 (0.1879)  loss_ce_unscaled: 0.1842 (0.1879)  loss_point_unscaled: 55.5921 (71.4906)\n",
      "[ep 745][lr 0.0001000][2.83s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1913 (0.1882)  loss_ce: 0.1913 (0.1882)  loss_ce_unscaled: 0.1913 (0.1882)  loss_point_unscaled: 55.2335 (75.3029)\n",
      "[ep 746][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1786 (0.1890)  loss_ce: 0.1786 (0.1890)  loss_ce_unscaled: 0.1786 (0.1890)  loss_point_unscaled: 53.2793 (69.4393)\n",
      "[ep 747][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1838 (0.1857)  loss_ce: 0.1838 (0.1857)  loss_ce_unscaled: 0.1838 (0.1857)  loss_point_unscaled: 53.1422 (62.2901)\n",
      "[ep 748][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1720 (0.1842)  loss_ce: 0.1720 (0.1842)  loss_ce_unscaled: 0.1720 (0.1842)  loss_point_unscaled: 51.2956 (82.0253)\n",
      "[ep 749][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1817 (0.1856)  loss_ce: 0.1817 (0.1856)  loss_ce_unscaled: 0.1817 (0.1856)  loss_point_unscaled: 51.7428 (66.2721)\n",
      "[ep 750][lr 0.0001000][2.34s]\n",
      "=======================================test=======================================\n",
      "mae: 142.6153846153846 mse: 203.92201159557743 time: 2.2923529148101807 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1857)  loss_ce: 0.1781 (0.1857)  loss_ce_unscaled: 0.1781 (0.1857)  loss_point_unscaled: 57.4870 (76.9990)\n",
      "[ep 751][lr 0.0001000][2.84s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1875 (0.1931)  loss_ce: 0.1875 (0.1931)  loss_ce_unscaled: 0.1875 (0.1931)  loss_point_unscaled: 52.2420 (89.2129)\n",
      "[ep 752][lr 0.0001000][3.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1745 (0.1821)  loss_ce: 0.1745 (0.1821)  loss_ce_unscaled: 0.1745 (0.1821)  loss_point_unscaled: 53.6231 (73.7748)\n",
      "[ep 753][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1803 (0.1820)  loss_ce: 0.1803 (0.1820)  loss_ce_unscaled: 0.1803 (0.1820)  loss_point_unscaled: 54.6045 (69.0146)\n",
      "[ep 754][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1775 (0.1775)  loss_ce: 0.1775 (0.1775)  loss_ce_unscaled: 0.1775 (0.1775)  loss_point_unscaled: 51.7646 (58.4902)\n",
      "[ep 755][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1815 (0.1887)  loss_ce: 0.1815 (0.1887)  loss_ce_unscaled: 0.1815 (0.1887)  loss_point_unscaled: 52.9035 (76.0522)\n",
      "[ep 756][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1882 (0.1881)  loss_ce: 0.1882 (0.1881)  loss_ce_unscaled: 0.1882 (0.1881)  loss_point_unscaled: 53.0597 (120.6952)\n",
      "[ep 757][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1770 (0.1840)  loss_ce: 0.1770 (0.1840)  loss_ce_unscaled: 0.1770 (0.1840)  loss_point_unscaled: 52.5623 (57.9403)\n",
      "[ep 758][lr 0.0001000][2.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1752 (0.1872)  loss_ce: 0.1752 (0.1872)  loss_ce_unscaled: 0.1752 (0.1872)  loss_point_unscaled: 53.6158 (85.5173)\n",
      "[ep 759][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1797 (0.1855)  loss_ce: 0.1797 (0.1855)  loss_ce_unscaled: 0.1797 (0.1855)  loss_point_unscaled: 54.6859 (62.6676)\n",
      "[ep 760][lr 0.0001000][3.12s]\n",
      "=======================================test=======================================\n",
      "mae: 182.51098901098902 mse: 268.6831235472762 time: 3.139113426208496 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1908 (0.1898)  loss_ce: 0.1908 (0.1898)  loss_ce_unscaled: 0.1908 (0.1898)  loss_point_unscaled: 51.6362 (68.3581)\n",
      "[ep 761][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1796 (0.1853)  loss_ce: 0.1796 (0.1853)  loss_ce_unscaled: 0.1796 (0.1853)  loss_point_unscaled: 53.5234 (88.1499)\n",
      "[ep 762][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1791 (0.1830)  loss_ce: 0.1791 (0.1830)  loss_ce_unscaled: 0.1791 (0.1830)  loss_point_unscaled: 53.9080 (92.1148)\n",
      "[ep 763][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1814 (0.1847)  loss_ce: 0.1814 (0.1847)  loss_ce_unscaled: 0.1814 (0.1847)  loss_point_unscaled: 54.1040 (87.0007)\n",
      "[ep 764][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1909)  loss_ce: 0.1839 (0.1909)  loss_ce_unscaled: 0.1839 (0.1909)  loss_point_unscaled: 51.9413 (106.5677)\n",
      "[ep 765][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1776 (0.1851)  loss_ce: 0.1776 (0.1851)  loss_ce_unscaled: 0.1776 (0.1851)  loss_point_unscaled: 55.9219 (65.0576)\n",
      "[ep 766][lr 0.0001000][3.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1827 (0.1887)  loss_ce: 0.1827 (0.1887)  loss_ce_unscaled: 0.1827 (0.1887)  loss_point_unscaled: 48.1715 (62.1374)\n",
      "[ep 767][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1836 (0.1844)  loss_ce: 0.1836 (0.1844)  loss_ce_unscaled: 0.1836 (0.1844)  loss_point_unscaled: 50.8630 (55.9528)\n",
      "[ep 768][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1852 (0.1891)  loss_ce: 0.1852 (0.1891)  loss_ce_unscaled: 0.1852 (0.1891)  loss_point_unscaled: 50.9246 (70.0573)\n",
      "[ep 769][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1793 (0.1891)  loss_ce: 0.1793 (0.1891)  loss_ce_unscaled: 0.1793 (0.1891)  loss_point_unscaled: 51.1040 (82.8882)\n",
      "[ep 770][lr 0.0001000][2.49s]\n",
      "=======================================test=======================================\n",
      "mae: 158.92857142857142 mse: 238.146932666498 time: 2.2737581729888916 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1882 (0.1925)  loss_ce: 0.1882 (0.1925)  loss_ce_unscaled: 0.1882 (0.1925)  loss_point_unscaled: 53.1731 (79.6433)\n",
      "[ep 771][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1875 (0.1891)  loss_ce: 0.1875 (0.1891)  loss_ce_unscaled: 0.1875 (0.1891)  loss_point_unscaled: 54.2603 (86.7850)\n",
      "[ep 772][lr 0.0001000][3.04s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1923 (0.1876)  loss_ce: 0.1923 (0.1876)  loss_ce_unscaled: 0.1923 (0.1876)  loss_point_unscaled: 50.2108 (107.1453)\n",
      "[ep 773][lr 0.0001000][2.81s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1784 (0.1882)  loss_ce: 0.1784 (0.1882)  loss_ce_unscaled: 0.1784 (0.1882)  loss_point_unscaled: 52.4732 (87.6458)\n",
      "[ep 774][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1789 (0.1785)  loss_ce: 0.1789 (0.1785)  loss_ce_unscaled: 0.1789 (0.1785)  loss_point_unscaled: 54.0177 (78.9703)\n",
      "[ep 775][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1842 (0.1918)  loss_ce: 0.1842 (0.1918)  loss_ce_unscaled: 0.1842 (0.1918)  loss_point_unscaled: 49.1179 (57.4383)\n",
      "[ep 776][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1837 (0.1908)  loss_ce: 0.1837 (0.1908)  loss_ce_unscaled: 0.1837 (0.1908)  loss_point_unscaled: 52.1775 (84.0900)\n",
      "[ep 777][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1820)  loss_ce: 0.1832 (0.1820)  loss_ce_unscaled: 0.1832 (0.1820)  loss_point_unscaled: 48.4436 (66.8068)\n",
      "[ep 778][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1843)  loss_ce: 0.1808 (0.1843)  loss_ce_unscaled: 0.1808 (0.1843)  loss_point_unscaled: 51.3121 (105.5018)\n",
      "[ep 779][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1853 (0.1920)  loss_ce: 0.1853 (0.1920)  loss_ce_unscaled: 0.1853 (0.1920)  loss_point_unscaled: 58.2102 (101.8885)\n",
      "[ep 780][lr 0.0001000][3.30s]\n",
      "=======================================test=======================================\n",
      "mae: 215.3901098901099 mse: 312.55904936613075 time: 2.293327569961548 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1803 (0.1860)  loss_ce: 0.1803 (0.1860)  loss_ce_unscaled: 0.1803 (0.1860)  loss_point_unscaled: 51.6110 (68.6734)\n",
      "[ep 781][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1800 (0.1809)  loss_ce: 0.1800 (0.1809)  loss_ce_unscaled: 0.1800 (0.1809)  loss_point_unscaled: 56.8161 (87.1344)\n",
      "[ep 782][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1746 (0.1747)  loss_ce: 0.1746 (0.1747)  loss_ce_unscaled: 0.1746 (0.1747)  loss_point_unscaled: 61.8373 (75.1897)\n",
      "[ep 783][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1848 (0.1890)  loss_ce: 0.1848 (0.1890)  loss_ce_unscaled: 0.1848 (0.1890)  loss_point_unscaled: 53.2534 (72.7085)\n",
      "[ep 784][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1761 (0.1852)  loss_ce: 0.1761 (0.1852)  loss_ce_unscaled: 0.1761 (0.1852)  loss_point_unscaled: 52.6129 (93.3300)\n",
      "[ep 785][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1931 (0.1947)  loss_ce: 0.1931 (0.1947)  loss_ce_unscaled: 0.1931 (0.1947)  loss_point_unscaled: 53.4922 (88.5815)\n",
      "[ep 786][lr 0.0001000][3.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1792 (0.1820)  loss_ce: 0.1792 (0.1820)  loss_ce_unscaled: 0.1792 (0.1820)  loss_point_unscaled: 50.2316 (85.1102)\n",
      "[ep 787][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1834 (0.1894)  loss_ce: 0.1834 (0.1894)  loss_ce_unscaled: 0.1834 (0.1894)  loss_point_unscaled: 55.4040 (85.6974)\n",
      "[ep 788][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1805 (0.1900)  loss_ce: 0.1805 (0.1900)  loss_ce_unscaled: 0.1805 (0.1900)  loss_point_unscaled: 60.1924 (96.6832)\n",
      "[ep 789][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1717 (0.1792)  loss_ce: 0.1717 (0.1792)  loss_ce_unscaled: 0.1717 (0.1792)  loss_point_unscaled: 61.3895 (91.2843)\n",
      "[ep 790][lr 0.0001000][3.05s]\n",
      "=======================================test=======================================\n",
      "mae: 145.74725274725276 mse: 222.76028114801537 time: 2.2833573818206787 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1841 (0.1875)  loss_ce: 0.1841 (0.1875)  loss_ce_unscaled: 0.1841 (0.1875)  loss_point_unscaled: 49.5503 (107.5518)\n",
      "[ep 791][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1716 (0.1780)  loss_ce: 0.1716 (0.1780)  loss_ce_unscaled: 0.1716 (0.1780)  loss_point_unscaled: 54.6894 (60.5068)\n",
      "[ep 792][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1747 (0.1863)  loss_ce: 0.1747 (0.1863)  loss_ce_unscaled: 0.1747 (0.1863)  loss_point_unscaled: 52.0779 (83.7428)\n",
      "[ep 793][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1943 (0.1942)  loss_ce: 0.1943 (0.1942)  loss_ce_unscaled: 0.1943 (0.1942)  loss_point_unscaled: 56.0193 (89.6528)\n",
      "[ep 794][lr 0.0001000][3.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1817 (0.1807)  loss_ce: 0.1817 (0.1807)  loss_ce_unscaled: 0.1817 (0.1807)  loss_point_unscaled: 53.7408 (112.6308)\n",
      "[ep 795][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1847 (0.1916)  loss_ce: 0.1847 (0.1916)  loss_ce_unscaled: 0.1847 (0.1916)  loss_point_unscaled: 52.3505 (89.0234)\n",
      "[ep 796][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1870)  loss_ce: 0.1884 (0.1870)  loss_ce_unscaled: 0.1884 (0.1870)  loss_point_unscaled: 56.8917 (77.1091)\n",
      "[ep 797][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1822 (0.1796)  loss_ce: 0.1822 (0.1796)  loss_ce_unscaled: 0.1822 (0.1796)  loss_point_unscaled: 52.6306 (70.0821)\n",
      "[ep 798][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1883 (0.1869)  loss_ce: 0.1883 (0.1869)  loss_ce_unscaled: 0.1883 (0.1869)  loss_point_unscaled: 55.7306 (94.0303)\n",
      "[ep 799][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1793 (0.1843)  loss_ce: 0.1793 (0.1843)  loss_ce_unscaled: 0.1793 (0.1843)  loss_point_unscaled: 50.5892 (83.4329)\n",
      "[ep 800][lr 0.0001000][2.37s]\n",
      "=======================================test=======================================\n",
      "mae: 155.02197802197801 mse: 251.43443424732962 time: 3.5385005474090576 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1723 (0.1803)  loss_ce: 0.1723 (0.1803)  loss_ce_unscaled: 0.1723 (0.1803)  loss_point_unscaled: 54.3213 (72.2718)\n",
      "[ep 801][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1889 (0.1876)  loss_ce: 0.1889 (0.1876)  loss_ce_unscaled: 0.1889 (0.1876)  loss_point_unscaled: 50.7090 (93.5860)\n",
      "[ep 802][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1731 (0.1810)  loss_ce: 0.1731 (0.1810)  loss_ce_unscaled: 0.1731 (0.1810)  loss_point_unscaled: 54.1410 (77.8430)\n",
      "[ep 803][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1870 (0.1911)  loss_ce: 0.1870 (0.1911)  loss_ce_unscaled: 0.1870 (0.1911)  loss_point_unscaled: 51.5567 (58.3351)\n",
      "[ep 804][lr 0.0001000][2.80s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1824 (0.1836)  loss_ce: 0.1824 (0.1836)  loss_ce_unscaled: 0.1824 (0.1836)  loss_point_unscaled: 50.4348 (59.3501)\n",
      "[ep 805][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1717 (0.1824)  loss_ce: 0.1717 (0.1824)  loss_ce_unscaled: 0.1717 (0.1824)  loss_point_unscaled: 49.4887 (73.5120)\n",
      "[ep 806][lr 0.0001000][2.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1874)  loss_ce: 0.1781 (0.1874)  loss_ce_unscaled: 0.1781 (0.1874)  loss_point_unscaled: 52.4410 (105.6497)\n",
      "[ep 807][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1895 (0.1922)  loss_ce: 0.1895 (0.1922)  loss_ce_unscaled: 0.1895 (0.1922)  loss_point_unscaled: 53.9252 (82.9389)\n",
      "[ep 808][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1864 (0.1880)  loss_ce: 0.1864 (0.1880)  loss_ce_unscaled: 0.1864 (0.1880)  loss_point_unscaled: 60.2574 (87.5079)\n",
      "[ep 809][lr 0.0001000][3.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1768 (0.1802)  loss_ce: 0.1768 (0.1802)  loss_ce_unscaled: 0.1768 (0.1802)  loss_point_unscaled: 52.4731 (64.7609)\n",
      "[ep 810][lr 0.0001000][3.13s]\n",
      "=======================================test=======================================\n",
      "mae: 160.97802197802199 mse: 237.9835620881741 time: 2.2653136253356934 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1783 (0.1827)  loss_ce: 0.1783 (0.1827)  loss_ce_unscaled: 0.1783 (0.1827)  loss_point_unscaled: 53.3952 (103.5562)\n",
      "[ep 811][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1825 (0.1884)  loss_ce: 0.1825 (0.1884)  loss_ce_unscaled: 0.1825 (0.1884)  loss_point_unscaled: 52.1210 (66.2157)\n",
      "[ep 812][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1845 (0.1942)  loss_ce: 0.1845 (0.1942)  loss_ce_unscaled: 0.1845 (0.1942)  loss_point_unscaled: 52.9969 (102.8134)\n",
      "[ep 813][lr 0.0001000][3.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1896 (0.1917)  loss_ce: 0.1896 (0.1917)  loss_ce_unscaled: 0.1896 (0.1917)  loss_point_unscaled: 54.2182 (78.0949)\n",
      "[ep 814][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1858 (0.1887)  loss_ce: 0.1858 (0.1887)  loss_ce_unscaled: 0.1858 (0.1887)  loss_point_unscaled: 54.2274 (100.5965)\n",
      "[ep 815][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1792 (0.1819)  loss_ce: 0.1792 (0.1819)  loss_ce_unscaled: 0.1792 (0.1819)  loss_point_unscaled: 53.1860 (113.1149)\n",
      "[ep 816][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1758 (0.1868)  loss_ce: 0.1758 (0.1868)  loss_ce_unscaled: 0.1758 (0.1868)  loss_point_unscaled: 52.2560 (91.8715)\n",
      "[ep 817][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1848 (0.1908)  loss_ce: 0.1848 (0.1908)  loss_ce_unscaled: 0.1848 (0.1908)  loss_point_unscaled: 52.1305 (92.9995)\n",
      "[ep 818][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1810 (0.1896)  loss_ce: 0.1810 (0.1896)  loss_ce_unscaled: 0.1810 (0.1896)  loss_point_unscaled: 51.0328 (95.8753)\n",
      "[ep 819][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1752 (0.1854)  loss_ce: 0.1752 (0.1854)  loss_ce_unscaled: 0.1752 (0.1854)  loss_point_unscaled: 61.9271 (100.7965)\n",
      "[ep 820][lr 0.0001000][2.54s]\n",
      "=======================================test=======================================\n",
      "mae: 165.41758241758242 mse: 255.48553215956701 time: 4.116675138473511 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1793 (0.1847)  loss_ce: 0.1793 (0.1847)  loss_ce_unscaled: 0.1793 (0.1847)  loss_point_unscaled: 53.8306 (100.6221)\n",
      "[ep 821][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1833 (0.1793)  loss_ce: 0.1833 (0.1793)  loss_ce_unscaled: 0.1833 (0.1793)  loss_point_unscaled: 55.5090 (79.1062)\n",
      "[ep 822][lr 0.0001000][2.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1804 (0.1807)  loss_ce: 0.1804 (0.1807)  loss_ce_unscaled: 0.1804 (0.1807)  loss_point_unscaled: 50.7968 (89.2707)\n",
      "[ep 823][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1829 (0.1884)  loss_ce: 0.1829 (0.1884)  loss_ce_unscaled: 0.1829 (0.1884)  loss_point_unscaled: 56.3631 (68.8923)\n",
      "[ep 824][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1842)  loss_ce: 0.1832 (0.1842)  loss_ce_unscaled: 0.1832 (0.1842)  loss_point_unscaled: 51.8850 (77.3323)\n",
      "[ep 825][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1840 (0.1841)  loss_ce: 0.1840 (0.1841)  loss_ce_unscaled: 0.1840 (0.1841)  loss_point_unscaled: 53.0140 (99.3669)\n",
      "[ep 826][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1792 (0.1789)  loss_ce: 0.1792 (0.1789)  loss_ce_unscaled: 0.1792 (0.1789)  loss_point_unscaled: 53.6805 (84.8028)\n",
      "[ep 827][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1821 (0.1960)  loss_ce: 0.1821 (0.1960)  loss_ce_unscaled: 0.1821 (0.1960)  loss_point_unscaled: 52.8820 (74.2660)\n",
      "[ep 828][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1973 (0.1929)  loss_ce: 0.1973 (0.1929)  loss_ce_unscaled: 0.1973 (0.1929)  loss_point_unscaled: 50.5769 (68.4783)\n",
      "[ep 829][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1867 (0.1892)  loss_ce: 0.1867 (0.1892)  loss_ce_unscaled: 0.1867 (0.1892)  loss_point_unscaled: 53.1924 (110.6004)\n",
      "[ep 830][lr 0.0001000][3.22s]\n",
      "=======================================test=======================================\n",
      "mae: 170.6098901098901 mse: 259.0378329532173 time: 2.270536184310913 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.2002 (0.1985)  loss_ce: 0.2002 (0.1985)  loss_ce_unscaled: 0.2002 (0.1985)  loss_point_unscaled: 52.8530 (96.4739)\n",
      "[ep 831][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1990 (0.1986)  loss_ce: 0.1990 (0.1986)  loss_ce_unscaled: 0.1990 (0.1986)  loss_point_unscaled: 52.6077 (101.6275)\n",
      "[ep 832][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1762 (0.1838)  loss_ce: 0.1762 (0.1838)  loss_ce_unscaled: 0.1762 (0.1838)  loss_point_unscaled: 61.9192 (84.6704)\n",
      "[ep 833][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1783 (0.1809)  loss_ce: 0.1783 (0.1809)  loss_ce_unscaled: 0.1783 (0.1809)  loss_point_unscaled: 54.3360 (86.9392)\n",
      "[ep 834][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1709 (0.1771)  loss_ce: 0.1709 (0.1771)  loss_ce_unscaled: 0.1709 (0.1771)  loss_point_unscaled: 54.6307 (77.9850)\n",
      "[ep 835][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1862)  loss_ce: 0.1808 (0.1862)  loss_ce_unscaled: 0.1808 (0.1862)  loss_point_unscaled: 50.8735 (66.0107)\n",
      "[ep 836][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1842 (0.1859)  loss_ce: 0.1842 (0.1859)  loss_ce_unscaled: 0.1842 (0.1859)  loss_point_unscaled: 50.7675 (88.3912)\n",
      "[ep 837][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1799 (0.1840)  loss_ce: 0.1799 (0.1840)  loss_ce_unscaled: 0.1799 (0.1840)  loss_point_unscaled: 53.2870 (76.3234)\n",
      "[ep 838][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1900 (0.1900)  loss_ce: 0.1900 (0.1900)  loss_ce_unscaled: 0.1900 (0.1900)  loss_point_unscaled: 55.6666 (73.5118)\n",
      "[ep 839][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1807 (0.1831)  loss_ce: 0.1807 (0.1831)  loss_ce_unscaled: 0.1807 (0.1831)  loss_point_unscaled: 52.9869 (137.3907)\n",
      "[ep 840][lr 0.0001000][2.79s]\n",
      "=======================================test=======================================\n",
      "mae: 149.17582417582418 mse: 234.94684926174682 time: 4.121721267700195 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1772 (0.1793)  loss_ce: 0.1772 (0.1793)  loss_ce_unscaled: 0.1772 (0.1793)  loss_point_unscaled: 67.9288 (102.1629)\n",
      "[ep 841][lr 0.0001000][3.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1749 (0.1834)  loss_ce: 0.1749 (0.1834)  loss_ce_unscaled: 0.1749 (0.1834)  loss_point_unscaled: 52.8728 (78.0777)\n",
      "[ep 842][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1719 (0.1888)  loss_ce: 0.1719 (0.1888)  loss_ce_unscaled: 0.1719 (0.1888)  loss_point_unscaled: 53.5184 (82.0563)\n",
      "[ep 843][lr 0.0001000][2.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1817 (0.1863)  loss_ce: 0.1817 (0.1863)  loss_ce_unscaled: 0.1817 (0.1863)  loss_point_unscaled: 51.9703 (73.4458)\n",
      "[ep 844][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1763 (0.1826)  loss_ce: 0.1763 (0.1826)  loss_ce_unscaled: 0.1763 (0.1826)  loss_point_unscaled: 54.5879 (116.0973)\n",
      "[ep 845][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1894 (0.1884)  loss_ce: 0.1894 (0.1884)  loss_ce_unscaled: 0.1894 (0.1884)  loss_point_unscaled: 56.7587 (90.4401)\n",
      "[ep 846][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1826 (0.1870)  loss_ce: 0.1826 (0.1870)  loss_ce_unscaled: 0.1826 (0.1870)  loss_point_unscaled: 51.4502 (84.1435)\n",
      "[ep 847][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1778 (0.1833)  loss_ce: 0.1778 (0.1833)  loss_ce_unscaled: 0.1778 (0.1833)  loss_point_unscaled: 57.3916 (91.8040)\n",
      "[ep 848][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1784 (0.1877)  loss_ce: 0.1784 (0.1877)  loss_ce_unscaled: 0.1784 (0.1877)  loss_point_unscaled: 51.7559 (59.4824)\n",
      "[ep 849][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1931 (0.1949)  loss_ce: 0.1931 (0.1949)  loss_ce_unscaled: 0.1931 (0.1949)  loss_point_unscaled: 53.1164 (60.8098)\n",
      "[ep 850][lr 0.0001000][2.36s]\n",
      "=======================================test=======================================\n",
      "mae: 173.5 mse: 279.3242532640846 time: 2.282904624938965 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1898 (0.1947)  loss_ce: 0.1898 (0.1947)  loss_ce_unscaled: 0.1898 (0.1947)  loss_point_unscaled: 55.7300 (79.9423)\n",
      "[ep 851][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1805 (0.1834)  loss_ce: 0.1805 (0.1834)  loss_ce_unscaled: 0.1805 (0.1834)  loss_point_unscaled: 53.3049 (84.3878)\n",
      "[ep 852][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1769 (0.1807)  loss_ce: 0.1769 (0.1807)  loss_ce_unscaled: 0.1769 (0.1807)  loss_point_unscaled: 54.3875 (103.3807)\n",
      "[ep 853][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1797 (0.1872)  loss_ce: 0.1797 (0.1872)  loss_ce_unscaled: 0.1797 (0.1872)  loss_point_unscaled: 50.0888 (79.2989)\n",
      "[ep 854][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1797 (0.1936)  loss_ce: 0.1797 (0.1936)  loss_ce_unscaled: 0.1797 (0.1936)  loss_point_unscaled: 52.1813 (87.7720)\n",
      "[ep 855][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1732 (0.1858)  loss_ce: 0.1732 (0.1858)  loss_ce_unscaled: 0.1732 (0.1858)  loss_point_unscaled: 55.0031 (72.2378)\n",
      "[ep 856][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1871)  loss_ce: 0.1809 (0.1871)  loss_ce_unscaled: 0.1809 (0.1871)  loss_point_unscaled: 50.6049 (92.9097)\n",
      "[ep 857][lr 0.0001000][3.51s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1837 (0.1832)  loss_ce: 0.1837 (0.1832)  loss_ce_unscaled: 0.1837 (0.1832)  loss_point_unscaled: 58.9172 (68.6984)\n",
      "[ep 858][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1957 (0.1899)  loss_ce: 0.1957 (0.1899)  loss_ce_unscaled: 0.1957 (0.1899)  loss_point_unscaled: 51.9069 (67.3505)\n",
      "[ep 859][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1696 (0.1844)  loss_ce: 0.1696 (0.1844)  loss_ce_unscaled: 0.1696 (0.1844)  loss_point_unscaled: 53.7084 (103.3977)\n",
      "[ep 860][lr 0.0001000][3.00s]\n",
      "=======================================test=======================================\n",
      "mae: 175.87362637362637 mse: 271.95926286732845 time: 2.372757911682129 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1748 (0.1815)  loss_ce: 0.1748 (0.1815)  loss_ce_unscaled: 0.1748 (0.1815)  loss_point_unscaled: 53.8572 (113.5437)\n",
      "[ep 861][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1811 (0.1820)  loss_ce: 0.1811 (0.1820)  loss_ce_unscaled: 0.1811 (0.1820)  loss_point_unscaled: 52.4974 (78.5115)\n",
      "[ep 862][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1798 (0.1916)  loss_ce: 0.1798 (0.1916)  loss_ce_unscaled: 0.1798 (0.1916)  loss_point_unscaled: 52.5179 (107.1947)\n",
      "[ep 863][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1821 (0.1922)  loss_ce: 0.1821 (0.1922)  loss_ce_unscaled: 0.1821 (0.1922)  loss_point_unscaled: 53.5424 (84.8053)\n",
      "[ep 864][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1792 (0.1884)  loss_ce: 0.1792 (0.1884)  loss_ce_unscaled: 0.1792 (0.1884)  loss_point_unscaled: 53.2799 (61.3561)\n",
      "[ep 865][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1721 (0.1821)  loss_ce: 0.1721 (0.1821)  loss_ce_unscaled: 0.1721 (0.1821)  loss_point_unscaled: 53.2079 (84.0410)\n",
      "[ep 866][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1858 (0.1845)  loss_ce: 0.1858 (0.1845)  loss_ce_unscaled: 0.1858 (0.1845)  loss_point_unscaled: 54.3297 (74.3976)\n",
      "[ep 867][lr 0.0001000][2.74s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1822 (0.1892)  loss_ce: 0.1822 (0.1892)  loss_ce_unscaled: 0.1822 (0.1892)  loss_point_unscaled: 55.1260 (73.7134)\n",
      "[ep 868][lr 0.0001000][3.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1745 (0.1768)  loss_ce: 0.1745 (0.1768)  loss_ce_unscaled: 0.1745 (0.1768)  loss_point_unscaled: 52.0468 (55.7550)\n",
      "[ep 869][lr 0.0001000][2.98s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1927 (0.1886)  loss_ce: 0.1927 (0.1886)  loss_ce_unscaled: 0.1927 (0.1886)  loss_point_unscaled: 54.8307 (70.8487)\n",
      "[ep 870][lr 0.0001000][3.08s]\n",
      "=======================================test=======================================\n",
      "mae: 157.47252747252747 mse: 260.57786585937066 time: 2.267657518386841 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1801 (0.1867)  loss_ce: 0.1801 (0.1867)  loss_ce_unscaled: 0.1801 (0.1867)  loss_point_unscaled: 53.4332 (104.1682)\n",
      "[ep 871][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1768 (0.1856)  loss_ce: 0.1768 (0.1856)  loss_ce_unscaled: 0.1768 (0.1856)  loss_point_unscaled: 49.8916 (65.3744)\n",
      "[ep 872][lr 0.0001000][2.55s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1785 (0.1855)  loss_ce: 0.1785 (0.1855)  loss_ce_unscaled: 0.1785 (0.1855)  loss_point_unscaled: 53.0613 (77.2841)\n",
      "[ep 873][lr 0.0001000][3.01s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1748 (0.1808)  loss_ce: 0.1748 (0.1808)  loss_ce_unscaled: 0.1748 (0.1808)  loss_point_unscaled: 53.8565 (72.4486)\n",
      "[ep 874][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1864 (0.1815)  loss_ce: 0.1864 (0.1815)  loss_ce_unscaled: 0.1864 (0.1815)  loss_point_unscaled: 55.4137 (91.5282)\n",
      "[ep 875][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1741 (0.1830)  loss_ce: 0.1741 (0.1830)  loss_ce_unscaled: 0.1741 (0.1830)  loss_point_unscaled: 55.1708 (76.4578)\n",
      "[ep 876][lr 0.0001000][3.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1873 (0.1850)  loss_ce: 0.1873 (0.1850)  loss_ce_unscaled: 0.1873 (0.1850)  loss_point_unscaled: 53.2781 (75.3640)\n",
      "[ep 877][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1804 (0.1896)  loss_ce: 0.1804 (0.1896)  loss_ce_unscaled: 0.1804 (0.1896)  loss_point_unscaled: 54.3030 (77.0926)\n",
      "[ep 878][lr 0.0001000][3.04s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1752 (0.1804)  loss_ce: 0.1752 (0.1804)  loss_ce_unscaled: 0.1752 (0.1804)  loss_point_unscaled: 50.3530 (117.7720)\n",
      "[ep 879][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1823)  loss_ce: 0.1884 (0.1823)  loss_ce_unscaled: 0.1884 (0.1823)  loss_point_unscaled: 53.2609 (93.3744)\n",
      "[ep 880][lr 0.0001000][2.60s]\n",
      "=======================================test=======================================\n",
      "mae: 148.25274725274724 mse: 238.84506909912758 time: 2.273958444595337 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1778 (0.1803)  loss_ce: 0.1778 (0.1803)  loss_ce_unscaled: 0.1778 (0.1803)  loss_point_unscaled: 53.4577 (82.8512)\n",
      "[ep 881][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1909 (0.1909)  loss_ce: 0.1909 (0.1909)  loss_ce_unscaled: 0.1909 (0.1909)  loss_point_unscaled: 50.2180 (71.5419)\n",
      "[ep 882][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1775 (0.1823)  loss_ce: 0.1775 (0.1823)  loss_ce_unscaled: 0.1775 (0.1823)  loss_point_unscaled: 61.5715 (82.7311)\n",
      "[ep 883][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1836 (0.1936)  loss_ce: 0.1836 (0.1936)  loss_ce_unscaled: 0.1836 (0.1936)  loss_point_unscaled: 48.4865 (70.7820)\n",
      "[ep 884][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1779 (0.1837)  loss_ce: 0.1779 (0.1837)  loss_ce_unscaled: 0.1779 (0.1837)  loss_point_unscaled: 53.0824 (89.7153)\n",
      "[ep 885][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1787 (0.1763)  loss_ce: 0.1787 (0.1763)  loss_ce_unscaled: 0.1787 (0.1763)  loss_point_unscaled: 51.6035 (73.5474)\n",
      "[ep 886][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1859 (0.1851)  loss_ce: 0.1859 (0.1851)  loss_ce_unscaled: 0.1859 (0.1851)  loss_point_unscaled: 51.9650 (104.4494)\n",
      "[ep 887][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1679 (0.1783)  loss_ce: 0.1679 (0.1783)  loss_ce_unscaled: 0.1679 (0.1783)  loss_point_unscaled: 52.9305 (135.7203)\n",
      "[ep 888][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1831 (0.1845)  loss_ce: 0.1831 (0.1845)  loss_ce_unscaled: 0.1831 (0.1845)  loss_point_unscaled: 53.9552 (119.3846)\n",
      "[ep 889][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1888 (0.1878)  loss_ce: 0.1888 (0.1878)  loss_ce_unscaled: 0.1888 (0.1878)  loss_point_unscaled: 50.0643 (115.0782)\n",
      "[ep 890][lr 0.0001000][3.20s]\n",
      "=======================================test=======================================\n",
      "mae: 148.85714285714286 mse: 231.46271624386725 time: 3.5993452072143555 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1875 (0.1835)  loss_ce: 0.1875 (0.1835)  loss_ce_unscaled: 0.1875 (0.1835)  loss_point_unscaled: 59.4350 (93.4439)\n",
      "[ep 891][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1812 (0.1806)  loss_ce: 0.1812 (0.1806)  loss_ce_unscaled: 0.1812 (0.1806)  loss_point_unscaled: 55.7580 (71.6809)\n",
      "[ep 892][lr 0.0001000][2.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1897)  loss_ce: 0.1823 (0.1897)  loss_ce_unscaled: 0.1823 (0.1897)  loss_point_unscaled: 51.0620 (78.9229)\n",
      "[ep 893][lr 0.0001000][2.57s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1814 (0.1854)  loss_ce: 0.1814 (0.1854)  loss_ce_unscaled: 0.1814 (0.1854)  loss_point_unscaled: 49.3579 (66.7632)\n",
      "[ep 894][lr 0.0001000][2.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1879 (0.1889)  loss_ce: 0.1879 (0.1889)  loss_ce_unscaled: 0.1879 (0.1889)  loss_point_unscaled: 52.2067 (89.0252)\n",
      "[ep 895][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1906 (0.1884)  loss_ce: 0.1906 (0.1884)  loss_ce_unscaled: 0.1906 (0.1884)  loss_point_unscaled: 55.3023 (86.7819)\n",
      "[ep 896][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1818 (0.1895)  loss_ce: 0.1818 (0.1895)  loss_ce_unscaled: 0.1818 (0.1895)  loss_point_unscaled: 58.7336 (102.2063)\n",
      "[ep 897][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1855)  loss_ce: 0.1865 (0.1855)  loss_ce_unscaled: 0.1865 (0.1855)  loss_point_unscaled: 49.0283 (56.2483)\n",
      "[ep 898][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1693 (0.1813)  loss_ce: 0.1693 (0.1813)  loss_ce_unscaled: 0.1693 (0.1813)  loss_point_unscaled: 64.3429 (99.7452)\n",
      "[ep 899][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1814 (0.1830)  loss_ce: 0.1814 (0.1830)  loss_ce_unscaled: 0.1814 (0.1830)  loss_point_unscaled: 53.9287 (68.4716)\n",
      "[ep 900][lr 0.0001000][3.34s]\n",
      "=======================================test=======================================\n",
      "mae: 145.73076923076923 mse: 227.7813077840765 time: 2.2804641723632812 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1838 (0.1851)  loss_ce: 0.1838 (0.1851)  loss_ce_unscaled: 0.1838 (0.1851)  loss_point_unscaled: 51.0822 (64.6776)\n",
      "[ep 901][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1754)  loss_ce: 0.1781 (0.1754)  loss_ce_unscaled: 0.1781 (0.1754)  loss_point_unscaled: 53.0890 (74.0479)\n",
      "[ep 902][lr 0.0001000][3.04s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1702 (0.1775)  loss_ce: 0.1702 (0.1775)  loss_ce_unscaled: 0.1702 (0.1775)  loss_point_unscaled: 51.5056 (62.1264)\n",
      "[ep 903][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1731 (0.1790)  loss_ce: 0.1731 (0.1790)  loss_ce_unscaled: 0.1731 (0.1790)  loss_point_unscaled: 55.3128 (101.6772)\n",
      "[ep 904][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1814 (0.1830)  loss_ce: 0.1814 (0.1830)  loss_ce_unscaled: 0.1814 (0.1830)  loss_point_unscaled: 51.1067 (61.0365)\n",
      "[ep 905][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1870 (0.1856)  loss_ce: 0.1870 (0.1856)  loss_ce_unscaled: 0.1870 (0.1856)  loss_point_unscaled: 54.5270 (69.7668)\n",
      "[ep 906][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1764 (0.1786)  loss_ce: 0.1764 (0.1786)  loss_ce_unscaled: 0.1764 (0.1786)  loss_point_unscaled: 53.8786 (63.1781)\n",
      "[ep 907][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1787 (0.1830)  loss_ce: 0.1787 (0.1830)  loss_ce_unscaled: 0.1787 (0.1830)  loss_point_unscaled: 51.1504 (69.5269)\n",
      "[ep 908][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1725 (0.1787)  loss_ce: 0.1725 (0.1787)  loss_ce_unscaled: 0.1725 (0.1787)  loss_point_unscaled: 51.0362 (79.0351)\n",
      "[ep 909][lr 0.0001000][2.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1817)  loss_ce: 0.1823 (0.1817)  loss_ce_unscaled: 0.1823 (0.1817)  loss_point_unscaled: 51.7615 (63.4516)\n",
      "[ep 910][lr 0.0001000][2.51s]\n",
      "=======================================test=======================================\n",
      "mae: 151.2912087912088 mse: 239.86366319865317 time: 2.2295258045196533 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1816)  loss_ce: 0.1781 (0.1816)  loss_ce_unscaled: 0.1781 (0.1816)  loss_point_unscaled: 53.3867 (67.1322)\n",
      "[ep 911][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1750 (0.1812)  loss_ce: 0.1750 (0.1812)  loss_ce_unscaled: 0.1750 (0.1812)  loss_point_unscaled: 56.6976 (68.2172)\n",
      "[ep 912][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1818 (0.1826)  loss_ce: 0.1818 (0.1826)  loss_ce_unscaled: 0.1818 (0.1826)  loss_point_unscaled: 53.5136 (77.6303)\n",
      "[ep 913][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1774 (0.1839)  loss_ce: 0.1774 (0.1839)  loss_ce_unscaled: 0.1774 (0.1839)  loss_point_unscaled: 54.7167 (89.5022)\n",
      "[ep 914][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1724 (0.1814)  loss_ce: 0.1724 (0.1814)  loss_ce_unscaled: 0.1724 (0.1814)  loss_point_unscaled: 50.9086 (67.9550)\n",
      "[ep 915][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1798)  loss_ce: 0.1865 (0.1798)  loss_ce_unscaled: 0.1865 (0.1798)  loss_point_unscaled: 53.8314 (109.5173)\n",
      "[ep 916][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1765 (0.1811)  loss_ce: 0.1765 (0.1811)  loss_ce_unscaled: 0.1765 (0.1811)  loss_point_unscaled: 54.7549 (101.6022)\n",
      "[ep 917][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1828 (0.1869)  loss_ce: 0.1828 (0.1869)  loss_ce_unscaled: 0.1828 (0.1869)  loss_point_unscaled: 49.6258 (59.9484)\n",
      "[ep 918][lr 0.0001000][3.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1855)  loss_ce: 0.1865 (0.1855)  loss_ce_unscaled: 0.1865 (0.1855)  loss_point_unscaled: 49.6525 (59.9705)\n",
      "[ep 919][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1854 (0.1865)  loss_ce: 0.1854 (0.1865)  loss_ce_unscaled: 0.1854 (0.1865)  loss_point_unscaled: 63.0412 (76.6378)\n",
      "[ep 920][lr 0.0001000][3.24s]\n",
      "=======================================test=======================================\n",
      "mae: 182.26923076923077 mse: 269.08247642847186 time: 3.194241762161255 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1816 (0.1877)  loss_ce: 0.1816 (0.1877)  loss_ce_unscaled: 0.1816 (0.1877)  loss_point_unscaled: 52.9564 (77.9607)\n",
      "[ep 921][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1897 (0.1909)  loss_ce: 0.1897 (0.1909)  loss_ce_unscaled: 0.1897 (0.1909)  loss_point_unscaled: 52.1689 (70.6499)\n",
      "[ep 922][lr 0.0001000][2.85s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1815 (0.1855)  loss_ce: 0.1815 (0.1855)  loss_ce_unscaled: 0.1815 (0.1855)  loss_point_unscaled: 51.4070 (65.8317)\n",
      "[ep 923][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1785 (0.1832)  loss_ce: 0.1785 (0.1832)  loss_ce_unscaled: 0.1785 (0.1832)  loss_point_unscaled: 55.9970 (119.6828)\n",
      "[ep 924][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1934 (0.1896)  loss_ce: 0.1934 (0.1896)  loss_ce_unscaled: 0.1934 (0.1896)  loss_point_unscaled: 51.1802 (103.4354)\n",
      "[ep 925][lr 0.0001000][3.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1800 (0.1909)  loss_ce: 0.1800 (0.1909)  loss_ce_unscaled: 0.1800 (0.1909)  loss_point_unscaled: 51.9012 (75.4664)\n",
      "[ep 926][lr 0.0001000][2.92s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1847)  loss_ce: 0.1865 (0.1847)  loss_ce_unscaled: 0.1865 (0.1847)  loss_point_unscaled: 51.6061 (106.5399)\n",
      "[ep 927][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1769 (0.1767)  loss_ce: 0.1769 (0.1767)  loss_ce_unscaled: 0.1769 (0.1767)  loss_point_unscaled: 49.4910 (65.0094)\n",
      "[ep 928][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1800 (0.1829)  loss_ce: 0.1800 (0.1829)  loss_ce_unscaled: 0.1800 (0.1829)  loss_point_unscaled: 49.8161 (72.6850)\n",
      "[ep 929][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1829 (0.1867)  loss_ce: 0.1829 (0.1867)  loss_ce_unscaled: 0.1829 (0.1867)  loss_point_unscaled: 49.8282 (66.2950)\n",
      "[ep 930][lr 0.0001000][3.31s]\n",
      "=======================================test=======================================\n",
      "mae: 166.36263736263737 mse: 237.42605557969384 time: 4.072765588760376 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1907)  loss_ce: 0.1839 (0.1907)  loss_ce_unscaled: 0.1839 (0.1907)  loss_point_unscaled: 50.9984 (73.9715)\n",
      "[ep 931][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1818 (0.1828)  loss_ce: 0.1818 (0.1828)  loss_ce_unscaled: 0.1818 (0.1828)  loss_point_unscaled: 85.1186 (99.6149)\n",
      "[ep 932][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1843 (0.1896)  loss_ce: 0.1843 (0.1896)  loss_ce_unscaled: 0.1843 (0.1896)  loss_point_unscaled: 52.8072 (86.6261)\n",
      "[ep 933][lr 0.0001000][2.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1870)  loss_ce: 0.1823 (0.1870)  loss_ce_unscaled: 0.1823 (0.1870)  loss_point_unscaled: 49.7369 (56.4322)\n",
      "[ep 934][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1880 (0.1864)  loss_ce: 0.1880 (0.1864)  loss_ce_unscaled: 0.1880 (0.1864)  loss_point_unscaled: 52.6380 (87.6349)\n",
      "[ep 935][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1785)  loss_ce: 0.1755 (0.1785)  loss_ce_unscaled: 0.1755 (0.1785)  loss_point_unscaled: 51.8310 (72.2444)\n",
      "[ep 936][lr 0.0001000][2.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1890 (0.1893)  loss_ce: 0.1890 (0.1893)  loss_ce_unscaled: 0.1890 (0.1893)  loss_point_unscaled: 55.1615 (69.1519)\n",
      "[ep 937][lr 0.0001000][3.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1780 (0.1818)  loss_ce: 0.1780 (0.1818)  loss_ce_unscaled: 0.1780 (0.1818)  loss_point_unscaled: 54.5033 (71.8438)\n",
      "[ep 938][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1788 (0.1844)  loss_ce: 0.1788 (0.1844)  loss_ce_unscaled: 0.1788 (0.1844)  loss_point_unscaled: 69.8912 (77.0532)\n",
      "[ep 939][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1775 (0.1795)  loss_ce: 0.1775 (0.1795)  loss_ce_unscaled: 0.1775 (0.1795)  loss_point_unscaled: 56.5966 (74.5255)\n",
      "[ep 940][lr 0.0001000][3.23s]\n",
      "=======================================test=======================================\n",
      "mae: 161.87362637362637 mse: 234.64503235481294 time: 2.259098529815674 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1828 (0.1915)  loss_ce: 0.1828 (0.1915)  loss_ce_unscaled: 0.1828 (0.1915)  loss_point_unscaled: 56.3744 (78.8546)\n",
      "[ep 941][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1892 (0.1856)  loss_ce: 0.1892 (0.1856)  loss_ce_unscaled: 0.1892 (0.1856)  loss_point_unscaled: 51.5731 (53.5342)\n",
      "[ep 942][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1835 (0.1874)  loss_ce: 0.1835 (0.1874)  loss_ce_unscaled: 0.1835 (0.1874)  loss_point_unscaled: 53.0065 (65.7852)\n",
      "[ep 943][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1774 (0.1816)  loss_ce: 0.1774 (0.1816)  loss_ce_unscaled: 0.1774 (0.1816)  loss_point_unscaled: 52.3263 (71.2543)\n",
      "[ep 944][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1821 (0.1866)  loss_ce: 0.1821 (0.1866)  loss_ce_unscaled: 0.1821 (0.1866)  loss_point_unscaled: 51.7598 (88.7367)\n",
      "[ep 945][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1779 (0.1782)  loss_ce: 0.1779 (0.1782)  loss_ce_unscaled: 0.1779 (0.1782)  loss_point_unscaled: 56.0350 (82.3312)\n",
      "[ep 946][lr 0.0001000][3.01s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1735 (0.1746)  loss_ce: 0.1735 (0.1746)  loss_ce_unscaled: 0.1735 (0.1746)  loss_point_unscaled: 54.1239 (79.8257)\n",
      "[ep 947][lr 0.0001000][2.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1726 (0.1810)  loss_ce: 0.1726 (0.1810)  loss_ce_unscaled: 0.1726 (0.1810)  loss_point_unscaled: 51.0503 (57.8557)\n",
      "[ep 948][lr 0.0001000][2.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1847 (0.1853)  loss_ce: 0.1847 (0.1853)  loss_ce_unscaled: 0.1847 (0.1853)  loss_point_unscaled: 54.1815 (80.1530)\n",
      "[ep 949][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1846 (0.1861)  loss_ce: 0.1846 (0.1861)  loss_ce_unscaled: 0.1846 (0.1861)  loss_point_unscaled: 56.9788 (78.0761)\n",
      "[ep 950][lr 0.0001000][3.28s]\n",
      "=======================================test=======================================\n",
      "mae: 151.6978021978022 mse: 221.76233441915463 time: 2.29111647605896 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1954 (0.1941)  loss_ce: 0.1954 (0.1941)  loss_ce_unscaled: 0.1954 (0.1941)  loss_point_unscaled: 50.9821 (63.0742)\n",
      "[ep 951][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1879 (0.1891)  loss_ce: 0.1879 (0.1891)  loss_ce_unscaled: 0.1879 (0.1891)  loss_point_unscaled: 48.0411 (57.4359)\n",
      "[ep 952][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1846 (0.1877)  loss_ce: 0.1846 (0.1877)  loss_ce_unscaled: 0.1846 (0.1877)  loss_point_unscaled: 53.7121 (76.9104)\n",
      "[ep 953][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1717 (0.1880)  loss_ce: 0.1717 (0.1880)  loss_ce_unscaled: 0.1717 (0.1880)  loss_point_unscaled: 50.6107 (53.8111)\n",
      "[ep 954][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1802 (0.1844)  loss_ce: 0.1802 (0.1844)  loss_ce_unscaled: 0.1802 (0.1844)  loss_point_unscaled: 57.1649 (89.9736)\n",
      "[ep 955][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1605 (0.1744)  loss_ce: 0.1605 (0.1744)  loss_ce_unscaled: 0.1605 (0.1744)  loss_point_unscaled: 48.8692 (62.0052)\n",
      "[ep 956][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1768 (0.1867)  loss_ce: 0.1768 (0.1867)  loss_ce_unscaled: 0.1768 (0.1867)  loss_point_unscaled: 55.5745 (79.9519)\n",
      "[ep 957][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1773 (0.1804)  loss_ce: 0.1773 (0.1804)  loss_ce_unscaled: 0.1773 (0.1804)  loss_point_unscaled: 51.3048 (81.1408)\n",
      "[ep 958][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1789 (0.1824)  loss_ce: 0.1789 (0.1824)  loss_ce_unscaled: 0.1789 (0.1824)  loss_point_unscaled: 51.4800 (68.2480)\n",
      "[ep 959][lr 0.0001000][2.54s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1909 (0.1913)  loss_ce: 0.1909 (0.1913)  loss_ce_unscaled: 0.1909 (0.1913)  loss_point_unscaled: 50.7698 (65.1470)\n",
      "[ep 960][lr 0.0001000][3.26s]\n",
      "=======================================test=======================================\n",
      "mae: 150.1098901098901 mse: 229.93854888586267 time: 4.2735419273376465 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1703 (0.1773)  loss_ce: 0.1703 (0.1773)  loss_ce_unscaled: 0.1703 (0.1773)  loss_point_unscaled: 53.6773 (79.4346)\n",
      "[ep 961][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1874 (0.1873)  loss_ce: 0.1874 (0.1873)  loss_ce_unscaled: 0.1874 (0.1873)  loss_point_unscaled: 51.5049 (79.9716)\n",
      "[ep 962][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1595 (0.1772)  loss_ce: 0.1595 (0.1772)  loss_ce_unscaled: 0.1595 (0.1772)  loss_point_unscaled: 51.4287 (76.9369)\n",
      "[ep 963][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1789 (0.1886)  loss_ce: 0.1789 (0.1886)  loss_ce_unscaled: 0.1789 (0.1886)  loss_point_unscaled: 51.9609 (77.2508)\n",
      "[ep 964][lr 0.0001000][2.96s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1812 (0.1793)  loss_ce: 0.1812 (0.1793)  loss_ce_unscaled: 0.1812 (0.1793)  loss_point_unscaled: 55.0831 (86.6520)\n",
      "[ep 965][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1887 (0.1899)  loss_ce: 0.1887 (0.1899)  loss_ce_unscaled: 0.1887 (0.1899)  loss_point_unscaled: 51.5879 (75.4063)\n",
      "[ep 966][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1886 (0.1919)  loss_ce: 0.1886 (0.1919)  loss_ce_unscaled: 0.1886 (0.1919)  loss_point_unscaled: 52.0864 (90.8573)\n",
      "[ep 967][lr 0.0001000][3.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1794 (0.1877)  loss_ce: 0.1794 (0.1877)  loss_ce_unscaled: 0.1794 (0.1877)  loss_point_unscaled: 51.2847 (67.3787)\n",
      "[ep 968][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1743 (0.1864)  loss_ce: 0.1743 (0.1864)  loss_ce_unscaled: 0.1743 (0.1864)  loss_point_unscaled: 52.2697 (107.3991)\n",
      "[ep 969][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1896 (0.1880)  loss_ce: 0.1896 (0.1880)  loss_ce_unscaled: 0.1896 (0.1880)  loss_point_unscaled: 56.8747 (110.0443)\n",
      "[ep 970][lr 0.0001000][2.39s]\n",
      "=======================================test=======================================\n",
      "mae: 159.56593406593407 mse: 249.2526962610567 time: 2.2798750400543213 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1732 (0.1751)  loss_ce: 0.1732 (0.1751)  loss_ce_unscaled: 0.1732 (0.1751)  loss_point_unscaled: 53.4666 (73.1337)\n",
      "[ep 971][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1918 (0.1899)  loss_ce: 0.1918 (0.1899)  loss_ce_unscaled: 0.1918 (0.1899)  loss_point_unscaled: 49.5827 (80.4127)\n",
      "[ep 972][lr 0.0001000][3.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1879 (0.1870)  loss_ce: 0.1879 (0.1870)  loss_ce_unscaled: 0.1879 (0.1870)  loss_point_unscaled: 48.8746 (71.7254)\n",
      "[ep 973][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1850 (0.1903)  loss_ce: 0.1850 (0.1903)  loss_ce_unscaled: 0.1850 (0.1903)  loss_point_unscaled: 53.9198 (80.2999)\n",
      "[ep 974][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1850)  loss_ce: 0.1851 (0.1850)  loss_ce_unscaled: 0.1851 (0.1850)  loss_point_unscaled: 53.5324 (76.5427)\n",
      "[ep 975][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1816 (0.1887)  loss_ce: 0.1816 (0.1887)  loss_ce_unscaled: 0.1816 (0.1887)  loss_point_unscaled: 51.7677 (90.1113)\n",
      "[ep 976][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1764 (0.1850)  loss_ce: 0.1764 (0.1850)  loss_ce_unscaled: 0.1764 (0.1850)  loss_point_unscaled: 53.0130 (98.9284)\n",
      "[ep 977][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1772 (0.1804)  loss_ce: 0.1772 (0.1804)  loss_ce_unscaled: 0.1772 (0.1804)  loss_point_unscaled: 51.1463 (85.9150)\n",
      "[ep 978][lr 0.0001000][3.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1766 (0.1845)  loss_ce: 0.1766 (0.1845)  loss_ce_unscaled: 0.1766 (0.1845)  loss_point_unscaled: 53.4484 (84.6371)\n",
      "[ep 979][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1813 (0.1819)  loss_ce: 0.1813 (0.1819)  loss_ce_unscaled: 0.1813 (0.1819)  loss_point_unscaled: 51.2250 (78.6798)\n",
      "[ep 980][lr 0.0001000][3.26s]\n",
      "=======================================test=======================================\n",
      "mae: 173.42307692307693 mse: 279.94352844971985 time: 4.247778654098511 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1862)  loss_ce: 0.1781 (0.1862)  loss_ce_unscaled: 0.1781 (0.1862)  loss_point_unscaled: 54.3329 (100.3731)\n",
      "[ep 981][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1713 (0.1800)  loss_ce: 0.1713 (0.1800)  loss_ce_unscaled: 0.1713 (0.1800)  loss_point_unscaled: 53.7186 (67.2706)\n",
      "[ep 982][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1806 (0.1822)  loss_ce: 0.1806 (0.1822)  loss_ce_unscaled: 0.1806 (0.1822)  loss_point_unscaled: 51.4915 (54.9450)\n",
      "[ep 983][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1887 (0.1824)  loss_ce: 0.1887 (0.1824)  loss_ce_unscaled: 0.1887 (0.1824)  loss_point_unscaled: 52.2246 (77.0093)\n",
      "[ep 984][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1892)  loss_ce: 0.1781 (0.1892)  loss_ce_unscaled: 0.1781 (0.1892)  loss_point_unscaled: 54.2872 (88.1208)\n",
      "[ep 985][lr 0.0001000][3.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1750 (0.1805)  loss_ce: 0.1750 (0.1805)  loss_ce_unscaled: 0.1750 (0.1805)  loss_point_unscaled: 54.4527 (88.5299)\n",
      "[ep 986][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1858 (0.1882)  loss_ce: 0.1858 (0.1882)  loss_ce_unscaled: 0.1858 (0.1882)  loss_point_unscaled: 49.2489 (67.1467)\n",
      "[ep 987][lr 0.0001000][2.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1868 (0.1819)  loss_ce: 0.1868 (0.1819)  loss_ce_unscaled: 0.1868 (0.1819)  loss_point_unscaled: 50.4686 (85.5331)\n",
      "[ep 988][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1731 (0.1855)  loss_ce: 0.1731 (0.1855)  loss_ce_unscaled: 0.1731 (0.1855)  loss_point_unscaled: 56.0493 (70.9417)\n",
      "[ep 989][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1847 (0.1855)  loss_ce: 0.1847 (0.1855)  loss_ce_unscaled: 0.1847 (0.1855)  loss_point_unscaled: 53.4428 (113.3518)\n",
      "[ep 990][lr 0.0001000][3.04s]\n",
      "=======================================test=======================================\n",
      "mae: 157.35714285714286 mse: 243.97870786220236 time: 4.165275812149048 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1735 (0.1735)  loss_ce: 0.1735 (0.1735)  loss_ce_unscaled: 0.1735 (0.1735)  loss_point_unscaled: 53.5694 (75.0371)\n",
      "[ep 991][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1818 (0.1876)  loss_ce: 0.1818 (0.1876)  loss_ce_unscaled: 0.1818 (0.1876)  loss_point_unscaled: 54.8263 (90.6917)\n",
      "[ep 992][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1912 (0.1865)  loss_ce: 0.1912 (0.1865)  loss_ce_unscaled: 0.1912 (0.1865)  loss_point_unscaled: 57.7415 (73.5672)\n",
      "[ep 993][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1772 (0.1820)  loss_ce: 0.1772 (0.1820)  loss_ce_unscaled: 0.1772 (0.1820)  loss_point_unscaled: 51.9495 (100.2761)\n",
      "[ep 994][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1843 (0.1836)  loss_ce: 0.1843 (0.1836)  loss_ce_unscaled: 0.1843 (0.1836)  loss_point_unscaled: 54.0575 (79.4919)\n",
      "[ep 995][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1775 (0.1844)  loss_ce: 0.1775 (0.1844)  loss_ce_unscaled: 0.1775 (0.1844)  loss_point_unscaled: 55.8652 (78.4303)\n",
      "[ep 996][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1785 (0.1822)  loss_ce: 0.1785 (0.1822)  loss_ce_unscaled: 0.1785 (0.1822)  loss_point_unscaled: 51.1108 (78.8280)\n",
      "[ep 997][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1780 (0.1749)  loss_ce: 0.1780 (0.1749)  loss_ce_unscaled: 0.1780 (0.1749)  loss_point_unscaled: 56.9157 (91.4573)\n",
      "[ep 998][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1757 (0.1888)  loss_ce: 0.1757 (0.1888)  loss_ce_unscaled: 0.1757 (0.1888)  loss_point_unscaled: 53.3974 (79.8881)\n",
      "[ep 999][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1845 (0.1880)  loss_ce: 0.1845 (0.1880)  loss_ce_unscaled: 0.1845 (0.1880)  loss_point_unscaled: 53.6255 (84.9664)\n",
      "[ep 1000][lr 0.0001000][3.18s]\n",
      "=======================================test=======================================\n",
      "mae: 145.35714285714286 mse: 233.30015331855557 time: 2.270582675933838 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1869 (0.1871)  loss_ce: 0.1869 (0.1871)  loss_ce_unscaled: 0.1869 (0.1871)  loss_point_unscaled: 54.9228 (66.0261)\n",
      "[ep 1001][lr 0.0001000][3.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1716 (0.1848)  loss_ce: 0.1716 (0.1848)  loss_ce_unscaled: 0.1716 (0.1848)  loss_point_unscaled: 54.4065 (74.6225)\n",
      "[ep 1002][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1802 (0.1809)  loss_ce: 0.1802 (0.1809)  loss_ce_unscaled: 0.1802 (0.1809)  loss_point_unscaled: 56.5775 (89.2974)\n",
      "[ep 1003][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1870 (0.1877)  loss_ce: 0.1870 (0.1877)  loss_ce_unscaled: 0.1870 (0.1877)  loss_point_unscaled: 53.3060 (111.2796)\n",
      "[ep 1004][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1801 (0.1840)  loss_ce: 0.1801 (0.1840)  loss_ce_unscaled: 0.1801 (0.1840)  loss_point_unscaled: 53.5527 (105.9326)\n",
      "[ep 1005][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1715 (0.1854)  loss_ce: 0.1715 (0.1854)  loss_ce_unscaled: 0.1715 (0.1854)  loss_point_unscaled: 49.5207 (68.3113)\n",
      "[ep 1006][lr 0.0001000][3.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1893 (0.1865)  loss_ce: 0.1893 (0.1865)  loss_ce_unscaled: 0.1893 (0.1865)  loss_point_unscaled: 56.4175 (70.4533)\n",
      "[ep 1007][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1786 (0.1815)  loss_ce: 0.1786 (0.1815)  loss_ce_unscaled: 0.1786 (0.1815)  loss_point_unscaled: 54.9968 (87.0898)\n",
      "[ep 1008][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1734 (0.1778)  loss_ce: 0.1734 (0.1778)  loss_ce_unscaled: 0.1734 (0.1778)  loss_point_unscaled: 51.5440 (73.7751)\n",
      "[ep 1009][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1793 (0.1826)  loss_ce: 0.1793 (0.1826)  loss_ce_unscaled: 0.1793 (0.1826)  loss_point_unscaled: 49.9583 (101.0345)\n",
      "[ep 1010][lr 0.0001000][3.26s]\n",
      "=======================================test=======================================\n",
      "mae: 146.62087912087912 mse: 232.97463664244938 time: 2.9900436401367188 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1775 (0.1806)  loss_ce: 0.1775 (0.1806)  loss_ce_unscaled: 0.1775 (0.1806)  loss_point_unscaled: 55.5760 (105.6059)\n",
      "[ep 1011][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1883)  loss_ce: 0.1809 (0.1883)  loss_ce_unscaled: 0.1809 (0.1883)  loss_point_unscaled: 57.0253 (92.0085)\n",
      "[ep 1012][lr 0.0001000][2.97s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1896)  loss_ce: 0.1823 (0.1896)  loss_ce_unscaled: 0.1823 (0.1896)  loss_point_unscaled: 54.1185 (88.1380)\n",
      "[ep 1013][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1799 (0.1883)  loss_ce: 0.1799 (0.1883)  loss_ce_unscaled: 0.1799 (0.1883)  loss_point_unscaled: 50.7816 (81.5637)\n",
      "[ep 1014][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1758 (0.1829)  loss_ce: 0.1758 (0.1829)  loss_ce_unscaled: 0.1758 (0.1829)  loss_point_unscaled: 57.3014 (79.1231)\n",
      "[ep 1015][lr 0.0001000][3.00s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1845 (0.1884)  loss_ce: 0.1845 (0.1884)  loss_ce_unscaled: 0.1845 (0.1884)  loss_point_unscaled: 55.0584 (92.3133)\n",
      "[ep 1016][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1844 (0.1852)  loss_ce: 0.1844 (0.1852)  loss_ce_unscaled: 0.1844 (0.1852)  loss_point_unscaled: 50.5871 (87.1424)\n",
      "[ep 1017][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1817 (0.1882)  loss_ce: 0.1817 (0.1882)  loss_ce_unscaled: 0.1817 (0.1882)  loss_point_unscaled: 52.4660 (97.9173)\n",
      "[ep 1018][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1986 (0.1897)  loss_ce: 0.1986 (0.1897)  loss_ce_unscaled: 0.1986 (0.1897)  loss_point_unscaled: 52.6007 (87.8832)\n",
      "[ep 1019][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1824 (0.1801)  loss_ce: 0.1824 (0.1801)  loss_ce_unscaled: 0.1824 (0.1801)  loss_point_unscaled: 52.3730 (63.5158)\n",
      "[ep 1020][lr 0.0001000][3.10s]\n",
      "=======================================test=======================================\n",
      "mae: 185.1813186813187 mse: 275.4535021889028 time: 4.173932313919067 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1876)  loss_ce: 0.1872 (0.1876)  loss_ce_unscaled: 0.1872 (0.1876)  loss_point_unscaled: 50.4264 (70.5408)\n",
      "[ep 1021][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1907)  loss_ce: 0.1851 (0.1907)  loss_ce_unscaled: 0.1851 (0.1907)  loss_point_unscaled: 52.2971 (74.6399)\n",
      "[ep 1022][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1692 (0.1774)  loss_ce: 0.1692 (0.1774)  loss_ce_unscaled: 0.1692 (0.1774)  loss_point_unscaled: 50.7514 (60.2802)\n",
      "[ep 1023][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1729 (0.1864)  loss_ce: 0.1729 (0.1864)  loss_ce_unscaled: 0.1729 (0.1864)  loss_point_unscaled: 54.1460 (113.6819)\n",
      "[ep 1024][lr 0.0001000][2.51s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1793 (0.1932)  loss_ce: 0.1793 (0.1932)  loss_ce_unscaled: 0.1793 (0.1932)  loss_point_unscaled: 54.5792 (68.5457)\n",
      "[ep 1025][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1859)  loss_ce: 0.1808 (0.1859)  loss_ce_unscaled: 0.1808 (0.1859)  loss_point_unscaled: 55.4558 (105.9623)\n",
      "[ep 1026][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1826 (0.1818)  loss_ce: 0.1826 (0.1818)  loss_ce_unscaled: 0.1826 (0.1818)  loss_point_unscaled: 52.9949 (80.4843)\n",
      "[ep 1027][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1810 (0.1793)  loss_ce: 0.1810 (0.1793)  loss_ce_unscaled: 0.1810 (0.1793)  loss_point_unscaled: 56.9753 (72.0871)\n",
      "[ep 1028][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1903 (0.1853)  loss_ce: 0.1903 (0.1853)  loss_ce_unscaled: 0.1903 (0.1853)  loss_point_unscaled: 49.1424 (79.5821)\n",
      "[ep 1029][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1740 (0.1837)  loss_ce: 0.1740 (0.1837)  loss_ce_unscaled: 0.1740 (0.1837)  loss_point_unscaled: 51.3177 (74.9982)\n",
      "[ep 1030][lr 0.0001000][2.90s]\n",
      "=======================================test=======================================\n",
      "mae: 155.5 mse: 232.7793725048168 time: 2.230936288833618 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1769 (0.1842)  loss_ce: 0.1769 (0.1842)  loss_ce_unscaled: 0.1769 (0.1842)  loss_point_unscaled: 53.7091 (83.6802)\n",
      "[ep 1031][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1771 (0.1854)  loss_ce: 0.1771 (0.1854)  loss_ce_unscaled: 0.1771 (0.1854)  loss_point_unscaled: 50.1065 (83.2937)\n",
      "[ep 1032][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1879 (0.1920)  loss_ce: 0.1879 (0.1920)  loss_ce_unscaled: 0.1879 (0.1920)  loss_point_unscaled: 64.6640 (96.9515)\n",
      "[ep 1033][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1760 (0.1897)  loss_ce: 0.1760 (0.1897)  loss_ce_unscaled: 0.1760 (0.1897)  loss_point_unscaled: 57.1841 (107.6058)\n",
      "[ep 1034][lr 0.0001000][3.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1848 (0.1843)  loss_ce: 0.1848 (0.1843)  loss_ce_unscaled: 0.1848 (0.1843)  loss_point_unscaled: 49.7539 (64.8404)\n",
      "[ep 1035][lr 0.0001000][3.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1856 (0.1822)  loss_ce: 0.1856 (0.1822)  loss_ce_unscaled: 0.1856 (0.1822)  loss_point_unscaled: 54.7168 (63.3693)\n",
      "[ep 1036][lr 0.0001000][2.94s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1743 (0.1877)  loss_ce: 0.1743 (0.1877)  loss_ce_unscaled: 0.1743 (0.1877)  loss_point_unscaled: 55.2070 (81.9813)\n",
      "[ep 1037][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1792)  loss_ce: 0.1839 (0.1792)  loss_ce_unscaled: 0.1839 (0.1792)  loss_point_unscaled: 51.6554 (85.0368)\n",
      "[ep 1038][lr 0.0001000][2.53s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1866 (0.1874)  loss_ce: 0.1866 (0.1874)  loss_ce_unscaled: 0.1866 (0.1874)  loss_point_unscaled: 53.3993 (91.9483)\n",
      "[ep 1039][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1742 (0.1898)  loss_ce: 0.1742 (0.1898)  loss_ce_unscaled: 0.1742 (0.1898)  loss_point_unscaled: 49.8117 (110.1800)\n",
      "[ep 1040][lr 0.0001000][2.42s]\n",
      "=======================================test=======================================\n",
      "mae: 158.95054945054946 mse: 245.50923753351057 time: 2.246403694152832 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1801 (0.1820)  loss_ce: 0.1801 (0.1820)  loss_ce_unscaled: 0.1801 (0.1820)  loss_point_unscaled: 53.0891 (56.6975)\n",
      "[ep 1041][lr 0.0001000][2.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1878 (0.1851)  loss_ce: 0.1878 (0.1851)  loss_ce_unscaled: 0.1878 (0.1851)  loss_point_unscaled: 50.4357 (72.4219)\n",
      "[ep 1042][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1789 (0.1846)  loss_ce: 0.1789 (0.1846)  loss_ce_unscaled: 0.1789 (0.1846)  loss_point_unscaled: 52.5182 (80.6174)\n",
      "[ep 1043][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1730 (0.1801)  loss_ce: 0.1730 (0.1801)  loss_ce_unscaled: 0.1730 (0.1801)  loss_point_unscaled: 50.5086 (93.6549)\n",
      "[ep 1044][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1826 (0.1808)  loss_ce: 0.1826 (0.1808)  loss_ce_unscaled: 0.1826 (0.1808)  loss_point_unscaled: 54.6200 (80.2582)\n",
      "[ep 1045][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1829)  loss_ce: 0.1781 (0.1829)  loss_ce_unscaled: 0.1781 (0.1829)  loss_point_unscaled: 52.0152 (70.8073)\n",
      "[ep 1046][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1804 (0.1835)  loss_ce: 0.1804 (0.1835)  loss_ce_unscaled: 0.1804 (0.1835)  loss_point_unscaled: 51.6860 (96.9689)\n",
      "[ep 1047][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1845)  loss_ce: 0.1851 (0.1845)  loss_ce_unscaled: 0.1851 (0.1845)  loss_point_unscaled: 55.9902 (70.8273)\n",
      "[ep 1048][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1740 (0.1807)  loss_ce: 0.1740 (0.1807)  loss_ce_unscaled: 0.1740 (0.1807)  loss_point_unscaled: 54.8378 (96.6127)\n",
      "[ep 1049][lr 0.0001000][2.96s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1732 (0.1914)  loss_ce: 0.1732 (0.1914)  loss_ce_unscaled: 0.1732 (0.1914)  loss_point_unscaled: 52.1589 (90.5514)\n",
      "[ep 1050][lr 0.0001000][3.21s]\n",
      "=======================================test=======================================\n",
      "mae: 167.5164835164835 mse: 252.37696382223928 time: 2.2490227222442627 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1854)  loss_ce: 0.1851 (0.1854)  loss_ce_unscaled: 0.1851 (0.1854)  loss_point_unscaled: 54.6949 (72.0550)\n",
      "[ep 1051][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1877 (0.1883)  loss_ce: 0.1877 (0.1883)  loss_ce_unscaled: 0.1877 (0.1883)  loss_point_unscaled: 48.6596 (96.2381)\n",
      "[ep 1052][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1734 (0.1834)  loss_ce: 0.1734 (0.1834)  loss_ce_unscaled: 0.1734 (0.1834)  loss_point_unscaled: 55.8627 (88.7880)\n",
      "[ep 1053][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1852 (0.1859)  loss_ce: 0.1852 (0.1859)  loss_ce_unscaled: 0.1852 (0.1859)  loss_point_unscaled: 51.5525 (80.9393)\n",
      "[ep 1054][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1949 (0.1946)  loss_ce: 0.1949 (0.1946)  loss_ce_unscaled: 0.1949 (0.1946)  loss_point_unscaled: 66.8970 (89.6125)\n",
      "[ep 1055][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1787 (0.1813)  loss_ce: 0.1787 (0.1813)  loss_ce_unscaled: 0.1787 (0.1813)  loss_point_unscaled: 51.6629 (74.0214)\n",
      "[ep 1056][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1735 (0.1814)  loss_ce: 0.1735 (0.1814)  loss_ce_unscaled: 0.1735 (0.1814)  loss_point_unscaled: 49.6275 (64.5809)\n",
      "[ep 1057][lr 0.0001000][2.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1744 (0.1819)  loss_ce: 0.1744 (0.1819)  loss_ce_unscaled: 0.1744 (0.1819)  loss_point_unscaled: 51.6345 (69.4785)\n",
      "[ep 1058][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1753 (0.1774)  loss_ce: 0.1753 (0.1774)  loss_ce_unscaled: 0.1753 (0.1774)  loss_point_unscaled: 49.9692 (57.4558)\n",
      "[ep 1059][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1858)  loss_ce: 0.1809 (0.1858)  loss_ce_unscaled: 0.1809 (0.1858)  loss_point_unscaled: 50.8167 (71.6222)\n",
      "[ep 1060][lr 0.0001000][3.17s]\n",
      "=======================================test=======================================\n",
      "mae: 151.24725274725276 mse: 233.50601799431303 time: 2.329766035079956 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1780 (0.1827)  loss_ce: 0.1780 (0.1827)  loss_ce_unscaled: 0.1780 (0.1827)  loss_point_unscaled: 52.0527 (72.5254)\n",
      "[ep 1061][lr 0.0001000][2.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1923 (0.1830)  loss_ce: 0.1923 (0.1830)  loss_ce_unscaled: 0.1923 (0.1830)  loss_point_unscaled: 51.9522 (67.5060)\n",
      "[ep 1062][lr 0.0001000][2.70s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1778 (0.1778)  loss_ce: 0.1778 (0.1778)  loss_ce_unscaled: 0.1778 (0.1778)  loss_point_unscaled: 62.1442 (113.2268)\n",
      "[ep 1063][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1704 (0.1742)  loss_ce: 0.1704 (0.1742)  loss_ce_unscaled: 0.1704 (0.1742)  loss_point_unscaled: 56.1407 (91.4747)\n",
      "[ep 1064][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1898 (0.1914)  loss_ce: 0.1898 (0.1914)  loss_ce_unscaled: 0.1898 (0.1914)  loss_point_unscaled: 47.5798 (70.3740)\n",
      "[ep 1065][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1663 (0.1760)  loss_ce: 0.1663 (0.1760)  loss_ce_unscaled: 0.1663 (0.1760)  loss_point_unscaled: 55.1028 (71.9683)\n",
      "[ep 1066][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1874)  loss_ce: 0.1884 (0.1874)  loss_ce_unscaled: 0.1884 (0.1874)  loss_point_unscaled: 52.8661 (63.0200)\n",
      "[ep 1067][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1773 (0.1799)  loss_ce: 0.1773 (0.1799)  loss_ce_unscaled: 0.1773 (0.1799)  loss_point_unscaled: 51.9813 (91.4317)\n",
      "[ep 1068][lr 0.0001000][3.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1746 (0.1851)  loss_ce: 0.1746 (0.1851)  loss_ce_unscaled: 0.1746 (0.1851)  loss_point_unscaled: 58.3223 (106.2891)\n",
      "[ep 1069][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1780 (0.1826)  loss_ce: 0.1780 (0.1826)  loss_ce_unscaled: 0.1780 (0.1826)  loss_point_unscaled: 53.0039 (74.4564)\n",
      "[ep 1070][lr 0.0001000][3.09s]\n",
      "=======================================test=======================================\n",
      "mae: 158.42857142857142 mse: 242.1966551796696 time: 2.2942519187927246 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1770 (0.1786)  loss_ce: 0.1770 (0.1786)  loss_ce_unscaled: 0.1770 (0.1786)  loss_point_unscaled: 56.0058 (70.7743)\n",
      "[ep 1071][lr 0.0001000][2.86s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1672 (0.1808)  loss_ce: 0.1672 (0.1808)  loss_ce_unscaled: 0.1672 (0.1808)  loss_point_unscaled: 52.9672 (63.9372)\n",
      "[ep 1072][lr 0.0001000][3.01s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1937)  loss_ce: 0.1884 (0.1937)  loss_ce_unscaled: 0.1884 (0.1937)  loss_point_unscaled: 51.3769 (63.6047)\n",
      "[ep 1073][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1910 (0.1883)  loss_ce: 0.1910 (0.1883)  loss_ce_unscaled: 0.1910 (0.1883)  loss_point_unscaled: 55.7585 (104.3306)\n",
      "[ep 1074][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1786 (0.1809)  loss_ce: 0.1786 (0.1809)  loss_ce_unscaled: 0.1786 (0.1809)  loss_point_unscaled: 50.0668 (54.2013)\n",
      "[ep 1075][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1915 (0.1897)  loss_ce: 0.1915 (0.1897)  loss_ce_unscaled: 0.1915 (0.1897)  loss_point_unscaled: 51.9741 (77.6140)\n",
      "[ep 1076][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1761 (0.1812)  loss_ce: 0.1761 (0.1812)  loss_ce_unscaled: 0.1761 (0.1812)  loss_point_unscaled: 54.1766 (97.0716)\n",
      "[ep 1077][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1772 (0.1776)  loss_ce: 0.1772 (0.1776)  loss_ce_unscaled: 0.1772 (0.1776)  loss_point_unscaled: 52.8753 (72.2373)\n",
      "[ep 1078][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1811 (0.1852)  loss_ce: 0.1811 (0.1852)  loss_ce_unscaled: 0.1811 (0.1852)  loss_point_unscaled: 51.5189 (51.7175)\n",
      "[ep 1079][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1882)  loss_ce: 0.1823 (0.1882)  loss_ce_unscaled: 0.1823 (0.1882)  loss_point_unscaled: 52.7308 (104.5234)\n",
      "[ep 1080][lr 0.0001000][3.27s]\n",
      "=======================================test=======================================\n",
      "mae: 148.52197802197801 mse: 226.37466512617752 time: 3.846268892288208 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1879 (0.1860)  loss_ce: 0.1879 (0.1860)  loss_ce_unscaled: 0.1879 (0.1860)  loss_point_unscaled: 53.0470 (72.5171)\n",
      "[ep 1081][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1877)  loss_ce: 0.1809 (0.1877)  loss_ce_unscaled: 0.1809 (0.1877)  loss_point_unscaled: 53.3001 (67.2216)\n",
      "[ep 1082][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1806 (0.1837)  loss_ce: 0.1806 (0.1837)  loss_ce_unscaled: 0.1806 (0.1837)  loss_point_unscaled: 54.5594 (98.5301)\n",
      "[ep 1083][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1710 (0.1823)  loss_ce: 0.1710 (0.1823)  loss_ce_unscaled: 0.1710 (0.1823)  loss_point_unscaled: 53.4668 (79.9745)\n",
      "[ep 1084][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1957 (0.1980)  loss_ce: 0.1957 (0.1980)  loss_ce_unscaled: 0.1957 (0.1980)  loss_point_unscaled: 51.7660 (66.2725)\n",
      "[ep 1085][lr 0.0001000][3.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1699 (0.1823)  loss_ce: 0.1699 (0.1823)  loss_ce_unscaled: 0.1699 (0.1823)  loss_point_unscaled: 53.1627 (91.8000)\n",
      "[ep 1086][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1766 (0.1790)  loss_ce: 0.1766 (0.1790)  loss_ce_unscaled: 0.1766 (0.1790)  loss_point_unscaled: 57.6318 (99.6448)\n",
      "[ep 1087][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1834)  loss_ce: 0.1832 (0.1834)  loss_ce_unscaled: 0.1832 (0.1834)  loss_point_unscaled: 56.9637 (67.7971)\n",
      "[ep 1088][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1836 (0.1901)  loss_ce: 0.1836 (0.1901)  loss_ce_unscaled: 0.1836 (0.1901)  loss_point_unscaled: 48.0001 (69.9706)\n",
      "[ep 1089][lr 0.0001000][3.48s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1742 (0.1807)  loss_ce: 0.1742 (0.1807)  loss_ce_unscaled: 0.1742 (0.1807)  loss_point_unscaled: 51.2170 (83.7788)\n",
      "[ep 1090][lr 0.0001000][2.55s]\n",
      "=======================================test=======================================\n",
      "mae: 158.04945054945054 mse: 231.58855377966816 time: 2.2304537296295166 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1813 (0.1885)  loss_ce: 0.1813 (0.1885)  loss_ce_unscaled: 0.1813 (0.1885)  loss_point_unscaled: 50.8004 (72.9540)\n",
      "[ep 1091][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1872)  loss_ce: 0.1808 (0.1872)  loss_ce_unscaled: 0.1808 (0.1872)  loss_point_unscaled: 50.7832 (80.0400)\n",
      "[ep 1092][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1712 (0.1855)  loss_ce: 0.1712 (0.1855)  loss_ce_unscaled: 0.1712 (0.1855)  loss_point_unscaled: 51.8803 (74.2704)\n",
      "[ep 1093][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1767 (0.1793)  loss_ce: 0.1767 (0.1793)  loss_ce_unscaled: 0.1767 (0.1793)  loss_point_unscaled: 53.0384 (62.0400)\n",
      "[ep 1094][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1791 (0.1824)  loss_ce: 0.1791 (0.1824)  loss_ce_unscaled: 0.1791 (0.1824)  loss_point_unscaled: 52.4681 (61.0134)\n",
      "[ep 1095][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1814 (0.1934)  loss_ce: 0.1814 (0.1934)  loss_ce_unscaled: 0.1814 (0.1934)  loss_point_unscaled: 50.7193 (77.8497)\n",
      "[ep 1096][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1930 (0.1904)  loss_ce: 0.1930 (0.1904)  loss_ce_unscaled: 0.1930 (0.1904)  loss_point_unscaled: 48.8340 (55.1482)\n",
      "[ep 1097][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1811 (0.1842)  loss_ce: 0.1811 (0.1842)  loss_ce_unscaled: 0.1811 (0.1842)  loss_point_unscaled: 51.0665 (108.1632)\n",
      "[ep 1098][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1683 (0.1859)  loss_ce: 0.1683 (0.1859)  loss_ce_unscaled: 0.1683 (0.1859)  loss_point_unscaled: 50.2185 (88.3326)\n",
      "[ep 1099][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1696 (0.1814)  loss_ce: 0.1696 (0.1814)  loss_ce_unscaled: 0.1696 (0.1814)  loss_point_unscaled: 59.4207 (84.1561)\n",
      "[ep 1100][lr 0.0001000][3.12s]\n",
      "=======================================test=======================================\n",
      "mae: 159.2032967032967 mse: 244.38160350539508 time: 2.3162591457366943 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1795 (0.1861)  loss_ce: 0.1795 (0.1861)  loss_ce_unscaled: 0.1795 (0.1861)  loss_point_unscaled: 52.0302 (66.2056)\n",
      "[ep 1101][lr 0.0001000][2.98s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1812 (0.1843)  loss_ce: 0.1812 (0.1843)  loss_ce_unscaled: 0.1812 (0.1843)  loss_point_unscaled: 62.2311 (85.7666)\n",
      "[ep 1102][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1759 (0.1859)  loss_ce: 0.1759 (0.1859)  loss_ce_unscaled: 0.1759 (0.1859)  loss_point_unscaled: 52.6049 (83.5924)\n",
      "[ep 1103][lr 0.0001000][3.19s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1745 (0.1792)  loss_ce: 0.1745 (0.1792)  loss_ce_unscaled: 0.1745 (0.1792)  loss_point_unscaled: 50.8609 (77.7410)\n",
      "[ep 1104][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1750 (0.1781)  loss_ce: 0.1750 (0.1781)  loss_ce_unscaled: 0.1750 (0.1781)  loss_point_unscaled: 50.8722 (74.1871)\n",
      "[ep 1105][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1821 (0.1851)  loss_ce: 0.1821 (0.1851)  loss_ce_unscaled: 0.1821 (0.1851)  loss_point_unscaled: 52.1681 (79.3028)\n",
      "[ep 1106][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1918 (0.1856)  loss_ce: 0.1918 (0.1856)  loss_ce_unscaled: 0.1918 (0.1856)  loss_point_unscaled: 50.2135 (85.6003)\n",
      "[ep 1107][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1770 (0.1797)  loss_ce: 0.1770 (0.1797)  loss_ce_unscaled: 0.1770 (0.1797)  loss_point_unscaled: 54.6889 (72.5915)\n",
      "[ep 1108][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1827 (0.1830)  loss_ce: 0.1827 (0.1830)  loss_ce_unscaled: 0.1827 (0.1830)  loss_point_unscaled: 57.0052 (96.3928)\n",
      "[ep 1109][lr 0.0001000][2.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1712 (0.1828)  loss_ce: 0.1712 (0.1828)  loss_ce_unscaled: 0.1712 (0.1828)  loss_point_unscaled: 54.2749 (95.8049)\n",
      "[ep 1110][lr 0.0001000][3.19s]\n",
      "=======================================test=======================================\n",
      "mae: 156.4010989010989 mse: 244.65810412101192 time: 4.160559415817261 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1759 (0.1777)  loss_ce: 0.1759 (0.1777)  loss_ce_unscaled: 0.1759 (0.1777)  loss_point_unscaled: 49.8929 (64.3823)\n",
      "[ep 1111][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1820 (0.1868)  loss_ce: 0.1820 (0.1868)  loss_ce_unscaled: 0.1820 (0.1868)  loss_point_unscaled: 55.0065 (70.8285)\n",
      "[ep 1112][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1799 (0.1811)  loss_ce: 0.1799 (0.1811)  loss_ce_unscaled: 0.1799 (0.1811)  loss_point_unscaled: 51.7234 (114.2165)\n",
      "[ep 1113][lr 0.0001000][3.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1807 (0.1840)  loss_ce: 0.1807 (0.1840)  loss_ce_unscaled: 0.1807 (0.1840)  loss_point_unscaled: 56.6285 (95.7274)\n",
      "[ep 1114][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1885)  loss_ce: 0.1809 (0.1885)  loss_ce_unscaled: 0.1809 (0.1885)  loss_point_unscaled: 52.8522 (79.8431)\n",
      "[ep 1115][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1844 (0.1903)  loss_ce: 0.1844 (0.1903)  loss_ce_unscaled: 0.1844 (0.1903)  loss_point_unscaled: 48.1083 (62.2880)\n",
      "[ep 1116][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1784 (0.1785)  loss_ce: 0.1784 (0.1785)  loss_ce_unscaled: 0.1784 (0.1785)  loss_point_unscaled: 50.6916 (74.3571)\n",
      "[ep 1117][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1695 (0.1749)  loss_ce: 0.1695 (0.1749)  loss_ce_unscaled: 0.1695 (0.1749)  loss_point_unscaled: 52.6009 (96.3845)\n",
      "[ep 1118][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1757 (0.1767)  loss_ce: 0.1757 (0.1767)  loss_ce_unscaled: 0.1757 (0.1767)  loss_point_unscaled: 56.1627 (69.4134)\n",
      "[ep 1119][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1793 (0.1851)  loss_ce: 0.1793 (0.1851)  loss_ce_unscaled: 0.1793 (0.1851)  loss_point_unscaled: 55.2372 (83.5316)\n",
      "[ep 1120][lr 0.0001000][3.29s]\n",
      "=======================================test=======================================\n",
      "mae: 167.27472527472528 mse: 244.89708535950854 time: 2.3389341831207275 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1756 (0.1842)  loss_ce: 0.1756 (0.1842)  loss_ce_unscaled: 0.1756 (0.1842)  loss_point_unscaled: 52.4960 (69.7376)\n",
      "[ep 1121][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1708 (0.1821)  loss_ce: 0.1708 (0.1821)  loss_ce_unscaled: 0.1708 (0.1821)  loss_point_unscaled: 57.2875 (71.1757)\n",
      "[ep 1122][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1769 (0.1844)  loss_ce: 0.1769 (0.1844)  loss_ce_unscaled: 0.1769 (0.1844)  loss_point_unscaled: 51.5966 (92.4464)\n",
      "[ep 1123][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1707 (0.1866)  loss_ce: 0.1707 (0.1866)  loss_ce_unscaled: 0.1707 (0.1866)  loss_point_unscaled: 54.9399 (82.7317)\n",
      "[ep 1124][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1774 (0.1830)  loss_ce: 0.1774 (0.1830)  loss_ce_unscaled: 0.1774 (0.1830)  loss_point_unscaled: 51.0674 (88.9147)\n",
      "[ep 1125][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1735 (0.1821)  loss_ce: 0.1735 (0.1821)  loss_ce_unscaled: 0.1735 (0.1821)  loss_point_unscaled: 55.7843 (74.8520)\n",
      "[ep 1126][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1844 (0.1889)  loss_ce: 0.1844 (0.1889)  loss_ce_unscaled: 0.1844 (0.1889)  loss_point_unscaled: 52.6328 (72.6699)\n",
      "[ep 1127][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1723 (0.1800)  loss_ce: 0.1723 (0.1800)  loss_ce_unscaled: 0.1723 (0.1800)  loss_point_unscaled: 52.7884 (62.8049)\n",
      "[ep 1128][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1786 (0.1830)  loss_ce: 0.1786 (0.1830)  loss_ce_unscaled: 0.1786 (0.1830)  loss_point_unscaled: 55.4208 (79.4513)\n",
      "[ep 1129][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1767 (0.1815)  loss_ce: 0.1767 (0.1815)  loss_ce_unscaled: 0.1767 (0.1815)  loss_point_unscaled: 49.0943 (63.9761)\n",
      "[ep 1130][lr 0.0001000][2.57s]\n",
      "=======================================test=======================================\n",
      "mae: 166.52747252747253 mse: 255.54806433783293 time: 4.1610565185546875 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1855 (0.1831)  loss_ce: 0.1855 (0.1831)  loss_ce_unscaled: 0.1855 (0.1831)  loss_point_unscaled: 53.7047 (102.5170)\n",
      "[ep 1131][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1793 (0.1823)  loss_ce: 0.1793 (0.1823)  loss_ce_unscaled: 0.1793 (0.1823)  loss_point_unscaled: 58.8365 (74.6891)\n",
      "[ep 1132][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1584 (0.1722)  loss_ce: 0.1584 (0.1722)  loss_ce_unscaled: 0.1584 (0.1722)  loss_point_unscaled: 50.2449 (69.3407)\n",
      "[ep 1133][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1918 (0.1912)  loss_ce: 0.1918 (0.1912)  loss_ce_unscaled: 0.1918 (0.1912)  loss_point_unscaled: 53.5301 (82.9609)\n",
      "[ep 1134][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1881 (0.1933)  loss_ce: 0.1881 (0.1933)  loss_ce_unscaled: 0.1881 (0.1933)  loss_point_unscaled: 49.8950 (65.3453)\n",
      "[ep 1135][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1749 (0.1831)  loss_ce: 0.1749 (0.1831)  loss_ce_unscaled: 0.1749 (0.1831)  loss_point_unscaled: 54.4438 (60.3791)\n",
      "[ep 1136][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1812 (0.1871)  loss_ce: 0.1812 (0.1871)  loss_ce_unscaled: 0.1812 (0.1871)  loss_point_unscaled: 53.8924 (87.5745)\n",
      "[ep 1137][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1804 (0.1859)  loss_ce: 0.1804 (0.1859)  loss_ce_unscaled: 0.1804 (0.1859)  loss_point_unscaled: 53.0734 (65.6382)\n",
      "[ep 1138][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1799 (0.1811)  loss_ce: 0.1799 (0.1811)  loss_ce_unscaled: 0.1799 (0.1811)  loss_point_unscaled: 47.1640 (62.9612)\n",
      "[ep 1139][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1733 (0.1766)  loss_ce: 0.1733 (0.1766)  loss_ce_unscaled: 0.1733 (0.1766)  loss_point_unscaled: 56.7768 (73.5556)\n",
      "[ep 1140][lr 0.0001000][2.74s]\n",
      "=======================================test=======================================\n",
      "mae: 163.46703296703296 mse: 259.21819270369787 time: 4.202744245529175 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1948 (0.1863)  loss_ce: 0.1948 (0.1863)  loss_ce_unscaled: 0.1948 (0.1863)  loss_point_unscaled: 63.9937 (80.8560)\n",
      "[ep 1141][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1748 (0.1806)  loss_ce: 0.1748 (0.1806)  loss_ce_unscaled: 0.1748 (0.1806)  loss_point_unscaled: 50.2870 (60.5789)\n",
      "[ep 1142][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1803 (0.1892)  loss_ce: 0.1803 (0.1892)  loss_ce_unscaled: 0.1803 (0.1892)  loss_point_unscaled: 49.9744 (66.4320)\n",
      "[ep 1143][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1810 (0.1819)  loss_ce: 0.1810 (0.1819)  loss_ce_unscaled: 0.1810 (0.1819)  loss_point_unscaled: 52.0544 (78.7678)\n",
      "[ep 1144][lr 0.0001000][3.03s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1732 (0.1737)  loss_ce: 0.1732 (0.1737)  loss_ce_unscaled: 0.1732 (0.1737)  loss_point_unscaled: 52.0548 (74.0032)\n",
      "[ep 1145][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1883)  loss_ce: 0.1809 (0.1883)  loss_ce_unscaled: 0.1809 (0.1883)  loss_point_unscaled: 51.9667 (75.5363)\n",
      "[ep 1146][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1679 (0.1792)  loss_ce: 0.1679 (0.1792)  loss_ce_unscaled: 0.1679 (0.1792)  loss_point_unscaled: 55.8159 (104.5794)\n",
      "[ep 1147][lr 0.0001000][2.88s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1768 (0.1817)  loss_ce: 0.1768 (0.1817)  loss_ce_unscaled: 0.1768 (0.1817)  loss_point_unscaled: 52.7076 (116.2617)\n",
      "[ep 1148][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1697 (0.1730)  loss_ce: 0.1697 (0.1730)  loss_ce_unscaled: 0.1697 (0.1730)  loss_point_unscaled: 53.2195 (79.4690)\n",
      "[ep 1149][lr 0.0001000][3.03s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1837 (0.1887)  loss_ce: 0.1837 (0.1887)  loss_ce_unscaled: 0.1837 (0.1887)  loss_point_unscaled: 50.9019 (56.4302)\n",
      "[ep 1150][lr 0.0001000][3.19s]\n",
      "=======================================test=======================================\n",
      "mae: 159.71428571428572 mse: 263.5798980145539 time: 3.876133680343628 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1714 (0.1807)  loss_ce: 0.1714 (0.1807)  loss_ce_unscaled: 0.1714 (0.1807)  loss_point_unscaled: 51.6167 (69.9182)\n",
      "[ep 1151][lr 0.0001000][2.96s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1723 (0.1852)  loss_ce: 0.1723 (0.1852)  loss_ce_unscaled: 0.1723 (0.1852)  loss_point_unscaled: 51.0758 (109.9760)\n",
      "[ep 1152][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1787 (0.1817)  loss_ce: 0.1787 (0.1817)  loss_ce_unscaled: 0.1787 (0.1817)  loss_point_unscaled: 53.7668 (71.9373)\n",
      "[ep 1153][lr 0.0001000][3.04s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1705 (0.1802)  loss_ce: 0.1705 (0.1802)  loss_ce_unscaled: 0.1705 (0.1802)  loss_point_unscaled: 52.8735 (105.9720)\n",
      "[ep 1154][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1757 (0.1811)  loss_ce: 0.1757 (0.1811)  loss_ce_unscaled: 0.1757 (0.1811)  loss_point_unscaled: 48.8889 (88.5257)\n",
      "[ep 1155][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1749 (0.1807)  loss_ce: 0.1749 (0.1807)  loss_ce_unscaled: 0.1749 (0.1807)  loss_point_unscaled: 52.0366 (68.3766)\n",
      "[ep 1156][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1805 (0.1866)  loss_ce: 0.1805 (0.1866)  loss_ce_unscaled: 0.1805 (0.1866)  loss_point_unscaled: 50.9342 (63.9474)\n",
      "[ep 1157][lr 0.0001000][2.51s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1841)  loss_ce: 0.1755 (0.1841)  loss_ce_unscaled: 0.1755 (0.1841)  loss_point_unscaled: 53.3788 (80.4916)\n",
      "[ep 1158][lr 0.0001000][3.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1710 (0.1804)  loss_ce: 0.1710 (0.1804)  loss_ce_unscaled: 0.1710 (0.1804)  loss_point_unscaled: 50.6649 (96.5670)\n",
      "[ep 1159][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1734 (0.1794)  loss_ce: 0.1734 (0.1794)  loss_ce_unscaled: 0.1734 (0.1794)  loss_point_unscaled: 54.2005 (68.4778)\n",
      "[ep 1160][lr 0.0001000][3.19s]\n",
      "=======================================test=======================================\n",
      "mae: 144.02747252747253 mse: 220.66245168224154 time: 2.276139736175537 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1825 (0.1836)  loss_ce: 0.1825 (0.1836)  loss_ce_unscaled: 0.1825 (0.1836)  loss_point_unscaled: 57.2766 (92.1631)\n",
      "[ep 1161][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1767 (0.1820)  loss_ce: 0.1767 (0.1820)  loss_ce_unscaled: 0.1767 (0.1820)  loss_point_unscaled: 53.3978 (58.4066)\n",
      "[ep 1162][lr 0.0001000][2.74s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1846 (0.1889)  loss_ce: 0.1846 (0.1889)  loss_ce_unscaled: 0.1846 (0.1889)  loss_point_unscaled: 48.8274 (68.7463)\n",
      "[ep 1163][lr 0.0001000][2.67s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1828 (0.1829)  loss_ce: 0.1828 (0.1829)  loss_ce_unscaled: 0.1828 (0.1829)  loss_point_unscaled: 58.8572 (88.2338)\n",
      "[ep 1164][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1764 (0.1830)  loss_ce: 0.1764 (0.1830)  loss_ce_unscaled: 0.1764 (0.1830)  loss_point_unscaled: 55.0542 (92.4972)\n",
      "[ep 1165][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1783 (0.1867)  loss_ce: 0.1783 (0.1867)  loss_ce_unscaled: 0.1783 (0.1867)  loss_point_unscaled: 51.1960 (132.5007)\n",
      "[ep 1166][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1803 (0.1865)  loss_ce: 0.1803 (0.1865)  loss_ce_unscaled: 0.1803 (0.1865)  loss_point_unscaled: 54.0837 (93.1895)\n",
      "[ep 1167][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1894)  loss_ce: 0.1851 (0.1894)  loss_ce_unscaled: 0.1851 (0.1894)  loss_point_unscaled: 50.6718 (92.9978)\n",
      "[ep 1168][lr 0.0001000][2.94s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1907 (0.1915)  loss_ce: 0.1907 (0.1915)  loss_ce_unscaled: 0.1907 (0.1915)  loss_point_unscaled: 54.6791 (61.1387)\n",
      "[ep 1169][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1748 (0.1827)  loss_ce: 0.1748 (0.1827)  loss_ce_unscaled: 0.1748 (0.1827)  loss_point_unscaled: 52.4892 (66.6684)\n",
      "[ep 1170][lr 0.0001000][3.27s]\n",
      "=======================================test=======================================\n",
      "mae: 142.93956043956044 mse: 224.33318773841538 time: 4.176934480667114 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1680 (0.1691)  loss_ce: 0.1680 (0.1691)  loss_ce_unscaled: 0.1680 (0.1691)  loss_point_unscaled: 54.5076 (90.5076)\n",
      "[ep 1171][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1750 (0.1814)  loss_ce: 0.1750 (0.1814)  loss_ce_unscaled: 0.1750 (0.1814)  loss_point_unscaled: 52.1646 (79.3955)\n",
      "[ep 1172][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1889 (0.1948)  loss_ce: 0.1889 (0.1948)  loss_ce_unscaled: 0.1889 (0.1948)  loss_point_unscaled: 53.5726 (91.4020)\n",
      "[ep 1173][lr 0.0001000][2.65s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1759 (0.1897)  loss_ce: 0.1759 (0.1897)  loss_ce_unscaled: 0.1759 (0.1897)  loss_point_unscaled: 59.5989 (83.0874)\n",
      "[ep 1174][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1846 (0.1882)  loss_ce: 0.1846 (0.1882)  loss_ce_unscaled: 0.1846 (0.1882)  loss_point_unscaled: 51.3434 (79.3776)\n",
      "[ep 1175][lr 0.0001000][2.65s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1683 (0.1782)  loss_ce: 0.1683 (0.1782)  loss_ce_unscaled: 0.1683 (0.1782)  loss_point_unscaled: 52.9571 (73.5688)\n",
      "[ep 1176][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1745 (0.1780)  loss_ce: 0.1745 (0.1780)  loss_ce_unscaled: 0.1745 (0.1780)  loss_point_unscaled: 53.6113 (103.8719)\n",
      "[ep 1177][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1831 (0.1849)  loss_ce: 0.1831 (0.1849)  loss_ce_unscaled: 0.1831 (0.1849)  loss_point_unscaled: 53.4538 (61.1945)\n",
      "[ep 1178][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1766 (0.1842)  loss_ce: 0.1766 (0.1842)  loss_ce_unscaled: 0.1766 (0.1842)  loss_point_unscaled: 51.8165 (82.1528)\n",
      "[ep 1179][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1705 (0.1740)  loss_ce: 0.1705 (0.1740)  loss_ce_unscaled: 0.1705 (0.1740)  loss_point_unscaled: 51.1444 (82.9075)\n",
      "[ep 1180][lr 0.0001000][3.02s]\n",
      "=======================================test=======================================\n",
      "mae: 160.26373626373626 mse: 255.08890930095657 time: 4.118219614028931 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1774 (0.1771)  loss_ce: 0.1774 (0.1771)  loss_ce_unscaled: 0.1774 (0.1771)  loss_point_unscaled: 53.0102 (74.8762)\n",
      "[ep 1181][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1791 (0.1788)  loss_ce: 0.1791 (0.1788)  loss_ce_unscaled: 0.1791 (0.1788)  loss_point_unscaled: 52.2482 (54.8784)\n",
      "[ep 1182][lr 0.0001000][3.00s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1737 (0.1775)  loss_ce: 0.1737 (0.1775)  loss_ce_unscaled: 0.1737 (0.1775)  loss_point_unscaled: 54.9298 (86.4981)\n",
      "[ep 1183][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1767 (0.1797)  loss_ce: 0.1767 (0.1797)  loss_ce_unscaled: 0.1767 (0.1797)  loss_point_unscaled: 60.7768 (89.5755)\n",
      "[ep 1184][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1844)  loss_ce: 0.1865 (0.1844)  loss_ce_unscaled: 0.1865 (0.1844)  loss_point_unscaled: 52.8131 (81.6109)\n",
      "[ep 1185][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1857)  loss_ce: 0.1808 (0.1857)  loss_ce_unscaled: 0.1808 (0.1857)  loss_point_unscaled: 52.4565 (59.4008)\n",
      "[ep 1186][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1861 (0.1852)  loss_ce: 0.1861 (0.1852)  loss_ce_unscaled: 0.1861 (0.1852)  loss_point_unscaled: 55.9263 (116.4306)\n",
      "[ep 1187][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1769 (0.1772)  loss_ce: 0.1769 (0.1772)  loss_ce_unscaled: 0.1769 (0.1772)  loss_point_unscaled: 62.0060 (73.4465)\n",
      "[ep 1188][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1769 (0.1788)  loss_ce: 0.1769 (0.1788)  loss_ce_unscaled: 0.1769 (0.1788)  loss_point_unscaled: 51.4120 (69.8044)\n",
      "[ep 1189][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1895 (0.1871)  loss_ce: 0.1895 (0.1871)  loss_ce_unscaled: 0.1895 (0.1871)  loss_point_unscaled: 55.1375 (95.2897)\n",
      "[ep 1190][lr 0.0001000][3.22s]\n",
      "=======================================test=======================================\n",
      "mae: 161.06593406593407 mse: 265.91595797036456 time: 2.292172908782959 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1845 (0.1833)  loss_ce: 0.1845 (0.1833)  loss_ce_unscaled: 0.1845 (0.1833)  loss_point_unscaled: 53.5639 (78.2228)\n",
      "[ep 1191][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1734 (0.1751)  loss_ce: 0.1734 (0.1751)  loss_ce_unscaled: 0.1734 (0.1751)  loss_point_unscaled: 60.3014 (63.4381)\n",
      "[ep 1192][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1901 (0.1918)  loss_ce: 0.1901 (0.1918)  loss_ce_unscaled: 0.1901 (0.1918)  loss_point_unscaled: 51.0270 (78.9243)\n",
      "[ep 1193][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1810 (0.1859)  loss_ce: 0.1810 (0.1859)  loss_ce_unscaled: 0.1810 (0.1859)  loss_point_unscaled: 49.5810 (109.4784)\n",
      "[ep 1194][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1796 (0.1830)  loss_ce: 0.1796 (0.1830)  loss_ce_unscaled: 0.1796 (0.1830)  loss_point_unscaled: 52.5784 (59.2445)\n",
      "[ep 1195][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1907 (0.1885)  loss_ce: 0.1907 (0.1885)  loss_ce_unscaled: 0.1907 (0.1885)  loss_point_unscaled: 59.0880 (75.4938)\n",
      "[ep 1196][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1936 (0.1889)  loss_ce: 0.1936 (0.1889)  loss_ce_unscaled: 0.1936 (0.1889)  loss_point_unscaled: 51.7641 (93.6940)\n",
      "[ep 1197][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1742 (0.1822)  loss_ce: 0.1742 (0.1822)  loss_ce_unscaled: 0.1742 (0.1822)  loss_point_unscaled: 54.6652 (90.6731)\n",
      "[ep 1198][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1687 (0.1711)  loss_ce: 0.1687 (0.1711)  loss_ce_unscaled: 0.1687 (0.1711)  loss_point_unscaled: 52.8761 (62.0000)\n",
      "[ep 1199][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1843 (0.1858)  loss_ce: 0.1843 (0.1858)  loss_ce_unscaled: 0.1843 (0.1858)  loss_point_unscaled: 55.6429 (90.2460)\n",
      "[ep 1200][lr 0.0001000][2.54s]\n",
      "=======================================test=======================================\n",
      "mae: 168.25824175824175 mse: 267.7650992038514 time: 3.2889394760131836 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1759 (0.1842)  loss_ce: 0.1759 (0.1842)  loss_ce_unscaled: 0.1759 (0.1842)  loss_point_unscaled: 49.5712 (79.8792)\n",
      "[ep 1201][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1717 (0.1736)  loss_ce: 0.1717 (0.1736)  loss_ce_unscaled: 0.1717 (0.1736)  loss_point_unscaled: 56.4243 (76.9867)\n",
      "[ep 1202][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1827 (0.1816)  loss_ce: 0.1827 (0.1816)  loss_ce_unscaled: 0.1827 (0.1816)  loss_point_unscaled: 61.6501 (112.3237)\n",
      "[ep 1203][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1771 (0.1826)  loss_ce: 0.1771 (0.1826)  loss_ce_unscaled: 0.1771 (0.1826)  loss_point_unscaled: 50.4878 (58.9508)\n",
      "[ep 1204][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1692 (0.1752)  loss_ce: 0.1692 (0.1752)  loss_ce_unscaled: 0.1692 (0.1752)  loss_point_unscaled: 53.4311 (67.7161)\n",
      "[ep 1205][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1774 (0.1812)  loss_ce: 0.1774 (0.1812)  loss_ce_unscaled: 0.1774 (0.1812)  loss_point_unscaled: 51.6484 (85.9722)\n",
      "[ep 1206][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1826 (0.1818)  loss_ce: 0.1826 (0.1818)  loss_ce_unscaled: 0.1826 (0.1818)  loss_point_unscaled: 51.4354 (65.9380)\n",
      "[ep 1207][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1724 (0.1700)  loss_ce: 0.1724 (0.1700)  loss_ce_unscaled: 0.1724 (0.1700)  loss_point_unscaled: 56.5160 (94.6668)\n",
      "[ep 1208][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1863 (0.1891)  loss_ce: 0.1863 (0.1891)  loss_ce_unscaled: 0.1863 (0.1891)  loss_point_unscaled: 49.0945 (83.3042)\n",
      "[ep 1209][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1788 (0.1814)  loss_ce: 0.1788 (0.1814)  loss_ce_unscaled: 0.1788 (0.1814)  loss_point_unscaled: 54.8308 (74.1832)\n",
      "[ep 1210][lr 0.0001000][2.43s]\n",
      "=======================================test=======================================\n",
      "mae: 164.83516483516485 mse: 254.458356013425 time: 4.147810697555542 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1794 (0.1823)  loss_ce: 0.1794 (0.1823)  loss_ce_unscaled: 0.1794 (0.1823)  loss_point_unscaled: 54.9450 (65.3391)\n",
      "[ep 1211][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1767 (0.1802)  loss_ce: 0.1767 (0.1802)  loss_ce_unscaled: 0.1767 (0.1802)  loss_point_unscaled: 50.0407 (55.2518)\n",
      "[ep 1212][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1861 (0.1824)  loss_ce: 0.1861 (0.1824)  loss_ce_unscaled: 0.1861 (0.1824)  loss_point_unscaled: 53.9191 (83.4505)\n",
      "[ep 1213][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1862)  loss_ce: 0.1884 (0.1862)  loss_ce_unscaled: 0.1884 (0.1862)  loss_point_unscaled: 47.1111 (82.5974)\n",
      "[ep 1214][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1820)  loss_ce: 0.1809 (0.1820)  loss_ce_unscaled: 0.1809 (0.1820)  loss_point_unscaled: 51.7711 (77.6437)\n",
      "[ep 1215][lr 0.0001000][2.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1769 (0.1811)  loss_ce: 0.1769 (0.1811)  loss_ce_unscaled: 0.1769 (0.1811)  loss_point_unscaled: 53.0509 (77.2490)\n",
      "[ep 1216][lr 0.0001000][2.61s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1836 (0.1838)  loss_ce: 0.1836 (0.1838)  loss_ce_unscaled: 0.1836 (0.1838)  loss_point_unscaled: 53.3412 (104.9635)\n",
      "[ep 1217][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1783 (0.1747)  loss_ce: 0.1783 (0.1747)  loss_ce_unscaled: 0.1783 (0.1747)  loss_point_unscaled: 53.2999 (78.5513)\n",
      "[ep 1218][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1789 (0.1861)  loss_ce: 0.1789 (0.1861)  loss_ce_unscaled: 0.1789 (0.1861)  loss_point_unscaled: 49.7701 (93.7199)\n",
      "[ep 1219][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1740 (0.1783)  loss_ce: 0.1740 (0.1783)  loss_ce_unscaled: 0.1740 (0.1783)  loss_point_unscaled: 51.1934 (57.0227)\n",
      "[ep 1220][lr 0.0001000][3.04s]\n",
      "=======================================test=======================================\n",
      "mae: 194.9945054945055 mse: 293.02136778402695 time: 4.09765100479126 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1845 (0.1900)  loss_ce: 0.1845 (0.1900)  loss_ce_unscaled: 0.1845 (0.1900)  loss_point_unscaled: 51.3367 (93.2363)\n",
      "[ep 1221][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1828 (0.1840)  loss_ce: 0.1828 (0.1840)  loss_ce_unscaled: 0.1828 (0.1840)  loss_point_unscaled: 51.1874 (71.2655)\n",
      "[ep 1222][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1848 (0.1849)  loss_ce: 0.1848 (0.1849)  loss_ce_unscaled: 0.1848 (0.1849)  loss_point_unscaled: 50.3290 (68.4113)\n",
      "[ep 1223][lr 0.0001000][2.74s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1818 (0.1868)  loss_ce: 0.1818 (0.1868)  loss_ce_unscaled: 0.1818 (0.1868)  loss_point_unscaled: 51.3672 (85.2469)\n",
      "[ep 1224][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1835 (0.1870)  loss_ce: 0.1835 (0.1870)  loss_ce_unscaled: 0.1835 (0.1870)  loss_point_unscaled: 54.7436 (122.9930)\n",
      "[ep 1225][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1734 (0.1797)  loss_ce: 0.1734 (0.1797)  loss_ce_unscaled: 0.1734 (0.1797)  loss_point_unscaled: 51.6696 (52.0607)\n",
      "[ep 1226][lr 0.0001000][2.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1856 (0.1882)  loss_ce: 0.1856 (0.1882)  loss_ce_unscaled: 0.1856 (0.1882)  loss_point_unscaled: 60.3712 (99.2699)\n",
      "[ep 1227][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1849 (0.1897)  loss_ce: 0.1849 (0.1897)  loss_ce_unscaled: 0.1849 (0.1897)  loss_point_unscaled: 51.5181 (118.8106)\n",
      "[ep 1228][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1765 (0.1821)  loss_ce: 0.1765 (0.1821)  loss_ce_unscaled: 0.1765 (0.1821)  loss_point_unscaled: 57.3520 (104.3872)\n",
      "[ep 1229][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1650 (0.1832)  loss_ce: 0.1650 (0.1832)  loss_ce_unscaled: 0.1650 (0.1832)  loss_point_unscaled: 54.2126 (97.1071)\n",
      "[ep 1230][lr 0.0001000][3.26s]\n",
      "=======================================test=======================================\n",
      "mae: 184.3846153846154 mse: 271.9004948889927 time: 2.5537526607513428 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1845 (0.1902)  loss_ce: 0.1845 (0.1902)  loss_ce_unscaled: 0.1845 (0.1902)  loss_point_unscaled: 49.9930 (92.2142)\n",
      "[ep 1231][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1729 (0.1777)  loss_ce: 0.1729 (0.1777)  loss_ce_unscaled: 0.1729 (0.1777)  loss_point_unscaled: 50.6540 (83.9317)\n",
      "[ep 1232][lr 0.0001000][3.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1870 (0.1896)  loss_ce: 0.1870 (0.1896)  loss_ce_unscaled: 0.1870 (0.1896)  loss_point_unscaled: 57.7863 (100.1752)\n",
      "[ep 1233][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1798 (0.1783)  loss_ce: 0.1798 (0.1783)  loss_ce_unscaled: 0.1798 (0.1783)  loss_point_unscaled: 55.1704 (94.8336)\n",
      "[ep 1234][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1692 (0.1764)  loss_ce: 0.1692 (0.1764)  loss_ce_unscaled: 0.1692 (0.1764)  loss_point_unscaled: 57.6388 (79.0287)\n",
      "[ep 1235][lr 0.0001000][3.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1750 (0.1930)  loss_ce: 0.1750 (0.1930)  loss_ce_unscaled: 0.1750 (0.1930)  loss_point_unscaled: 50.9023 (77.2357)\n",
      "[ep 1236][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1801)  loss_ce: 0.1755 (0.1801)  loss_ce_unscaled: 0.1755 (0.1801)  loss_point_unscaled: 54.6454 (86.3005)\n",
      "[ep 1237][lr 0.0001000][2.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1783 (0.1807)  loss_ce: 0.1783 (0.1807)  loss_ce_unscaled: 0.1783 (0.1807)  loss_point_unscaled: 52.9607 (71.8814)\n",
      "[ep 1238][lr 0.0001000][2.98s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1808 (0.1844)  loss_ce: 0.1808 (0.1844)  loss_ce_unscaled: 0.1808 (0.1844)  loss_point_unscaled: 65.5276 (99.3609)\n",
      "[ep 1239][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1812 (0.1778)  loss_ce: 0.1812 (0.1778)  loss_ce_unscaled: 0.1812 (0.1778)  loss_point_unscaled: 53.0114 (86.7969)\n",
      "[ep 1240][lr 0.0001000][3.05s]\n",
      "=======================================test=======================================\n",
      "mae: 161.5 mse: 246.52917042032138 time: 2.3021414279937744 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1814 (0.1805)  loss_ce: 0.1814 (0.1805)  loss_ce_unscaled: 0.1814 (0.1805)  loss_point_unscaled: 61.5688 (117.2713)\n",
      "[ep 1241][lr 0.0001000][3.02s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1795 (0.1886)  loss_ce: 0.1795 (0.1886)  loss_ce_unscaled: 0.1795 (0.1886)  loss_point_unscaled: 50.7275 (87.4067)\n",
      "[ep 1242][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1912 (0.1965)  loss_ce: 0.1912 (0.1965)  loss_ce_unscaled: 0.1912 (0.1965)  loss_point_unscaled: 49.9214 (71.7603)\n",
      "[ep 1243][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1702 (0.1867)  loss_ce: 0.1702 (0.1867)  loss_ce_unscaled: 0.1702 (0.1867)  loss_point_unscaled: 58.2555 (76.5914)\n",
      "[ep 1244][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1877 (0.1860)  loss_ce: 0.1877 (0.1860)  loss_ce_unscaled: 0.1877 (0.1860)  loss_point_unscaled: 52.5996 (82.7786)\n",
      "[ep 1245][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1735 (0.1821)  loss_ce: 0.1735 (0.1821)  loss_ce_unscaled: 0.1735 (0.1821)  loss_point_unscaled: 53.2201 (102.8679)\n",
      "[ep 1246][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1820 (0.1829)  loss_ce: 0.1820 (0.1829)  loss_ce_unscaled: 0.1820 (0.1829)  loss_point_unscaled: 51.3662 (74.3109)\n",
      "[ep 1247][lr 0.0001000][3.04s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1787 (0.1859)  loss_ce: 0.1787 (0.1859)  loss_ce_unscaled: 0.1787 (0.1859)  loss_point_unscaled: 49.9009 (76.4659)\n",
      "[ep 1248][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1678 (0.1751)  loss_ce: 0.1678 (0.1751)  loss_ce_unscaled: 0.1678 (0.1751)  loss_point_unscaled: 53.5848 (75.1900)\n",
      "[ep 1249][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1769 (0.1840)  loss_ce: 0.1769 (0.1840)  loss_ce_unscaled: 0.1769 (0.1840)  loss_point_unscaled: 55.3239 (91.1669)\n",
      "[ep 1250][lr 0.0001000][2.49s]\n",
      "=======================================test=======================================\n",
      "mae: 167.55494505494505 mse: 246.79872343695357 time: 3.087066650390625 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1734 (0.1826)  loss_ce: 0.1734 (0.1826)  loss_ce_unscaled: 0.1734 (0.1826)  loss_point_unscaled: 51.5097 (63.9116)\n",
      "[ep 1251][lr 0.0001000][2.90s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1912 (0.1902)  loss_ce: 0.1912 (0.1902)  loss_ce_unscaled: 0.1912 (0.1902)  loss_point_unscaled: 54.1502 (98.3387)\n",
      "[ep 1252][lr 0.0001000][2.69s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1796 (0.1880)  loss_ce: 0.1796 (0.1880)  loss_ce_unscaled: 0.1796 (0.1880)  loss_point_unscaled: 55.4520 (80.3294)\n",
      "[ep 1253][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1845 (0.1844)  loss_ce: 0.1845 (0.1844)  loss_ce_unscaled: 0.1845 (0.1844)  loss_point_unscaled: 51.6114 (74.0603)\n",
      "[ep 1254][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1730 (0.1858)  loss_ce: 0.1730 (0.1858)  loss_ce_unscaled: 0.1730 (0.1858)  loss_point_unscaled: 59.7139 (81.4443)\n",
      "[ep 1255][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1780 (0.1842)  loss_ce: 0.1780 (0.1842)  loss_ce_unscaled: 0.1780 (0.1842)  loss_point_unscaled: 54.5165 (86.9083)\n",
      "[ep 1256][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1829 (0.1885)  loss_ce: 0.1829 (0.1885)  loss_ce_unscaled: 0.1829 (0.1885)  loss_point_unscaled: 51.7375 (85.5585)\n",
      "[ep 1257][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1812 (0.1876)  loss_ce: 0.1812 (0.1876)  loss_ce_unscaled: 0.1812 (0.1876)  loss_point_unscaled: 49.6219 (64.1942)\n",
      "[ep 1258][lr 0.0001000][2.98s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1788 (0.1834)  loss_ce: 0.1788 (0.1834)  loss_ce_unscaled: 0.1788 (0.1834)  loss_point_unscaled: 54.5559 (67.3745)\n",
      "[ep 1259][lr 0.0001000][2.89s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1807 (0.1850)  loss_ce: 0.1807 (0.1850)  loss_ce_unscaled: 0.1807 (0.1850)  loss_point_unscaled: 57.1227 (68.4785)\n",
      "[ep 1260][lr 0.0001000][3.29s]\n",
      "=======================================test=======================================\n",
      "mae: 145.9010989010989 mse: 223.9333446745769 time: 4.231124401092529 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1742 (0.1859)  loss_ce: 0.1742 (0.1859)  loss_ce_unscaled: 0.1742 (0.1859)  loss_point_unscaled: 50.5904 (63.4495)\n",
      "[ep 1261][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1937 (0.1949)  loss_ce: 0.1937 (0.1949)  loss_ce_unscaled: 0.1937 (0.1949)  loss_point_unscaled: 51.5079 (67.1195)\n",
      "[ep 1262][lr 0.0001000][3.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1764 (0.1824)  loss_ce: 0.1764 (0.1824)  loss_ce_unscaled: 0.1764 (0.1824)  loss_point_unscaled: 51.9838 (68.0069)\n",
      "[ep 1263][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1790 (0.1825)  loss_ce: 0.1790 (0.1825)  loss_ce_unscaled: 0.1790 (0.1825)  loss_point_unscaled: 64.2203 (110.6187)\n",
      "[ep 1264][lr 0.0001000][2.67s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1863 (0.1912)  loss_ce: 0.1863 (0.1912)  loss_ce_unscaled: 0.1863 (0.1912)  loss_point_unscaled: 49.0286 (64.8618)\n",
      "[ep 1265][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1813 (0.1815)  loss_ce: 0.1813 (0.1815)  loss_ce_unscaled: 0.1813 (0.1815)  loss_point_unscaled: 57.4781 (76.4513)\n",
      "[ep 1266][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1735 (0.1954)  loss_ce: 0.1735 (0.1954)  loss_ce_unscaled: 0.1735 (0.1954)  loss_point_unscaled: 51.7433 (76.9203)\n",
      "[ep 1267][lr 0.0001000][3.04s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1931 (0.1848)  loss_ce: 0.1931 (0.1848)  loss_ce_unscaled: 0.1931 (0.1848)  loss_point_unscaled: 49.8173 (92.5861)\n",
      "[ep 1268][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1821 (0.1829)  loss_ce: 0.1821 (0.1829)  loss_ce_unscaled: 0.1821 (0.1829)  loss_point_unscaled: 50.6872 (82.0135)\n",
      "[ep 1269][lr 0.0001000][3.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1743 (0.1803)  loss_ce: 0.1743 (0.1803)  loss_ce_unscaled: 0.1743 (0.1803)  loss_point_unscaled: 50.1338 (81.2293)\n",
      "[ep 1270][lr 0.0001000][3.29s]\n",
      "=======================================test=======================================\n",
      "mae: 206.28571428571428 mse: 331.4555313021179 time: 4.224345445632935 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1779 (0.1829)  loss_ce: 0.1779 (0.1829)  loss_ce_unscaled: 0.1779 (0.1829)  loss_point_unscaled: 63.0535 (68.8519)\n",
      "[ep 1271][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1881 (0.1901)  loss_ce: 0.1881 (0.1901)  loss_ce_unscaled: 0.1881 (0.1901)  loss_point_unscaled: 53.6038 (109.7110)\n",
      "[ep 1272][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1634 (0.1808)  loss_ce: 0.1634 (0.1808)  loss_ce_unscaled: 0.1634 (0.1808)  loss_point_unscaled: 58.7789 (120.4957)\n",
      "[ep 1273][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1864 (0.1864)  loss_ce: 0.1864 (0.1864)  loss_ce_unscaled: 0.1864 (0.1864)  loss_point_unscaled: 51.5621 (94.0898)\n",
      "[ep 1274][lr 0.0001000][2.53s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1812 (0.1862)  loss_ce: 0.1812 (0.1862)  loss_ce_unscaled: 0.1812 (0.1862)  loss_point_unscaled: 50.8052 (80.4826)\n",
      "[ep 1275][lr 0.0001000][2.67s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1831 (0.1841)  loss_ce: 0.1831 (0.1841)  loss_ce_unscaled: 0.1831 (0.1841)  loss_point_unscaled: 51.1807 (74.7532)\n",
      "[ep 1276][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1881 (0.1898)  loss_ce: 0.1881 (0.1898)  loss_ce_unscaled: 0.1881 (0.1898)  loss_point_unscaled: 54.9976 (82.1865)\n",
      "[ep 1277][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1655 (0.1765)  loss_ce: 0.1655 (0.1765)  loss_ce_unscaled: 0.1655 (0.1765)  loss_point_unscaled: 49.4086 (74.1385)\n",
      "[ep 1278][lr 0.0001000][2.78s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1789 (0.1838)  loss_ce: 0.1789 (0.1838)  loss_ce_unscaled: 0.1789 (0.1838)  loss_point_unscaled: 51.0003 (73.8282)\n",
      "[ep 1279][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1896 (0.1884)  loss_ce: 0.1896 (0.1884)  loss_ce_unscaled: 0.1896 (0.1884)  loss_point_unscaled: 51.7198 (94.4792)\n",
      "[ep 1280][lr 0.0001000][3.28s]\n",
      "=======================================test=======================================\n",
      "mae: 157.9120879120879 mse: 239.42185400632118 time: 4.326736211776733 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1730 (0.1820)  loss_ce: 0.1730 (0.1820)  loss_ce_unscaled: 0.1730 (0.1820)  loss_point_unscaled: 52.8346 (68.1255)\n",
      "[ep 1281][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1791 (0.1815)  loss_ce: 0.1791 (0.1815)  loss_ce_unscaled: 0.1791 (0.1815)  loss_point_unscaled: 50.7897 (85.7671)\n",
      "[ep 1282][lr 0.0001000][2.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1651 (0.1801)  loss_ce: 0.1651 (0.1801)  loss_ce_unscaled: 0.1651 (0.1801)  loss_point_unscaled: 60.2506 (86.2008)\n",
      "[ep 1283][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1739 (0.1806)  loss_ce: 0.1739 (0.1806)  loss_ce_unscaled: 0.1739 (0.1806)  loss_point_unscaled: 58.2855 (112.8811)\n",
      "[ep 1284][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1672 (0.1760)  loss_ce: 0.1672 (0.1760)  loss_ce_unscaled: 0.1672 (0.1760)  loss_point_unscaled: 55.5529 (85.6403)\n",
      "[ep 1285][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1851 (0.1829)  loss_ce: 0.1851 (0.1829)  loss_ce_unscaled: 0.1851 (0.1829)  loss_point_unscaled: 50.0010 (65.4508)\n",
      "[ep 1286][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1777 (0.1880)  loss_ce: 0.1777 (0.1880)  loss_ce_unscaled: 0.1777 (0.1880)  loss_point_unscaled: 52.3831 (63.3478)\n",
      "[ep 1287][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1735 (0.1727)  loss_ce: 0.1735 (0.1727)  loss_ce_unscaled: 0.1735 (0.1727)  loss_point_unscaled: 52.6165 (87.7092)\n",
      "[ep 1288][lr 0.0001000][2.96s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1718 (0.1796)  loss_ce: 0.1718 (0.1796)  loss_ce_unscaled: 0.1718 (0.1796)  loss_point_unscaled: 48.6914 (126.3400)\n",
      "[ep 1289][lr 0.0001000][2.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1778 (0.1831)  loss_ce: 0.1778 (0.1831)  loss_ce_unscaled: 0.1778 (0.1831)  loss_point_unscaled: 57.2840 (83.4868)\n",
      "[ep 1290][lr 0.0001000][3.15s]\n",
      "=======================================test=======================================\n",
      "mae: 148.7912087912088 mse: 234.12638645647107 time: 4.091895580291748 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1738 (0.1805)  loss_ce: 0.1738 (0.1805)  loss_ce_unscaled: 0.1738 (0.1805)  loss_point_unscaled: 52.1261 (104.3444)\n",
      "[ep 1291][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1718 (0.1758)  loss_ce: 0.1718 (0.1758)  loss_ce_unscaled: 0.1718 (0.1758)  loss_point_unscaled: 55.1545 (77.9488)\n",
      "[ep 1292][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1736 (0.1816)  loss_ce: 0.1736 (0.1816)  loss_ce_unscaled: 0.1736 (0.1816)  loss_point_unscaled: 56.2613 (61.9271)\n",
      "[ep 1293][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1795 (0.1835)  loss_ce: 0.1795 (0.1835)  loss_ce_unscaled: 0.1795 (0.1835)  loss_point_unscaled: 50.1880 (74.2133)\n",
      "[ep 1294][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1850 (0.1801)  loss_ce: 0.1850 (0.1801)  loss_ce_unscaled: 0.1850 (0.1801)  loss_point_unscaled: 51.4279 (78.1251)\n",
      "[ep 1295][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1834 (0.1844)  loss_ce: 0.1834 (0.1844)  loss_ce_unscaled: 0.1834 (0.1844)  loss_point_unscaled: 51.9710 (100.2320)\n",
      "[ep 1296][lr 0.0001000][2.98s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1840 (0.1811)  loss_ce: 0.1840 (0.1811)  loss_ce_unscaled: 0.1840 (0.1811)  loss_point_unscaled: 56.7265 (90.1735)\n",
      "[ep 1297][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1815 (0.1831)  loss_ce: 0.1815 (0.1831)  loss_ce_unscaled: 0.1815 (0.1831)  loss_point_unscaled: 53.4220 (82.0905)\n",
      "[ep 1298][lr 0.0001000][2.94s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1819)  loss_ce: 0.1832 (0.1819)  loss_ce_unscaled: 0.1832 (0.1819)  loss_point_unscaled: 49.3927 (60.1088)\n",
      "[ep 1299][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1751 (0.1832)  loss_ce: 0.1751 (0.1832)  loss_ce_unscaled: 0.1751 (0.1832)  loss_point_unscaled: 53.6826 (77.2578)\n",
      "[ep 1300][lr 0.0001000][3.13s]\n",
      "=======================================test=======================================\n",
      "mae: 146.98901098901098 mse: 218.95975343969866 time: 2.2943549156188965 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1786 (0.1810)  loss_ce: 0.1786 (0.1810)  loss_ce_unscaled: 0.1786 (0.1810)  loss_point_unscaled: 52.8263 (77.5765)\n",
      "[ep 1301][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1827 (0.1844)  loss_ce: 0.1827 (0.1844)  loss_ce_unscaled: 0.1827 (0.1844)  loss_point_unscaled: 51.0878 (82.9866)\n",
      "[ep 1302][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1889 (0.1880)  loss_ce: 0.1889 (0.1880)  loss_ce_unscaled: 0.1889 (0.1880)  loss_point_unscaled: 51.2770 (58.3230)\n",
      "[ep 1303][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1827 (0.1854)  loss_ce: 0.1827 (0.1854)  loss_ce_unscaled: 0.1827 (0.1854)  loss_point_unscaled: 51.3853 (77.2637)\n",
      "[ep 1304][lr 0.0001000][2.61s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1752 (0.1859)  loss_ce: 0.1752 (0.1859)  loss_ce_unscaled: 0.1752 (0.1859)  loss_point_unscaled: 54.4494 (95.5110)\n",
      "[ep 1305][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1712 (0.1795)  loss_ce: 0.1712 (0.1795)  loss_ce_unscaled: 0.1712 (0.1795)  loss_point_unscaled: 49.3526 (82.0143)\n",
      "[ep 1306][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1805 (0.1795)  loss_ce: 0.1805 (0.1795)  loss_ce_unscaled: 0.1805 (0.1795)  loss_point_unscaled: 51.7394 (61.4314)\n",
      "[ep 1307][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1802 (0.1842)  loss_ce: 0.1802 (0.1842)  loss_ce_unscaled: 0.1802 (0.1842)  loss_point_unscaled: 50.4448 (66.0596)\n",
      "[ep 1308][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1798 (0.1819)  loss_ce: 0.1798 (0.1819)  loss_ce_unscaled: 0.1798 (0.1819)  loss_point_unscaled: 60.1078 (94.8955)\n",
      "[ep 1309][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1861 (0.1831)  loss_ce: 0.1861 (0.1831)  loss_ce_unscaled: 0.1861 (0.1831)  loss_point_unscaled: 52.6395 (78.7723)\n",
      "[ep 1310][lr 0.0001000][2.38s]\n",
      "=======================================test=======================================\n",
      "mae: 147.22527472527472 mse: 223.15897600461554 time: 3.5899817943573 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1805 (0.1839)  loss_ce: 0.1805 (0.1839)  loss_ce_unscaled: 0.1805 (0.1839)  loss_point_unscaled: 54.3247 (85.6520)\n",
      "[ep 1311][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1836 (0.1801)  loss_ce: 0.1836 (0.1801)  loss_ce_unscaled: 0.1836 (0.1801)  loss_point_unscaled: 50.0043 (64.0724)\n",
      "[ep 1312][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1794 (0.1934)  loss_ce: 0.1794 (0.1934)  loss_ce_unscaled: 0.1794 (0.1934)  loss_point_unscaled: 51.7855 (65.0224)\n",
      "[ep 1313][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1832 (0.1787)  loss_ce: 0.1832 (0.1787)  loss_ce_unscaled: 0.1832 (0.1787)  loss_point_unscaled: 52.3710 (79.1747)\n",
      "[ep 1314][lr 0.0001000][2.36s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1758 (0.1821)  loss_ce: 0.1758 (0.1821)  loss_ce_unscaled: 0.1758 (0.1821)  loss_point_unscaled: 55.6221 (85.7877)\n",
      "[ep 1315][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1932 (0.1919)  loss_ce: 0.1932 (0.1919)  loss_ce_unscaled: 0.1932 (0.1919)  loss_point_unscaled: 56.6048 (88.2419)\n",
      "[ep 1316][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1736 (0.1793)  loss_ce: 0.1736 (0.1793)  loss_ce_unscaled: 0.1736 (0.1793)  loss_point_unscaled: 50.9041 (96.4376)\n",
      "[ep 1317][lr 0.0001000][2.93s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1822 (0.1760)  loss_ce: 0.1822 (0.1760)  loss_ce_unscaled: 0.1822 (0.1760)  loss_point_unscaled: 54.1563 (78.9525)\n",
      "[ep 1318][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1765 (0.1828)  loss_ce: 0.1765 (0.1828)  loss_ce_unscaled: 0.1765 (0.1828)  loss_point_unscaled: 51.4557 (53.7970)\n",
      "[ep 1319][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1864 (0.1839)  loss_ce: 0.1864 (0.1839)  loss_ce_unscaled: 0.1864 (0.1839)  loss_point_unscaled: 53.5335 (77.9966)\n",
      "[ep 1320][lr 0.0001000][3.22s]\n",
      "=======================================test=======================================\n",
      "mae: 169.0989010989011 mse: 260.61942275372246 time: 4.134829521179199 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1720 (0.1749)  loss_ce: 0.1720 (0.1749)  loss_ce_unscaled: 0.1720 (0.1749)  loss_point_unscaled: 50.4860 (102.4522)\n",
      "[ep 1321][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1726 (0.1803)  loss_ce: 0.1726 (0.1803)  loss_ce_unscaled: 0.1726 (0.1803)  loss_point_unscaled: 51.0798 (68.0046)\n",
      "[ep 1322][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1822)  loss_ce: 0.1755 (0.1822)  loss_ce_unscaled: 0.1755 (0.1822)  loss_point_unscaled: 50.5950 (73.7716)\n",
      "[ep 1323][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1719 (0.1743)  loss_ce: 0.1719 (0.1743)  loss_ce_unscaled: 0.1719 (0.1743)  loss_point_unscaled: 50.0061 (59.6566)\n",
      "[ep 1324][lr 0.0001000][2.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.2050 (0.1968)  loss_ce: 0.2050 (0.1968)  loss_ce_unscaled: 0.2050 (0.1968)  loss_point_unscaled: 49.0565 (69.0420)\n",
      "[ep 1325][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1692 (0.1836)  loss_ce: 0.1692 (0.1836)  loss_ce_unscaled: 0.1692 (0.1836)  loss_point_unscaled: 50.9093 (105.5916)\n",
      "[ep 1326][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1821 (0.1793)  loss_ce: 0.1821 (0.1793)  loss_ce_unscaled: 0.1821 (0.1793)  loss_point_unscaled: 52.4631 (65.1732)\n",
      "[ep 1327][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1691 (0.1792)  loss_ce: 0.1691 (0.1792)  loss_ce_unscaled: 0.1691 (0.1792)  loss_point_unscaled: 55.1722 (66.8428)\n",
      "[ep 1328][lr 0.0001000][3.04s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1790 (0.1837)  loss_ce: 0.1790 (0.1837)  loss_ce_unscaled: 0.1790 (0.1837)  loss_point_unscaled: 50.2820 (64.7180)\n",
      "[ep 1329][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1843 (0.1786)  loss_ce: 0.1843 (0.1786)  loss_ce_unscaled: 0.1843 (0.1786)  loss_point_unscaled: 50.7068 (83.7715)\n",
      "[ep 1330][lr 0.0001000][3.32s]\n",
      "=======================================test=======================================\n",
      "mae: 185.82967032967034 mse: 294.6444775180219 time: 2.3265798091888428 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1844 (0.1857)  loss_ce: 0.1844 (0.1857)  loss_ce_unscaled: 0.1844 (0.1857)  loss_point_unscaled: 51.8611 (81.9821)\n",
      "[ep 1331][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1807 (0.1866)  loss_ce: 0.1807 (0.1866)  loss_ce_unscaled: 0.1807 (0.1866)  loss_point_unscaled: 50.2433 (63.3895)\n",
      "[ep 1332][lr 0.0001000][2.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1842 (0.1814)  loss_ce: 0.1842 (0.1814)  loss_ce_unscaled: 0.1842 (0.1814)  loss_point_unscaled: 62.7066 (99.0342)\n",
      "[ep 1333][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1815 (0.1810)  loss_ce: 0.1815 (0.1810)  loss_ce_unscaled: 0.1815 (0.1810)  loss_point_unscaled: 53.6154 (97.7455)\n",
      "[ep 1334][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1888 (0.1837)  loss_ce: 0.1888 (0.1837)  loss_ce_unscaled: 0.1888 (0.1837)  loss_point_unscaled: 53.5788 (90.6010)\n",
      "[ep 1335][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1796 (0.1800)  loss_ce: 0.1796 (0.1800)  loss_ce_unscaled: 0.1796 (0.1800)  loss_point_unscaled: 57.3661 (71.1947)\n",
      "[ep 1336][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1846)  loss_ce: 0.1839 (0.1846)  loss_ce_unscaled: 0.1839 (0.1846)  loss_point_unscaled: 50.5123 (72.8452)\n",
      "[ep 1337][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1753 (0.1753)  loss_ce: 0.1753 (0.1753)  loss_ce_unscaled: 0.1753 (0.1753)  loss_point_unscaled: 51.2189 (54.3917)\n",
      "[ep 1338][lr 0.0001000][2.62s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1689 (0.1806)  loss_ce: 0.1689 (0.1806)  loss_ce_unscaled: 0.1689 (0.1806)  loss_point_unscaled: 55.2026 (72.0157)\n",
      "[ep 1339][lr 0.0001000][2.93s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1749)  loss_ce: 0.1755 (0.1749)  loss_ce_unscaled: 0.1755 (0.1749)  loss_point_unscaled: 56.5418 (101.9586)\n",
      "[ep 1340][lr 0.0001000][3.23s]\n",
      "=======================================test=======================================\n",
      "mae: 149.42307692307693 mse: 222.37853827587384 time: 2.2943778038024902 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1690 (0.1723)  loss_ce: 0.1690 (0.1723)  loss_ce_unscaled: 0.1690 (0.1723)  loss_point_unscaled: 51.3374 (76.5913)\n",
      "[ep 1341][lr 0.0001000][2.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1786 (0.1845)  loss_ce: 0.1786 (0.1845)  loss_ce_unscaled: 0.1786 (0.1845)  loss_point_unscaled: 55.3174 (78.1253)\n",
      "[ep 1342][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1787 (0.1779)  loss_ce: 0.1787 (0.1779)  loss_ce_unscaled: 0.1787 (0.1779)  loss_point_unscaled: 49.7151 (64.9803)\n",
      "[ep 1343][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1841 (0.1863)  loss_ce: 0.1841 (0.1863)  loss_ce_unscaled: 0.1841 (0.1863)  loss_point_unscaled: 54.0485 (97.6006)\n",
      "[ep 1344][lr 0.0001000][2.51s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1788 (0.1815)  loss_ce: 0.1788 (0.1815)  loss_ce_unscaled: 0.1788 (0.1815)  loss_point_unscaled: 54.0568 (78.8163)\n",
      "[ep 1345][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1911 (0.1925)  loss_ce: 0.1911 (0.1925)  loss_ce_unscaled: 0.1911 (0.1925)  loss_point_unscaled: 52.0728 (74.5799)\n",
      "[ep 1346][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1624 (0.1758)  loss_ce: 0.1624 (0.1758)  loss_ce_unscaled: 0.1624 (0.1758)  loss_point_unscaled: 49.8204 (56.8445)\n",
      "[ep 1347][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1664 (0.1727)  loss_ce: 0.1664 (0.1727)  loss_ce_unscaled: 0.1664 (0.1727)  loss_point_unscaled: 52.4404 (92.7769)\n",
      "[ep 1348][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1764 (0.1877)  loss_ce: 0.1764 (0.1877)  loss_ce_unscaled: 0.1764 (0.1877)  loss_point_unscaled: 53.6355 (118.8148)\n",
      "[ep 1349][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1838 (0.1819)  loss_ce: 0.1838 (0.1819)  loss_ce_unscaled: 0.1838 (0.1819)  loss_point_unscaled: 56.7919 (82.2477)\n",
      "[ep 1350][lr 0.0001000][2.45s]\n",
      "=======================================test=======================================\n",
      "mae: 170.3846153846154 mse: 264.86646023655345 time: 2.364189863204956 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1751 (0.1788)  loss_ce: 0.1751 (0.1788)  loss_ce_unscaled: 0.1751 (0.1788)  loss_point_unscaled: 53.9552 (77.5616)\n",
      "[ep 1351][lr 0.0001000][2.50s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1784 (0.1790)  loss_ce: 0.1784 (0.1790)  loss_ce_unscaled: 0.1784 (0.1790)  loss_point_unscaled: 51.5484 (56.9501)\n",
      "[ep 1352][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1901 (0.1902)  loss_ce: 0.1901 (0.1902)  loss_ce_unscaled: 0.1901 (0.1902)  loss_point_unscaled: 53.0294 (67.7993)\n",
      "[ep 1353][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1895 (0.1876)  loss_ce: 0.1895 (0.1876)  loss_ce_unscaled: 0.1895 (0.1876)  loss_point_unscaled: 51.1640 (95.4867)\n",
      "[ep 1354][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1675 (0.1762)  loss_ce: 0.1675 (0.1762)  loss_ce_unscaled: 0.1675 (0.1762)  loss_point_unscaled: 52.1062 (89.4490)\n",
      "[ep 1355][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1794 (0.1861)  loss_ce: 0.1794 (0.1861)  loss_ce_unscaled: 0.1794 (0.1861)  loss_point_unscaled: 51.9750 (72.6731)\n",
      "[ep 1356][lr 0.0001000][3.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1781 (0.1741)  loss_ce: 0.1781 (0.1741)  loss_ce_unscaled: 0.1781 (0.1741)  loss_point_unscaled: 49.4296 (54.3794)\n",
      "[ep 1357][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1724 (0.1790)  loss_ce: 0.1724 (0.1790)  loss_ce_unscaled: 0.1724 (0.1790)  loss_point_unscaled: 52.6759 (62.3279)\n",
      "[ep 1358][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1817 (0.1910)  loss_ce: 0.1817 (0.1910)  loss_ce_unscaled: 0.1817 (0.1910)  loss_point_unscaled: 51.5067 (89.9266)\n",
      "[ep 1359][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1806 (0.1855)  loss_ce: 0.1806 (0.1855)  loss_ce_unscaled: 0.1806 (0.1855)  loss_point_unscaled: 52.9644 (89.2331)\n",
      "[ep 1360][lr 0.0001000][3.12s]\n",
      "=======================================test=======================================\n",
      "mae: 151.67032967032966 mse: 230.46601141509439 time: 2.291806697845459 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1763 (0.1836)  loss_ce: 0.1763 (0.1836)  loss_ce_unscaled: 0.1763 (0.1836)  loss_point_unscaled: 55.4335 (78.8779)\n",
      "[ep 1361][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1865 (0.1791)  loss_ce: 0.1865 (0.1791)  loss_ce_unscaled: 0.1865 (0.1791)  loss_point_unscaled: 51.3249 (86.3924)\n",
      "[ep 1362][lr 0.0001000][2.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1708 (0.1761)  loss_ce: 0.1708 (0.1761)  loss_ce_unscaled: 0.1708 (0.1761)  loss_point_unscaled: 51.8209 (69.5715)\n",
      "[ep 1363][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1732 (0.1796)  loss_ce: 0.1732 (0.1796)  loss_ce_unscaled: 0.1732 (0.1796)  loss_point_unscaled: 55.8674 (89.3174)\n",
      "[ep 1364][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1758 (0.1801)  loss_ce: 0.1758 (0.1801)  loss_ce_unscaled: 0.1758 (0.1801)  loss_point_unscaled: 56.0860 (68.9977)\n",
      "[ep 1365][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1914 (0.1878)  loss_ce: 0.1914 (0.1878)  loss_ce_unscaled: 0.1914 (0.1878)  loss_point_unscaled: 50.0558 (67.0289)\n",
      "[ep 1366][lr 0.0001000][2.96s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1791 (0.1821)  loss_ce: 0.1791 (0.1821)  loss_ce_unscaled: 0.1791 (0.1821)  loss_point_unscaled: 53.5169 (69.7278)\n",
      "[ep 1367][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1699 (0.1812)  loss_ce: 0.1699 (0.1812)  loss_ce_unscaled: 0.1699 (0.1812)  loss_point_unscaled: 51.1981 (67.1621)\n",
      "[ep 1368][lr 0.0001000][3.29s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1806 (0.1822)  loss_ce: 0.1806 (0.1822)  loss_ce_unscaled: 0.1806 (0.1822)  loss_point_unscaled: 55.4652 (76.7008)\n",
      "[ep 1369][lr 0.0001000][2.84s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1721 (0.1782)  loss_ce: 0.1721 (0.1782)  loss_ce_unscaled: 0.1721 (0.1782)  loss_point_unscaled: 53.3131 (98.6713)\n",
      "[ep 1370][lr 0.0001000][3.06s]\n",
      "=======================================test=======================================\n",
      "mae: 151.94505494505495 mse: 234.98692972771303 time: 4.154996156692505 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1774 (0.1811)  loss_ce: 0.1774 (0.1811)  loss_ce_unscaled: 0.1774 (0.1811)  loss_point_unscaled: 57.8350 (70.5801)\n",
      "[ep 1371][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1799 (0.1824)  loss_ce: 0.1799 (0.1824)  loss_ce_unscaled: 0.1799 (0.1824)  loss_point_unscaled: 51.1281 (74.8640)\n",
      "[ep 1372][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1870 (0.1849)  loss_ce: 0.1870 (0.1849)  loss_ce_unscaled: 0.1870 (0.1849)  loss_point_unscaled: 51.0020 (71.7647)\n",
      "[ep 1373][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1888 (0.1844)  loss_ce: 0.1888 (0.1844)  loss_ce_unscaled: 0.1888 (0.1844)  loss_point_unscaled: 53.2143 (75.5580)\n",
      "[ep 1374][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1827 (0.1814)  loss_ce: 0.1827 (0.1814)  loss_ce_unscaled: 0.1827 (0.1814)  loss_point_unscaled: 53.7722 (77.0807)\n",
      "[ep 1375][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1950 (0.1858)  loss_ce: 0.1950 (0.1858)  loss_ce_unscaled: 0.1950 (0.1858)  loss_point_unscaled: 53.9450 (83.0216)\n",
      "[ep 1376][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1714 (0.1727)  loss_ce: 0.1714 (0.1727)  loss_ce_unscaled: 0.1714 (0.1727)  loss_point_unscaled: 56.5158 (78.6643)\n",
      "[ep 1377][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1757 (0.1800)  loss_ce: 0.1757 (0.1800)  loss_ce_unscaled: 0.1757 (0.1800)  loss_point_unscaled: 55.0295 (99.5492)\n",
      "[ep 1378][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1806 (0.1850)  loss_ce: 0.1806 (0.1850)  loss_ce_unscaled: 0.1806 (0.1850)  loss_point_unscaled: 49.9560 (75.2199)\n",
      "[ep 1379][lr 0.0001000][2.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1761 (0.1888)  loss_ce: 0.1761 (0.1888)  loss_ce_unscaled: 0.1761 (0.1888)  loss_point_unscaled: 50.7780 (80.4412)\n",
      "[ep 1380][lr 0.0001000][2.41s]\n",
      "=======================================test=======================================\n",
      "mae: 168.7087912087912 mse: 249.6597794890188 time: 2.235507011413574 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1800 (0.1779)  loss_ce: 0.1800 (0.1779)  loss_ce_unscaled: 0.1800 (0.1779)  loss_point_unscaled: 52.6391 (65.0412)\n",
      "[ep 1381][lr 0.0001000][3.01s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1722 (0.1762)  loss_ce: 0.1722 (0.1762)  loss_ce_unscaled: 0.1722 (0.1762)  loss_point_unscaled: 52.4949 (90.9892)\n",
      "[ep 1382][lr 0.0001000][2.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1827 (0.1872)  loss_ce: 0.1827 (0.1872)  loss_ce_unscaled: 0.1827 (0.1872)  loss_point_unscaled: 50.3185 (82.4086)\n",
      "[ep 1383][lr 0.0001000][3.15s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1734 (0.1790)  loss_ce: 0.1734 (0.1790)  loss_ce_unscaled: 0.1734 (0.1790)  loss_point_unscaled: 53.1017 (67.8066)\n",
      "[ep 1384][lr 0.0001000][3.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1651 (0.1735)  loss_ce: 0.1651 (0.1735)  loss_ce_unscaled: 0.1651 (0.1735)  loss_point_unscaled: 56.3896 (95.0584)\n",
      "[ep 1385][lr 0.0001000][2.95s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1738 (0.1759)  loss_ce: 0.1738 (0.1759)  loss_ce_unscaled: 0.1738 (0.1759)  loss_point_unscaled: 55.1019 (92.0834)\n",
      "[ep 1386][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1656 (0.1730)  loss_ce: 0.1656 (0.1730)  loss_ce_unscaled: 0.1656 (0.1730)  loss_point_unscaled: 51.1327 (70.2401)\n",
      "[ep 1387][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1774 (0.1915)  loss_ce: 0.1774 (0.1915)  loss_ce_unscaled: 0.1774 (0.1915)  loss_point_unscaled: 50.7165 (75.6571)\n",
      "[ep 1388][lr 0.0001000][3.07s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1720 (0.1815)  loss_ce: 0.1720 (0.1815)  loss_ce_unscaled: 0.1720 (0.1815)  loss_point_unscaled: 51.1880 (78.9066)\n",
      "[ep 1389][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1763 (0.1781)  loss_ce: 0.1763 (0.1781)  loss_ce_unscaled: 0.1763 (0.1781)  loss_point_unscaled: 60.5780 (106.4685)\n",
      "[ep 1390][lr 0.0001000][3.27s]\n",
      "=======================================test=======================================\n",
      "mae: 149.7912087912088 mse: 233.06969669282088 time: 4.124483823776245 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1785 (0.1773)  loss_ce: 0.1785 (0.1773)  loss_ce_unscaled: 0.1785 (0.1773)  loss_point_unscaled: 53.3848 (79.9514)\n",
      "[ep 1391][lr 0.0001000][3.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1867 (0.1869)  loss_ce: 0.1867 (0.1869)  loss_ce_unscaled: 0.1867 (0.1869)  loss_point_unscaled: 53.2424 (71.0895)\n",
      "[ep 1392][lr 0.0001000][2.83s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1700 (0.1858)  loss_ce: 0.1700 (0.1858)  loss_ce_unscaled: 0.1700 (0.1858)  loss_point_unscaled: 58.8117 (75.3601)\n",
      "[ep 1393][lr 0.0001000][3.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1787 (0.1791)  loss_ce: 0.1787 (0.1791)  loss_ce_unscaled: 0.1787 (0.1791)  loss_point_unscaled: 49.3926 (72.6472)\n",
      "[ep 1394][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1822 (0.1820)  loss_ce: 0.1822 (0.1820)  loss_ce_unscaled: 0.1822 (0.1820)  loss_point_unscaled: 52.3411 (68.8899)\n",
      "[ep 1395][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1739 (0.1802)  loss_ce: 0.1739 (0.1802)  loss_ce_unscaled: 0.1739 (0.1802)  loss_point_unscaled: 51.6366 (90.0944)\n",
      "[ep 1396][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1792 (0.1803)  loss_ce: 0.1792 (0.1803)  loss_ce_unscaled: 0.1792 (0.1803)  loss_point_unscaled: 54.7418 (103.9326)\n",
      "[ep 1397][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1948 (0.1958)  loss_ce: 0.1948 (0.1958)  loss_ce_unscaled: 0.1948 (0.1958)  loss_point_unscaled: 77.3823 (146.7119)\n",
      "[ep 1398][lr 0.0001000][3.35s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1858 (0.1846)  loss_ce: 0.1858 (0.1846)  loss_ce_unscaled: 0.1858 (0.1846)  loss_point_unscaled: 56.1148 (85.0573)\n",
      "[ep 1399][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1921 (0.1846)  loss_ce: 0.1921 (0.1846)  loss_ce_unscaled: 0.1921 (0.1846)  loss_point_unscaled: 56.1423 (68.2717)\n",
      "[ep 1400][lr 0.0001000][3.13s]\n",
      "=======================================test=======================================\n",
      "mae: 158.2967032967033 mse: 240.68526254092833 time: 2.297543525695801 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1723 (0.1826)  loss_ce: 0.1723 (0.1826)  loss_ce_unscaled: 0.1723 (0.1826)  loss_point_unscaled: 54.1556 (68.8791)\n",
      "[ep 1401][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1874 (0.1922)  loss_ce: 0.1874 (0.1922)  loss_ce_unscaled: 0.1874 (0.1922)  loss_point_unscaled: 51.0341 (90.9597)\n",
      "[ep 1402][lr 0.0001000][2.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1784 (0.1831)  loss_ce: 0.1784 (0.1831)  loss_ce_unscaled: 0.1784 (0.1831)  loss_point_unscaled: 49.8372 (92.1555)\n",
      "[ep 1403][lr 0.0001000][3.28s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1723 (0.1809)  loss_ce: 0.1723 (0.1809)  loss_ce_unscaled: 0.1723 (0.1809)  loss_point_unscaled: 54.2305 (95.8922)\n",
      "[ep 1404][lr 0.0001000][2.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1714 (0.1750)  loss_ce: 0.1714 (0.1750)  loss_ce_unscaled: 0.1714 (0.1750)  loss_point_unscaled: 56.8976 (104.0359)\n",
      "[ep 1405][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1866 (0.1824)  loss_ce: 0.1866 (0.1824)  loss_ce_unscaled: 0.1866 (0.1824)  loss_point_unscaled: 51.7824 (93.3437)\n",
      "[ep 1406][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1822 (0.1850)  loss_ce: 0.1822 (0.1850)  loss_ce_unscaled: 0.1822 (0.1850)  loss_point_unscaled: 52.2018 (78.4086)\n",
      "[ep 1407][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1750 (0.1801)  loss_ce: 0.1750 (0.1801)  loss_ce_unscaled: 0.1750 (0.1801)  loss_point_unscaled: 53.3563 (111.5912)\n",
      "[ep 1408][lr 0.0001000][3.08s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1810 (0.1850)  loss_ce: 0.1810 (0.1850)  loss_ce_unscaled: 0.1810 (0.1850)  loss_point_unscaled: 52.4655 (86.5744)\n",
      "[ep 1409][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1723 (0.1754)  loss_ce: 0.1723 (0.1754)  loss_ce_unscaled: 0.1723 (0.1754)  loss_point_unscaled: 50.9985 (94.0693)\n",
      "[ep 1410][lr 0.0001000][2.48s]\n",
      "=======================================test=======================================\n",
      "mae: 177.43956043956044 mse: 284.50778779970386 time: 4.141195058822632 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1741 (0.1813)  loss_ce: 0.1741 (0.1813)  loss_ce_unscaled: 0.1741 (0.1813)  loss_point_unscaled: 52.2518 (65.6753)\n",
      "[ep 1411][lr 0.0001000][2.49s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1676 (0.1763)  loss_ce: 0.1676 (0.1763)  loss_ce_unscaled: 0.1676 (0.1763)  loss_point_unscaled: 50.9892 (73.7185)\n",
      "[ep 1412][lr 0.0001000][3.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1702 (0.1806)  loss_ce: 0.1702 (0.1806)  loss_ce_unscaled: 0.1702 (0.1806)  loss_point_unscaled: 51.5255 (74.7779)\n",
      "[ep 1413][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1780 (0.1790)  loss_ce: 0.1780 (0.1790)  loss_ce_unscaled: 0.1780 (0.1790)  loss_point_unscaled: 50.2827 (75.6494)\n",
      "[ep 1414][lr 0.0001000][3.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1756 (0.1839)  loss_ce: 0.1756 (0.1839)  loss_ce_unscaled: 0.1756 (0.1839)  loss_point_unscaled: 57.6339 (69.5490)\n",
      "[ep 1415][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1756 (0.1782)  loss_ce: 0.1756 (0.1782)  loss_ce_unscaled: 0.1756 (0.1782)  loss_point_unscaled: 51.3853 (75.4046)\n",
      "[ep 1416][lr 0.0001000][2.34s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1750 (0.1791)  loss_ce: 0.1750 (0.1791)  loss_ce_unscaled: 0.1750 (0.1791)  loss_point_unscaled: 52.5201 (59.3744)\n",
      "[ep 1417][lr 0.0001000][3.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1797 (0.1802)  loss_ce: 0.1797 (0.1802)  loss_ce_unscaled: 0.1797 (0.1802)  loss_point_unscaled: 51.0388 (76.8008)\n",
      "[ep 1418][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1829)  loss_ce: 0.1872 (0.1829)  loss_ce_unscaled: 0.1872 (0.1829)  loss_point_unscaled: 53.3973 (117.6658)\n",
      "[ep 1419][lr 0.0001000][3.26s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1790 (0.1836)  loss_ce: 0.1790 (0.1836)  loss_ce_unscaled: 0.1790 (0.1836)  loss_point_unscaled: 48.7381 (61.1109)\n",
      "[ep 1420][lr 0.0001000][2.46s]\n",
      "=======================================test=======================================\n",
      "mae: 147.51098901098902 mse: 231.56160028224613 time: 4.303725957870483 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1836 (0.1825)  loss_ce: 0.1836 (0.1825)  loss_ce_unscaled: 0.1836 (0.1825)  loss_point_unscaled: 54.2776 (78.8584)\n",
      "[ep 1421][lr 0.0001000][2.44s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1802 (0.1822)  loss_ce: 0.1802 (0.1822)  loss_ce_unscaled: 0.1802 (0.1822)  loss_point_unscaled: 49.1260 (66.3315)\n",
      "[ep 1422][lr 0.0001000][3.09s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1748 (0.1814)  loss_ce: 0.1748 (0.1814)  loss_ce_unscaled: 0.1748 (0.1814)  loss_point_unscaled: 51.0487 (81.1293)\n",
      "[ep 1423][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1780 (0.1855)  loss_ce: 0.1780 (0.1855)  loss_ce_unscaled: 0.1780 (0.1855)  loss_point_unscaled: 51.5300 (66.3966)\n",
      "[ep 1424][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1777 (0.1800)  loss_ce: 0.1777 (0.1800)  loss_ce_unscaled: 0.1777 (0.1800)  loss_point_unscaled: 52.8497 (63.3407)\n",
      "[ep 1425][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1750 (0.1802)  loss_ce: 0.1750 (0.1802)  loss_ce_unscaled: 0.1750 (0.1802)  loss_point_unscaled: 53.6262 (75.3593)\n",
      "[ep 1426][lr 0.0001000][3.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1880 (0.1859)  loss_ce: 0.1880 (0.1859)  loss_ce_unscaled: 0.1880 (0.1859)  loss_point_unscaled: 47.0647 (82.7546)\n",
      "[ep 1427][lr 0.0001000][3.05s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1714 (0.1795)  loss_ce: 0.1714 (0.1795)  loss_ce_unscaled: 0.1714 (0.1795)  loss_point_unscaled: 52.5888 (89.2397)\n",
      "[ep 1428][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1753 (0.1811)  loss_ce: 0.1753 (0.1811)  loss_ce_unscaled: 0.1753 (0.1811)  loss_point_unscaled: 54.0512 (92.6742)\n",
      "[ep 1429][lr 0.0001000][3.16s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1733 (0.1777)  loss_ce: 0.1733 (0.1777)  loss_ce_unscaled: 0.1733 (0.1777)  loss_point_unscaled: 53.3574 (102.7947)\n",
      "[ep 1430][lr 0.0001000][2.35s]\n",
      "=======================================test=======================================\n",
      "mae: 173.71978021978023 mse: 260.8791244665191 time: 2.2198421955108643 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1725 (0.1775)  loss_ce: 0.1725 (0.1775)  loss_ce_unscaled: 0.1725 (0.1775)  loss_point_unscaled: 52.5133 (88.4559)\n",
      "[ep 1431][lr 0.0001000][2.88s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1710 (0.1820)  loss_ce: 0.1710 (0.1820)  loss_ce_unscaled: 0.1710 (0.1820)  loss_point_unscaled: 55.0982 (80.2981)\n",
      "[ep 1432][lr 0.0001000][3.56s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1827 (0.1861)  loss_ce: 0.1827 (0.1861)  loss_ce_unscaled: 0.1827 (0.1861)  loss_point_unscaled: 54.3285 (79.6501)\n",
      "[ep 1433][lr 0.0001000][2.52s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1822 (0.1797)  loss_ce: 0.1822 (0.1797)  loss_ce_unscaled: 0.1822 (0.1797)  loss_point_unscaled: 53.5205 (107.1061)\n",
      "[ep 1434][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1744 (0.1834)  loss_ce: 0.1744 (0.1834)  loss_ce_unscaled: 0.1744 (0.1834)  loss_point_unscaled: 54.0195 (78.8151)\n",
      "[ep 1435][lr 0.0001000][3.43s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1797 (0.1800)  loss_ce: 0.1797 (0.1800)  loss_ce_unscaled: 0.1797 (0.1800)  loss_point_unscaled: 57.1661 (69.2281)\n",
      "[ep 1436][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1745 (0.1803)  loss_ce: 0.1745 (0.1803)  loss_ce_unscaled: 0.1745 (0.1803)  loss_point_unscaled: 56.5908 (96.4601)\n",
      "[ep 1437][lr 0.0001000][3.14s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1815 (0.1850)  loss_ce: 0.1815 (0.1850)  loss_ce_unscaled: 0.1815 (0.1850)  loss_point_unscaled: 51.9925 (96.2358)\n",
      "[ep 1438][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1753 (0.1839)  loss_ce: 0.1753 (0.1839)  loss_ce_unscaled: 0.1753 (0.1839)  loss_point_unscaled: 51.8284 (80.3070)\n",
      "[ep 1439][lr 0.0001000][2.99s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1846 (0.1838)  loss_ce: 0.1846 (0.1838)  loss_ce_unscaled: 0.1846 (0.1838)  loss_point_unscaled: 52.3925 (81.4070)\n",
      "[ep 1440][lr 0.0001000][3.16s]\n",
      "=======================================test=======================================\n",
      "mae: 156.8186813186813 mse: 258.973067036998 time: 2.3217735290527344 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1776 (0.1826)  loss_ce: 0.1776 (0.1826)  loss_ce_unscaled: 0.1776 (0.1826)  loss_point_unscaled: 52.3548 (97.8428)\n",
      "[ep 1441][lr 0.0001000][2.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1780 (0.1842)  loss_ce: 0.1780 (0.1842)  loss_ce_unscaled: 0.1780 (0.1842)  loss_point_unscaled: 49.4814 (67.7532)\n",
      "[ep 1442][lr 0.0001000][2.63s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1721 (0.1785)  loss_ce: 0.1721 (0.1785)  loss_ce_unscaled: 0.1721 (0.1785)  loss_point_unscaled: 55.1896 (94.4419)\n",
      "[ep 1443][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1861 (0.1892)  loss_ce: 0.1861 (0.1892)  loss_ce_unscaled: 0.1861 (0.1892)  loss_point_unscaled: 52.4622 (92.0588)\n",
      "[ep 1444][lr 0.0001000][2.38s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1759 (0.1808)  loss_ce: 0.1759 (0.1808)  loss_ce_unscaled: 0.1759 (0.1808)  loss_point_unscaled: 55.3624 (86.1371)\n",
      "[ep 1445][lr 0.0001000][3.11s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1733 (0.1762)  loss_ce: 0.1733 (0.1762)  loss_ce_unscaled: 0.1733 (0.1762)  loss_point_unscaled: 56.7732 (87.2322)\n",
      "[ep 1446][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1925 (0.1922)  loss_ce: 0.1925 (0.1922)  loss_ce_unscaled: 0.1925 (0.1922)  loss_point_unscaled: 54.7128 (114.8665)\n",
      "[ep 1447][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1883)  loss_ce: 0.1872 (0.1883)  loss_ce_unscaled: 0.1872 (0.1883)  loss_point_unscaled: 51.8149 (111.5688)\n",
      "[ep 1448][lr 0.0001000][3.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1713 (0.1787)  loss_ce: 0.1713 (0.1787)  loss_ce_unscaled: 0.1713 (0.1787)  loss_point_unscaled: 50.8270 (74.2984)\n",
      "[ep 1449][lr 0.0001000][2.27s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1758 (0.1802)  loss_ce: 0.1758 (0.1802)  loss_ce_unscaled: 0.1758 (0.1802)  loss_point_unscaled: 55.2095 (65.1357)\n",
      "[ep 1450][lr 0.0001000][3.05s]\n",
      "=======================================test=======================================\n",
      "mae: 163.28021978021977 mse: 238.8947420982676 time: 4.099591255187988 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1836 (0.1825)  loss_ce: 0.1836 (0.1825)  loss_ce_unscaled: 0.1836 (0.1825)  loss_point_unscaled: 50.2072 (106.3100)\n",
      "[ep 1451][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1890 (0.1854)  loss_ce: 0.1890 (0.1854)  loss_ce_unscaled: 0.1890 (0.1854)  loss_point_unscaled: 50.0921 (68.5315)\n",
      "[ep 1452][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1830 (0.1879)  loss_ce: 0.1830 (0.1879)  loss_ce_unscaled: 0.1830 (0.1879)  loss_point_unscaled: 56.8165 (85.0631)\n",
      "[ep 1453][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1831)  loss_ce: 0.1755 (0.1831)  loss_ce_unscaled: 0.1755 (0.1831)  loss_point_unscaled: 54.1157 (83.4419)\n",
      "[ep 1454][lr 0.0001000][2.45s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1775 (0.1833)  loss_ce: 0.1775 (0.1833)  loss_ce_unscaled: 0.1775 (0.1833)  loss_point_unscaled: 56.3072 (75.2038)\n",
      "[ep 1455][lr 0.0001000][3.22s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1757 (0.1773)  loss_ce: 0.1757 (0.1773)  loss_ce_unscaled: 0.1757 (0.1773)  loss_point_unscaled: 51.3795 (67.2486)\n",
      "[ep 1456][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1898 (0.1877)  loss_ce: 0.1898 (0.1877)  loss_ce_unscaled: 0.1898 (0.1877)  loss_point_unscaled: 62.9731 (78.4032)\n",
      "[ep 1457][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1843 (0.1833)  loss_ce: 0.1843 (0.1833)  loss_ce_unscaled: 0.1843 (0.1833)  loss_point_unscaled: 51.6801 (76.7512)\n",
      "[ep 1458][lr 0.0001000][3.20s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1773 (0.1845)  loss_ce: 0.1773 (0.1845)  loss_ce_unscaled: 0.1773 (0.1845)  loss_point_unscaled: 52.3526 (70.5402)\n",
      "[ep 1459][lr 0.0001000][2.41s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1725 (0.1818)  loss_ce: 0.1725 (0.1818)  loss_ce_unscaled: 0.1725 (0.1818)  loss_point_unscaled: 53.4111 (66.9832)\n",
      "[ep 1460][lr 0.0001000][3.40s]\n",
      "=======================================test=======================================\n",
      "mae: 161.78021978021977 mse: 237.81128819130683 time: 4.17592978477478 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1785 (0.1777)  loss_ce: 0.1785 (0.1777)  loss_ce_unscaled: 0.1785 (0.1777)  loss_point_unscaled: 50.2776 (85.1467)\n",
      "[ep 1461][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1782 (0.1830)  loss_ce: 0.1782 (0.1830)  loss_ce_unscaled: 0.1782 (0.1830)  loss_point_unscaled: 51.5401 (66.3213)\n",
      "[ep 1462][lr 0.0001000][2.96s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1680 (0.1791)  loss_ce: 0.1680 (0.1791)  loss_ce_unscaled: 0.1680 (0.1791)  loss_point_unscaled: 54.1674 (99.8018)\n",
      "[ep 1463][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1757 (0.1789)  loss_ce: 0.1757 (0.1789)  loss_ce_unscaled: 0.1757 (0.1789)  loss_point_unscaled: 54.6566 (77.8068)\n",
      "[ep 1464][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1860 (0.1813)  loss_ce: 0.1860 (0.1813)  loss_ce_unscaled: 0.1860 (0.1813)  loss_point_unscaled: 51.4001 (89.3616)\n",
      "[ep 1465][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1831 (0.1917)  loss_ce: 0.1831 (0.1917)  loss_ce_unscaled: 0.1831 (0.1917)  loss_point_unscaled: 52.6375 (70.9994)\n",
      "[ep 1466][lr 0.0001000][2.47s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1854 (0.1863)  loss_ce: 0.1854 (0.1863)  loss_ce_unscaled: 0.1854 (0.1863)  loss_point_unscaled: 50.5839 (52.4464)\n",
      "[ep 1467][lr 0.0001000][2.42s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1795 (0.1834)  loss_ce: 0.1795 (0.1834)  loss_ce_unscaled: 0.1795 (0.1834)  loss_point_unscaled: 53.3511 (66.2312)\n",
      "[ep 1468][lr 0.0001000][2.62s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1804 (0.1836)  loss_ce: 0.1804 (0.1836)  loss_ce_unscaled: 0.1804 (0.1836)  loss_point_unscaled: 56.6139 (85.6155)\n",
      "[ep 1469][lr 0.0001000][2.39s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1822 (0.1809)  loss_ce: 0.1822 (0.1809)  loss_ce_unscaled: 0.1822 (0.1809)  loss_point_unscaled: 48.9060 (66.6291)\n",
      "[ep 1470][lr 0.0001000][3.17s]\n",
      "=======================================test=======================================\n",
      "mae: 185.92307692307693 mse: 272.47798580298695 time: 2.313194990158081 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1751 (0.1740)  loss_ce: 0.1751 (0.1740)  loss_ce_unscaled: 0.1751 (0.1740)  loss_point_unscaled: 55.6866 (91.3989)\n",
      "[ep 1471][lr 0.0001000][3.17s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1758 (0.1801)  loss_ce: 0.1758 (0.1801)  loss_ce_unscaled: 0.1758 (0.1801)  loss_point_unscaled: 52.6734 (69.8064)\n",
      "[ep 1472][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1823 (0.1838)  loss_ce: 0.1823 (0.1838)  loss_ce_unscaled: 0.1823 (0.1838)  loss_point_unscaled: 51.8333 (82.6176)\n",
      "[ep 1473][lr 0.0001000][3.10s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1891 (0.1914)  loss_ce: 0.1891 (0.1914)  loss_ce_unscaled: 0.1891 (0.1914)  loss_point_unscaled: 50.9111 (97.8733)\n",
      "[ep 1474][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1894 (0.1897)  loss_ce: 0.1894 (0.1897)  loss_ce_unscaled: 0.1894 (0.1897)  loss_point_unscaled: 53.5189 (87.6216)\n",
      "[ep 1475][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1750 (0.1865)  loss_ce: 0.1750 (0.1865)  loss_ce_unscaled: 0.1750 (0.1865)  loss_point_unscaled: 53.6142 (86.2072)\n",
      "[ep 1476][lr 0.0001000][3.31s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1795 (0.1792)  loss_ce: 0.1795 (0.1792)  loss_ce_unscaled: 0.1795 (0.1792)  loss_point_unscaled: 54.1215 (98.8123)\n",
      "[ep 1477][lr 0.0001000][2.91s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1773 (0.1859)  loss_ce: 0.1773 (0.1859)  loss_ce_unscaled: 0.1773 (0.1859)  loss_point_unscaled: 53.2225 (106.6107)\n",
      "[ep 1478][lr 0.0001000][3.30s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1824 (0.1853)  loss_ce: 0.1824 (0.1853)  loss_ce_unscaled: 0.1824 (0.1853)  loss_point_unscaled: 52.2439 (86.9744)\n",
      "[ep 1479][lr 0.0001000][3.13s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1749 (0.1776)  loss_ce: 0.1749 (0.1776)  loss_ce_unscaled: 0.1749 (0.1776)  loss_point_unscaled: 50.6812 (78.0105)\n",
      "[ep 1480][lr 0.0001000][3.29s]\n",
      "=======================================test=======================================\n",
      "mae: 180.78021978021977 mse: 277.3842100769256 time: 4.126915454864502 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1750 (0.1769)  loss_ce: 0.1750 (0.1769)  loss_ce_unscaled: 0.1750 (0.1769)  loss_point_unscaled: 49.7239 (73.7048)\n",
      "[ep 1481][lr 0.0001000][3.25s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1908 (0.1834)  loss_ce: 0.1908 (0.1834)  loss_ce_unscaled: 0.1908 (0.1834)  loss_point_unscaled: 57.9067 (83.3486)\n",
      "[ep 1482][lr 0.0001000][3.18s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1755 (0.1783)  loss_ce: 0.1755 (0.1783)  loss_ce_unscaled: 0.1755 (0.1783)  loss_point_unscaled: 57.1573 (85.7550)\n",
      "[ep 1483][lr 0.0001000][2.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1676 (0.1759)  loss_ce: 0.1676 (0.1759)  loss_ce_unscaled: 0.1676 (0.1759)  loss_point_unscaled: 53.1050 (129.5740)\n",
      "[ep 1484][lr 0.0001000][2.64s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1805 (0.1853)  loss_ce: 0.1805 (0.1853)  loss_ce_unscaled: 0.1805 (0.1853)  loss_point_unscaled: 54.9953 (82.7346)\n",
      "[ep 1485][lr 0.0001000][3.23s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1917 (0.1851)  loss_ce: 0.1917 (0.1851)  loss_ce_unscaled: 0.1917 (0.1851)  loss_point_unscaled: 49.5354 (85.6605)\n",
      "[ep 1486][lr 0.0001000][2.33s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1831)  loss_ce: 0.1809 (0.1831)  loss_ce_unscaled: 0.1809 (0.1831)  loss_point_unscaled: 50.6622 (71.8384)\n",
      "[ep 1487][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1713 (0.1831)  loss_ce: 0.1713 (0.1831)  loss_ce_unscaled: 0.1713 (0.1831)  loss_point_unscaled: 69.7900 (108.6963)\n",
      "[ep 1488][lr 0.0001000][3.37s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1757 (0.1774)  loss_ce: 0.1757 (0.1774)  loss_ce_unscaled: 0.1757 (0.1774)  loss_point_unscaled: 50.3438 (75.4479)\n",
      "[ep 1489][lr 0.0001000][2.46s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1772 (0.1726)  loss_ce: 0.1772 (0.1726)  loss_ce_unscaled: 0.1772 (0.1726)  loss_point_unscaled: 51.8104 (68.7526)\n",
      "[ep 1490][lr 0.0001000][2.32s]\n",
      "=======================================test=======================================\n",
      "mae: 156.47252747252747 mse: 231.84972871085424 time: 2.270437479019165 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000100  loss: 0.1783 (0.1799)  loss_ce: 0.1783 (0.1799)  loss_ce_unscaled: 0.1783 (0.1799)  loss_point_unscaled: 49.7091 (79.1789)\n",
      "[ep 1491][lr 0.0001000][3.24s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1709 (0.1741)  loss_ce: 0.1709 (0.1741)  loss_ce_unscaled: 0.1709 (0.1741)  loss_point_unscaled: 52.3226 (67.9264)\n",
      "[ep 1492][lr 0.0001000][2.99s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1833 (0.1883)  loss_ce: 0.1833 (0.1883)  loss_ce_unscaled: 0.1833 (0.1883)  loss_point_unscaled: 55.5581 (124.2972)\n",
      "[ep 1493][lr 0.0001000][3.32s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1788 (0.1817)  loss_ce: 0.1788 (0.1817)  loss_ce_unscaled: 0.1788 (0.1817)  loss_point_unscaled: 53.1817 (90.8268)\n",
      "[ep 1494][lr 0.0001000][3.21s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1761 (0.1786)  loss_ce: 0.1761 (0.1786)  loss_ce_unscaled: 0.1761 (0.1786)  loss_point_unscaled: 52.6929 (76.4894)\n",
      "[ep 1495][lr 0.0001000][3.12s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1708 (0.1816)  loss_ce: 0.1708 (0.1816)  loss_ce_unscaled: 0.1708 (0.1816)  loss_point_unscaled: 54.5228 (90.6508)\n",
      "[ep 1496][lr 0.0001000][2.86s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1809 (0.1832)  loss_ce: 0.1809 (0.1832)  loss_ce_unscaled: 0.1809 (0.1832)  loss_point_unscaled: 56.5258 (74.1829)\n",
      "[ep 1497][lr 0.0001000][2.57s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1882 (0.1865)  loss_ce: 0.1882 (0.1865)  loss_ce_unscaled: 0.1882 (0.1865)  loss_point_unscaled: 51.3666 (63.3404)\n",
      "[ep 1498][lr 0.0001000][3.06s]\n",
      "Averaged stats: lr: 0.000100  loss: 0.1695 (0.1766)  loss_ce: 0.1695 (0.1766)  loss_ce_unscaled: 0.1695 (0.1766)  loss_point_unscaled: 56.6049 (86.6308)\n",
      "[ep 1499][lr 0.0001000][2.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1819 (0.1817)  loss_ce: 0.1819 (0.1817)  loss_ce_unscaled: 0.1819 (0.1817)  loss_point_unscaled: 50.8445 (74.4009)\n",
      "[ep 1500][lr 0.0000100][2.77s]\n",
      "=======================================test=======================================\n",
      "mae: 176.6098901098901 mse: 279.2919520958212 time: 4.347767353057861 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1801 (0.1803)  loss_ce: 0.1801 (0.1803)  loss_ce_unscaled: 0.1801 (0.1803)  loss_point_unscaled: 58.4212 (86.6521)\n",
      "[ep 1501][lr 0.0000100][2.56s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1720 (0.1731)  loss_ce: 0.1720 (0.1731)  loss_ce_unscaled: 0.1720 (0.1731)  loss_point_unscaled: 56.2445 (90.5319)\n",
      "[ep 1502][lr 0.0000100][3.12s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1807 (0.1807)  loss_ce: 0.1807 (0.1807)  loss_ce_unscaled: 0.1807 (0.1807)  loss_point_unscaled: 53.6923 (89.6272)\n",
      "[ep 1503][lr 0.0000100][2.90s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1801 (0.1789)  loss_ce: 0.1801 (0.1789)  loss_ce_unscaled: 0.1801 (0.1789)  loss_point_unscaled: 51.7467 (102.8983)\n",
      "[ep 1504][lr 0.0000100][3.44s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1853 (0.1840)  loss_ce: 0.1853 (0.1840)  loss_ce_unscaled: 0.1853 (0.1840)  loss_point_unscaled: 52.4569 (73.8872)\n",
      "[ep 1505][lr 0.0000100][2.38s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1698 (0.1764)  loss_ce: 0.1698 (0.1764)  loss_ce_unscaled: 0.1698 (0.1764)  loss_point_unscaled: 60.0321 (90.9820)\n",
      "[ep 1506][lr 0.0000100][3.33s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1698 (0.1758)  loss_ce: 0.1698 (0.1758)  loss_ce_unscaled: 0.1698 (0.1758)  loss_point_unscaled: 53.4044 (93.5199)\n",
      "[ep 1507][lr 0.0000100][2.43s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1671 (0.1697)  loss_ce: 0.1671 (0.1697)  loss_ce_unscaled: 0.1671 (0.1697)  loss_point_unscaled: 53.6256 (82.5059)\n",
      "[ep 1508][lr 0.0000100][3.14s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1683 (0.1693)  loss_ce: 0.1683 (0.1693)  loss_ce_unscaled: 0.1683 (0.1693)  loss_point_unscaled: 54.9356 (90.0323)\n",
      "[ep 1509][lr 0.0000100][2.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1744 (0.1794)  loss_ce: 0.1744 (0.1794)  loss_ce_unscaled: 0.1744 (0.1794)  loss_point_unscaled: 49.5652 (75.9282)\n",
      "[ep 1510][lr 0.0000100][3.16s]\n",
      "=======================================test=======================================\n",
      "mae: 150.06043956043956 mse: 235.77859096203784 time: 4.183973550796509 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1833 (0.1853)  loss_ce: 0.1833 (0.1853)  loss_ce_unscaled: 0.1833 (0.1853)  loss_point_unscaled: 57.5441 (92.8656)\n",
      "[ep 1511][lr 0.0000100][3.30s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1777 (0.1765)  loss_ce: 0.1777 (0.1765)  loss_ce_unscaled: 0.1777 (0.1765)  loss_point_unscaled: 50.5840 (77.8694)\n",
      "[ep 1512][lr 0.0000100][2.47s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1790 (0.1755)  loss_ce: 0.1790 (0.1755)  loss_ce_unscaled: 0.1790 (0.1755)  loss_point_unscaled: 52.6768 (86.8258)\n",
      "[ep 1513][lr 0.0000100][3.29s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1685 (0.1665)  loss_ce: 0.1685 (0.1665)  loss_ce_unscaled: 0.1685 (0.1665)  loss_point_unscaled: 57.8927 (91.0160)\n",
      "[ep 1514][lr 0.0000100][3.13s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1753 (0.1739)  loss_ce: 0.1753 (0.1739)  loss_ce_unscaled: 0.1753 (0.1739)  loss_point_unscaled: 53.4258 (103.8875)\n",
      "[ep 1515][lr 0.0000100][2.31s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1748 (0.1705)  loss_ce: 0.1748 (0.1705)  loss_ce_unscaled: 0.1748 (0.1705)  loss_point_unscaled: 52.3420 (109.1230)\n",
      "[ep 1516][lr 0.0000100][2.42s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1731 (0.1756)  loss_ce: 0.1731 (0.1756)  loss_ce_unscaled: 0.1731 (0.1756)  loss_point_unscaled: 54.2152 (71.7808)\n",
      "[ep 1517][lr 0.0000100][3.12s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1638 (0.1681)  loss_ce: 0.1638 (0.1681)  loss_ce_unscaled: 0.1638 (0.1681)  loss_point_unscaled: 59.2454 (88.3201)\n",
      "[ep 1518][lr 0.0000100][3.03s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1647 (0.1706)  loss_ce: 0.1647 (0.1706)  loss_ce_unscaled: 0.1647 (0.1706)  loss_point_unscaled: 55.4929 (86.8713)\n",
      "[ep 1519][lr 0.0000100][3.15s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1696 (0.1784)  loss_ce: 0.1696 (0.1784)  loss_ce_unscaled: 0.1696 (0.1784)  loss_point_unscaled: 54.6090 (69.9065)\n",
      "[ep 1520][lr 0.0000100][3.22s]\n",
      "=======================================test=======================================\n",
      "mae: 151.0054945054945 mse: 236.7643493916623 time: 2.3050403594970703 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1595 (0.1678)  loss_ce: 0.1595 (0.1678)  loss_ce_unscaled: 0.1595 (0.1678)  loss_point_unscaled: 51.8073 (61.1229)\n",
      "[ep 1521][lr 0.0000100][2.85s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1712 (0.1763)  loss_ce: 0.1712 (0.1763)  loss_ce_unscaled: 0.1712 (0.1763)  loss_point_unscaled: 53.9327 (110.8224)\n",
      "[ep 1522][lr 0.0000100][3.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1830 (0.1886)  loss_ce: 0.1830 (0.1886)  loss_ce_unscaled: 0.1830 (0.1886)  loss_point_unscaled: 53.7886 (66.0123)\n",
      "[ep 1523][lr 0.0000100][3.36s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1652 (0.1726)  loss_ce: 0.1652 (0.1726)  loss_ce_unscaled: 0.1652 (0.1726)  loss_point_unscaled: 53.6557 (105.6869)\n",
      "[ep 1524][lr 0.0000100][2.75s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1685 (0.1745)  loss_ce: 0.1685 (0.1745)  loss_ce_unscaled: 0.1685 (0.1745)  loss_point_unscaled: 51.3781 (91.3135)\n",
      "[ep 1525][lr 0.0000100][3.24s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1690 (0.1718)  loss_ce: 0.1690 (0.1718)  loss_ce_unscaled: 0.1690 (0.1718)  loss_point_unscaled: 53.0232 (103.5577)\n",
      "[ep 1526][lr 0.0000100][2.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1833 (0.1806)  loss_ce: 0.1833 (0.1806)  loss_ce_unscaled: 0.1833 (0.1806)  loss_point_unscaled: 57.9247 (85.1572)\n",
      "[ep 1527][lr 0.0000100][3.17s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1736 (0.1775)  loss_ce: 0.1736 (0.1775)  loss_ce_unscaled: 0.1736 (0.1775)  loss_point_unscaled: 52.0118 (63.8660)\n",
      "[ep 1528][lr 0.0000100][3.17s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1691 (0.1705)  loss_ce: 0.1691 (0.1705)  loss_ce_unscaled: 0.1691 (0.1705)  loss_point_unscaled: 56.4834 (85.8407)\n",
      "[ep 1529][lr 0.0000100][3.31s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1776 (0.1838)  loss_ce: 0.1776 (0.1838)  loss_ce_unscaled: 0.1776 (0.1838)  loss_point_unscaled: 52.8296 (71.5433)\n",
      "[ep 1530][lr 0.0000100][3.22s]\n",
      "=======================================test=======================================\n",
      "mae: 148.34065934065933 mse: 233.76695448884345 time: 4.098966360092163 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1658 (0.1759)  loss_ce: 0.1658 (0.1759)  loss_ce_unscaled: 0.1658 (0.1759)  loss_point_unscaled: 50.3822 (74.3719)\n",
      "[ep 1531][lr 0.0000100][2.36s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1684 (0.1693)  loss_ce: 0.1684 (0.1693)  loss_ce_unscaled: 0.1684 (0.1693)  loss_point_unscaled: 59.7754 (94.6484)\n",
      "[ep 1532][lr 0.0000100][3.08s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1920 (0.1861)  loss_ce: 0.1920 (0.1861)  loss_ce_unscaled: 0.1920 (0.1861)  loss_point_unscaled: 54.6188 (98.5539)\n",
      "[ep 1533][lr 0.0000100][3.32s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1666 (0.1690)  loss_ce: 0.1666 (0.1690)  loss_ce_unscaled: 0.1666 (0.1690)  loss_point_unscaled: 52.0230 (58.8361)\n",
      "[ep 1534][lr 0.0000100][3.18s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1784 (0.1763)  loss_ce: 0.1784 (0.1763)  loss_ce_unscaled: 0.1784 (0.1763)  loss_point_unscaled: 51.1453 (65.6552)\n",
      "[ep 1535][lr 0.0000100][2.44s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1756 (0.1878)  loss_ce: 0.1756 (0.1878)  loss_ce_unscaled: 0.1756 (0.1878)  loss_point_unscaled: 52.3046 (95.1365)\n",
      "[ep 1536][lr 0.0000100][3.27s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1861 (0.1862)  loss_ce: 0.1861 (0.1862)  loss_ce_unscaled: 0.1861 (0.1862)  loss_point_unscaled: 52.1397 (65.9754)\n",
      "[ep 1537][lr 0.0000100][2.53s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1634 (0.1724)  loss_ce: 0.1634 (0.1724)  loss_ce_unscaled: 0.1634 (0.1724)  loss_point_unscaled: 54.6279 (109.9824)\n",
      "[ep 1538][lr 0.0000100][3.24s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1674 (0.1701)  loss_ce: 0.1674 (0.1701)  loss_ce_unscaled: 0.1674 (0.1701)  loss_point_unscaled: 55.0940 (81.8470)\n",
      "[ep 1539][lr 0.0000100][3.09s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1700 (0.1783)  loss_ce: 0.1700 (0.1783)  loss_ce_unscaled: 0.1700 (0.1783)  loss_point_unscaled: 50.2963 (85.5161)\n",
      "[ep 1540][lr 0.0000100][2.48s]\n",
      "=======================================test=======================================\n",
      "mae: 147.07692307692307 mse: 227.9352375730276 time: 2.236400842666626 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1737 (0.1787)  loss_ce: 0.1737 (0.1787)  loss_ce_unscaled: 0.1737 (0.1787)  loss_point_unscaled: 58.6584 (154.3870)\n",
      "[ep 1541][lr 0.0000100][3.36s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1602 (0.1712)  loss_ce: 0.1602 (0.1712)  loss_ce_unscaled: 0.1602 (0.1712)  loss_point_unscaled: 55.9103 (84.0598)\n",
      "[ep 1542][lr 0.0000100][2.37s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1724 (0.1789)  loss_ce: 0.1724 (0.1789)  loss_ce_unscaled: 0.1724 (0.1789)  loss_point_unscaled: 53.3580 (101.0316)\n",
      "[ep 1543][lr 0.0000100][3.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1695 (0.1754)  loss_ce: 0.1695 (0.1754)  loss_ce_unscaled: 0.1695 (0.1754)  loss_point_unscaled: 50.7485 (67.4328)\n",
      "[ep 1544][lr 0.0000100][3.09s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1750 (0.1731)  loss_ce: 0.1750 (0.1731)  loss_ce_unscaled: 0.1750 (0.1731)  loss_point_unscaled: 56.7937 (71.4288)\n",
      "[ep 1545][lr 0.0000100][2.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1697 (0.1725)  loss_ce: 0.1697 (0.1725)  loss_ce_unscaled: 0.1697 (0.1725)  loss_point_unscaled: 52.1768 (70.2466)\n",
      "[ep 1546][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1698 (0.1737)  loss_ce: 0.1698 (0.1737)  loss_ce_unscaled: 0.1698 (0.1737)  loss_point_unscaled: 55.2773 (101.8387)\n",
      "[ep 1547][lr 0.0000100][3.24s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1721 (0.1791)  loss_ce: 0.1721 (0.1791)  loss_ce_unscaled: 0.1721 (0.1791)  loss_point_unscaled: 52.3440 (108.8249)\n",
      "[ep 1548][lr 0.0000100][3.09s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1773 (0.1757)  loss_ce: 0.1773 (0.1757)  loss_ce_unscaled: 0.1773 (0.1757)  loss_point_unscaled: 51.4492 (73.1780)\n",
      "[ep 1549][lr 0.0000100][2.38s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1706 (0.1769)  loss_ce: 0.1706 (0.1769)  loss_ce_unscaled: 0.1706 (0.1769)  loss_point_unscaled: 52.6885 (63.0747)\n",
      "[ep 1550][lr 0.0000100][3.22s]\n",
      "=======================================test=======================================\n",
      "mae: 149.35714285714286 mse: 233.646052870529 time: 2.240382671356201 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1673 (0.1724)  loss_ce: 0.1673 (0.1724)  loss_ce_unscaled: 0.1673 (0.1724)  loss_point_unscaled: 52.9663 (102.7474)\n",
      "[ep 1551][lr 0.0000100][3.21s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1688 (0.1740)  loss_ce: 0.1688 (0.1740)  loss_ce_unscaled: 0.1688 (0.1740)  loss_point_unscaled: 52.5022 (66.9970)\n",
      "[ep 1552][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1694 (0.1672)  loss_ce: 0.1694 (0.1672)  loss_ce_unscaled: 0.1694 (0.1672)  loss_point_unscaled: 59.2805 (74.1200)\n",
      "[ep 1553][lr 0.0000100][3.04s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1723 (0.1762)  loss_ce: 0.1723 (0.1762)  loss_ce_unscaled: 0.1723 (0.1762)  loss_point_unscaled: 50.2262 (67.8760)\n",
      "[ep 1554][lr 0.0000100][2.44s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1696 (0.1721)  loss_ce: 0.1696 (0.1721)  loss_ce_unscaled: 0.1696 (0.1721)  loss_point_unscaled: 50.6658 (63.5105)\n",
      "[ep 1555][lr 0.0000100][2.47s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1743 (0.1767)  loss_ce: 0.1743 (0.1767)  loss_ce_unscaled: 0.1743 (0.1767)  loss_point_unscaled: 54.0364 (89.6890)\n",
      "[ep 1556][lr 0.0000100][3.17s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1718 (0.1743)  loss_ce: 0.1718 (0.1743)  loss_ce_unscaled: 0.1718 (0.1743)  loss_point_unscaled: 77.5336 (131.5102)\n",
      "[ep 1557][lr 0.0000100][3.13s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1801 (0.1809)  loss_ce: 0.1801 (0.1809)  loss_ce_unscaled: 0.1801 (0.1809)  loss_point_unscaled: 52.2240 (102.3612)\n",
      "[ep 1558][lr 0.0000100][3.18s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1661 (0.1726)  loss_ce: 0.1661 (0.1726)  loss_ce_unscaled: 0.1661 (0.1726)  loss_point_unscaled: 53.5781 (99.5323)\n",
      "[ep 1559][lr 0.0000100][2.45s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1649 (0.1710)  loss_ce: 0.1649 (0.1710)  loss_ce_unscaled: 0.1649 (0.1710)  loss_point_unscaled: 53.2001 (70.8522)\n",
      "[ep 1560][lr 0.0000100][3.13s]\n",
      "=======================================test=======================================\n",
      "mae: 150.9010989010989 mse: 237.50530358450786 time: 2.289707899093628 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1807 (0.1845)  loss_ce: 0.1807 (0.1845)  loss_ce_unscaled: 0.1807 (0.1845)  loss_point_unscaled: 54.4690 (85.9055)\n",
      "[ep 1561][lr 0.0000100][2.56s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1710 (0.1700)  loss_ce: 0.1710 (0.1700)  loss_ce_unscaled: 0.1710 (0.1700)  loss_point_unscaled: 53.1426 (88.6861)\n",
      "[ep 1562][lr 0.0000100][2.42s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1766 (0.1736)  loss_ce: 0.1766 (0.1736)  loss_ce_unscaled: 0.1766 (0.1736)  loss_point_unscaled: 54.7661 (86.4338)\n",
      "[ep 1563][lr 0.0000100][2.33s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1767 (0.1788)  loss_ce: 0.1767 (0.1788)  loss_ce_unscaled: 0.1767 (0.1788)  loss_point_unscaled: 54.0939 (93.8257)\n",
      "[ep 1564][lr 0.0000100][2.45s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1796 (0.1799)  loss_ce: 0.1796 (0.1799)  loss_ce_unscaled: 0.1796 (0.1799)  loss_point_unscaled: 56.9378 (84.8914)\n",
      "[ep 1565][lr 0.0000100][3.38s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1689 (0.1783)  loss_ce: 0.1689 (0.1783)  loss_ce_unscaled: 0.1689 (0.1783)  loss_point_unscaled: 50.6925 (86.9179)\n",
      "[ep 1566][lr 0.0000100][3.16s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1715 (0.1697)  loss_ce: 0.1715 (0.1697)  loss_ce_unscaled: 0.1715 (0.1697)  loss_point_unscaled: 58.2607 (123.7390)\n",
      "[ep 1567][lr 0.0000100][2.49s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1634 (0.1732)  loss_ce: 0.1634 (0.1732)  loss_ce_unscaled: 0.1634 (0.1732)  loss_point_unscaled: 55.0909 (73.3010)\n",
      "[ep 1568][lr 0.0000100][2.44s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1695 (0.1734)  loss_ce: 0.1695 (0.1734)  loss_ce_unscaled: 0.1695 (0.1734)  loss_point_unscaled: 52.4666 (92.0139)\n",
      "[ep 1569][lr 0.0000100][3.23s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1659 (0.1716)  loss_ce: 0.1659 (0.1716)  loss_ce_unscaled: 0.1659 (0.1716)  loss_point_unscaled: 55.8094 (69.9335)\n",
      "[ep 1570][lr 0.0000100][3.14s]\n",
      "=======================================test=======================================\n",
      "mae: 147.06593406593407 mse: 227.5243254716266 time: 4.267015218734741 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1830 (0.1780)  loss_ce: 0.1830 (0.1780)  loss_ce_unscaled: 0.1830 (0.1780)  loss_point_unscaled: 61.6299 (111.7953)\n",
      "[ep 1571][lr 0.0000100][2.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1761 (0.1784)  loss_ce: 0.1761 (0.1784)  loss_ce_unscaled: 0.1761 (0.1784)  loss_point_unscaled: 55.9296 (105.0914)\n",
      "[ep 1572][lr 0.0000100][3.35s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1726 (0.1769)  loss_ce: 0.1726 (0.1769)  loss_ce_unscaled: 0.1726 (0.1769)  loss_point_unscaled: 53.1211 (70.8794)\n",
      "[ep 1573][lr 0.0000100][2.48s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1653 (0.1727)  loss_ce: 0.1653 (0.1727)  loss_ce_unscaled: 0.1653 (0.1727)  loss_point_unscaled: 52.1325 (72.5590)\n",
      "[ep 1574][lr 0.0000100][3.35s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1604 (0.1674)  loss_ce: 0.1604 (0.1674)  loss_ce_unscaled: 0.1604 (0.1674)  loss_point_unscaled: 54.6730 (96.4290)\n",
      "[ep 1575][lr 0.0000100][3.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1645 (0.1704)  loss_ce: 0.1645 (0.1704)  loss_ce_unscaled: 0.1645 (0.1704)  loss_point_unscaled: 58.0832 (72.8904)\n",
      "[ep 1576][lr 0.0000100][3.24s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1753 (0.1811)  loss_ce: 0.1753 (0.1811)  loss_ce_unscaled: 0.1753 (0.1811)  loss_point_unscaled: 57.0331 (82.0767)\n",
      "[ep 1577][lr 0.0000100][2.50s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1678 (0.1676)  loss_ce: 0.1678 (0.1676)  loss_ce_unscaled: 0.1678 (0.1676)  loss_point_unscaled: 55.7897 (76.0719)\n",
      "[ep 1578][lr 0.0000100][2.44s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1720 (0.1739)  loss_ce: 0.1720 (0.1739)  loss_ce_unscaled: 0.1720 (0.1739)  loss_point_unscaled: 58.0954 (109.9087)\n",
      "[ep 1579][lr 0.0000100][3.25s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1751 (0.1807)  loss_ce: 0.1751 (0.1807)  loss_ce_unscaled: 0.1751 (0.1807)  loss_point_unscaled: 51.6662 (100.0417)\n",
      "[ep 1580][lr 0.0000100][3.34s]\n",
      "=======================================test=======================================\n",
      "mae: 149.25274725274724 mse: 229.09535420580076 time: 4.195943117141724 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1555 (0.1682)  loss_ce: 0.1555 (0.1682)  loss_ce_unscaled: 0.1555 (0.1682)  loss_point_unscaled: 50.8188 (80.9198)\n",
      "[ep 1581][lr 0.0000100][2.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1640 (0.1717)  loss_ce: 0.1640 (0.1717)  loss_ce_unscaled: 0.1640 (0.1717)  loss_point_unscaled: 66.1432 (113.4341)\n",
      "[ep 1582][lr 0.0000100][3.18s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1821 (0.1787)  loss_ce: 0.1821 (0.1787)  loss_ce_unscaled: 0.1821 (0.1787)  loss_point_unscaled: 49.8451 (72.4379)\n",
      "[ep 1583][lr 0.0000100][2.51s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1656 (0.1697)  loss_ce: 0.1656 (0.1697)  loss_ce_unscaled: 0.1656 (0.1697)  loss_point_unscaled: 54.2526 (78.5059)\n",
      "[ep 1584][lr 0.0000100][3.13s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1764 (0.1725)  loss_ce: 0.1764 (0.1725)  loss_ce_unscaled: 0.1764 (0.1725)  loss_point_unscaled: 51.8862 (85.8094)\n",
      "[ep 1585][lr 0.0000100][3.27s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1824 (0.1797)  loss_ce: 0.1824 (0.1797)  loss_ce_unscaled: 0.1824 (0.1797)  loss_point_unscaled: 54.6626 (85.9728)\n",
      "[ep 1586][lr 0.0000100][3.28s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1667 (0.1645)  loss_ce: 0.1667 (0.1645)  loss_ce_unscaled: 0.1667 (0.1645)  loss_point_unscaled: 55.9514 (80.9142)\n",
      "[ep 1587][lr 0.0000100][3.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1696 (0.1752)  loss_ce: 0.1696 (0.1752)  loss_ce_unscaled: 0.1696 (0.1752)  loss_point_unscaled: 54.6404 (76.6137)\n",
      "[ep 1588][lr 0.0000100][3.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1734 (0.1743)  loss_ce: 0.1734 (0.1743)  loss_ce_unscaled: 0.1734 (0.1743)  loss_point_unscaled: 54.4318 (68.4636)\n",
      "[ep 1589][lr 0.0000100][2.37s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1606 (0.1666)  loss_ce: 0.1606 (0.1666)  loss_ce_unscaled: 0.1606 (0.1666)  loss_point_unscaled: 52.7858 (59.3468)\n",
      "[ep 1590][lr 0.0000100][2.50s]\n",
      "=======================================test=======================================\n",
      "mae: 148.19230769230768 mse: 233.2432230503195 time: 2.244986057281494 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1698 (0.1740)  loss_ce: 0.1698 (0.1740)  loss_ce_unscaled: 0.1698 (0.1740)  loss_point_unscaled: 55.8126 (83.1819)\n",
      "[ep 1591][lr 0.0000100][3.24s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1668 (0.1699)  loss_ce: 0.1668 (0.1699)  loss_ce_unscaled: 0.1668 (0.1699)  loss_point_unscaled: 55.0339 (105.7363)\n",
      "[ep 1592][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1682 (0.1792)  loss_ce: 0.1682 (0.1792)  loss_ce_unscaled: 0.1682 (0.1792)  loss_point_unscaled: 55.6128 (84.7976)\n",
      "[ep 1593][lr 0.0000100][2.62s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1678 (0.1745)  loss_ce: 0.1678 (0.1745)  loss_ce_unscaled: 0.1678 (0.1745)  loss_point_unscaled: 53.5878 (82.0599)\n",
      "[ep 1594][lr 0.0000100][3.19s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1771 (0.1770)  loss_ce: 0.1771 (0.1770)  loss_ce_unscaled: 0.1771 (0.1770)  loss_point_unscaled: 49.3099 (74.4354)\n",
      "[ep 1595][lr 0.0000100][3.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1727 (0.1764)  loss_ce: 0.1727 (0.1764)  loss_ce_unscaled: 0.1727 (0.1764)  loss_point_unscaled: 51.2769 (69.2146)\n",
      "[ep 1596][lr 0.0000100][2.33s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1694 (0.1727)  loss_ce: 0.1694 (0.1727)  loss_ce_unscaled: 0.1694 (0.1727)  loss_point_unscaled: 60.9258 (81.1894)\n",
      "[ep 1597][lr 0.0000100][3.13s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1750 (0.1796)  loss_ce: 0.1750 (0.1796)  loss_ce_unscaled: 0.1750 (0.1796)  loss_point_unscaled: 52.6380 (87.1069)\n",
      "[ep 1598][lr 0.0000100][3.18s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1614 (0.1678)  loss_ce: 0.1614 (0.1678)  loss_ce_unscaled: 0.1614 (0.1678)  loss_point_unscaled: 52.0916 (119.7418)\n",
      "[ep 1599][lr 0.0000100][3.28s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1789 (0.1837)  loss_ce: 0.1789 (0.1837)  loss_ce_unscaled: 0.1789 (0.1837)  loss_point_unscaled: 54.3434 (88.0681)\n",
      "[ep 1600][lr 0.0000100][2.47s]\n",
      "=======================================test=======================================\n",
      "mae: 145.30769230769232 mse: 225.82289082813998 time: 2.26715087890625 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1653 (0.1766)  loss_ce: 0.1653 (0.1766)  loss_ce_unscaled: 0.1653 (0.1766)  loss_point_unscaled: 52.9602 (75.9905)\n",
      "[ep 1601][lr 0.0000100][2.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1731 (0.1773)  loss_ce: 0.1731 (0.1773)  loss_ce_unscaled: 0.1731 (0.1773)  loss_point_unscaled: 54.7916 (86.9708)\n",
      "[ep 1602][lr 0.0000100][2.52s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1705 (0.1777)  loss_ce: 0.1705 (0.1777)  loss_ce_unscaled: 0.1705 (0.1777)  loss_point_unscaled: 52.2624 (100.5611)\n",
      "[ep 1603][lr 0.0000100][3.10s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1742 (0.1717)  loss_ce: 0.1742 (0.1717)  loss_ce_unscaled: 0.1742 (0.1717)  loss_point_unscaled: 57.3528 (99.7872)\n",
      "[ep 1604][lr 0.0000100][2.43s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1745 (0.1756)  loss_ce: 0.1745 (0.1756)  loss_ce_unscaled: 0.1745 (0.1756)  loss_point_unscaled: 51.9106 (62.8561)\n",
      "[ep 1605][lr 0.0000100][3.11s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1795 (0.1746)  loss_ce: 0.1795 (0.1746)  loss_ce_unscaled: 0.1795 (0.1746)  loss_point_unscaled: 55.4698 (71.5757)\n",
      "[ep 1606][lr 0.0000100][2.38s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1702 (0.1769)  loss_ce: 0.1702 (0.1769)  loss_ce_unscaled: 0.1702 (0.1769)  loss_point_unscaled: 51.2365 (93.8750)\n",
      "[ep 1607][lr 0.0000100][3.48s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1727 (0.1748)  loss_ce: 0.1727 (0.1748)  loss_ce_unscaled: 0.1727 (0.1748)  loss_point_unscaled: 56.4553 (96.3065)\n",
      "[ep 1608][lr 0.0000100][3.25s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1780 (0.1733)  loss_ce: 0.1780 (0.1733)  loss_ce_unscaled: 0.1780 (0.1733)  loss_point_unscaled: 52.2658 (93.1080)\n",
      "[ep 1609][lr 0.0000100][3.08s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1683 (0.1741)  loss_ce: 0.1683 (0.1741)  loss_ce_unscaled: 0.1683 (0.1741)  loss_point_unscaled: 52.8695 (67.9639)\n",
      "[ep 1610][lr 0.0000100][2.36s]\n",
      "=======================================test=======================================\n",
      "mae: 151.0164835164835 mse: 238.22084360876218 time: 4.189124822616577 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1673 (0.1718)  loss_ce: 0.1673 (0.1718)  loss_ce_unscaled: 0.1673 (0.1718)  loss_point_unscaled: 69.1675 (98.0453)\n",
      "[ep 1611][lr 0.0000100][3.11s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1763 (0.1794)  loss_ce: 0.1763 (0.1794)  loss_ce_unscaled: 0.1763 (0.1794)  loss_point_unscaled: 51.3676 (79.8838)\n",
      "[ep 1612][lr 0.0000100][3.15s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1844 (0.1845)  loss_ce: 0.1844 (0.1845)  loss_ce_unscaled: 0.1844 (0.1845)  loss_point_unscaled: 62.1477 (96.0591)\n",
      "[ep 1613][lr 0.0000100][2.50s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1767 (0.1823)  loss_ce: 0.1767 (0.1823)  loss_ce_unscaled: 0.1767 (0.1823)  loss_point_unscaled: 51.2331 (100.7598)\n",
      "[ep 1614][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1731 (0.1747)  loss_ce: 0.1731 (0.1747)  loss_ce_unscaled: 0.1731 (0.1747)  loss_point_unscaled: 52.5392 (70.6151)\n",
      "[ep 1615][lr 0.0000100][2.31s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1771 (0.1780)  loss_ce: 0.1771 (0.1780)  loss_ce_unscaled: 0.1771 (0.1780)  loss_point_unscaled: 54.3239 (109.4115)\n",
      "[ep 1616][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1719 (0.1726)  loss_ce: 0.1719 (0.1726)  loss_ce_unscaled: 0.1719 (0.1726)  loss_point_unscaled: 55.6500 (95.6790)\n",
      "[ep 1617][lr 0.0000100][3.02s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1661 (0.1780)  loss_ce: 0.1661 (0.1780)  loss_ce_unscaled: 0.1661 (0.1780)  loss_point_unscaled: 50.8929 (70.9433)\n",
      "[ep 1618][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1777 (0.1768)  loss_ce: 0.1777 (0.1768)  loss_ce_unscaled: 0.1777 (0.1768)  loss_point_unscaled: 52.4933 (88.9715)\n",
      "[ep 1619][lr 0.0000100][3.26s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1654 (0.1725)  loss_ce: 0.1654 (0.1725)  loss_ce_unscaled: 0.1654 (0.1725)  loss_point_unscaled: 55.0632 (70.8471)\n",
      "[ep 1620][lr 0.0000100][2.37s]\n",
      "=======================================test=======================================\n",
      "mae: 148.57692307692307 mse: 234.49045764225616 time: 4.21378231048584 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1711 (0.1723)  loss_ce: 0.1711 (0.1723)  loss_ce_unscaled: 0.1711 (0.1723)  loss_point_unscaled: 54.4950 (94.7941)\n",
      "[ep 1621][lr 0.0000100][2.36s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1730 (0.1719)  loss_ce: 0.1730 (0.1719)  loss_ce_unscaled: 0.1730 (0.1719)  loss_point_unscaled: 59.5988 (88.3505)\n",
      "[ep 1622][lr 0.0000100][3.24s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1641 (0.1715)  loss_ce: 0.1641 (0.1715)  loss_ce_unscaled: 0.1641 (0.1715)  loss_point_unscaled: 53.0605 (66.8072)\n",
      "[ep 1623][lr 0.0000100][2.53s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1668 (0.1694)  loss_ce: 0.1668 (0.1694)  loss_ce_unscaled: 0.1668 (0.1694)  loss_point_unscaled: 52.1854 (72.2634)\n",
      "[ep 1624][lr 0.0000100][3.03s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1636 (0.1696)  loss_ce: 0.1636 (0.1696)  loss_ce_unscaled: 0.1636 (0.1696)  loss_point_unscaled: 50.0404 (54.8394)\n",
      "[ep 1625][lr 0.0000100][2.77s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1697 (0.1728)  loss_ce: 0.1697 (0.1728)  loss_ce_unscaled: 0.1697 (0.1728)  loss_point_unscaled: 54.5000 (119.7266)\n",
      "[ep 1626][lr 0.0000100][3.23s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1719 (0.1732)  loss_ce: 0.1719 (0.1732)  loss_ce_unscaled: 0.1719 (0.1732)  loss_point_unscaled: 63.1478 (121.2552)\n",
      "[ep 1627][lr 0.0000100][3.60s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1600 (0.1699)  loss_ce: 0.1600 (0.1699)  loss_ce_unscaled: 0.1600 (0.1699)  loss_point_unscaled: 58.6677 (79.8592)\n",
      "[ep 1628][lr 0.0000100][3.12s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1631 (0.1693)  loss_ce: 0.1631 (0.1693)  loss_ce_unscaled: 0.1631 (0.1693)  loss_point_unscaled: 53.5740 (93.7739)\n",
      "[ep 1629][lr 0.0000100][3.13s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1905 (0.1821)  loss_ce: 0.1905 (0.1821)  loss_ce_unscaled: 0.1905 (0.1821)  loss_point_unscaled: 55.5963 (81.4455)\n",
      "[ep 1630][lr 0.0000100][3.14s]\n",
      "=======================================test=======================================\n",
      "mae: 150.67032967032966 mse: 237.84229237870215 time: 4.198425531387329 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1730 (0.1758)  loss_ce: 0.1730 (0.1758)  loss_ce_unscaled: 0.1730 (0.1758)  loss_point_unscaled: 52.6825 (78.5237)\n",
      "[ep 1631][lr 0.0000100][2.66s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1801 (0.1811)  loss_ce: 0.1801 (0.1811)  loss_ce_unscaled: 0.1801 (0.1811)  loss_point_unscaled: 51.6405 (67.1077)\n",
      "[ep 1632][lr 0.0000100][3.21s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1739 (0.1755)  loss_ce: 0.1739 (0.1755)  loss_ce_unscaled: 0.1739 (0.1755)  loss_point_unscaled: 53.8649 (73.5529)\n",
      "[ep 1633][lr 0.0000100][2.38s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1794 (0.1776)  loss_ce: 0.1794 (0.1776)  loss_ce_unscaled: 0.1794 (0.1776)  loss_point_unscaled: 52.9737 (82.8745)\n",
      "[ep 1634][lr 0.0000100][3.36s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1667 (0.1738)  loss_ce: 0.1667 (0.1738)  loss_ce_unscaled: 0.1667 (0.1738)  loss_point_unscaled: 53.1910 (60.1990)\n",
      "[ep 1635][lr 0.0000100][3.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1755 (0.1813)  loss_ce: 0.1755 (0.1813)  loss_ce_unscaled: 0.1755 (0.1813)  loss_point_unscaled: 52.6345 (67.8376)\n",
      "[ep 1636][lr 0.0000100][3.21s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1698 (0.1690)  loss_ce: 0.1698 (0.1690)  loss_ce_unscaled: 0.1698 (0.1690)  loss_point_unscaled: 60.0946 (83.8520)\n",
      "[ep 1637][lr 0.0000100][3.07s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1683 (0.1714)  loss_ce: 0.1683 (0.1714)  loss_ce_unscaled: 0.1683 (0.1714)  loss_point_unscaled: 55.6526 (93.0650)\n",
      "[ep 1638][lr 0.0000100][2.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1711 (0.1747)  loss_ce: 0.1711 (0.1747)  loss_ce_unscaled: 0.1711 (0.1747)  loss_point_unscaled: 59.0484 (76.4889)\n",
      "[ep 1639][lr 0.0000100][3.06s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1735 (0.1772)  loss_ce: 0.1735 (0.1772)  loss_ce_unscaled: 0.1735 (0.1772)  loss_point_unscaled: 52.9208 (141.5851)\n",
      "[ep 1640][lr 0.0000100][3.29s]\n",
      "=======================================test=======================================\n",
      "mae: 148.0 mse: 232.4350316885214 time: 2.696498155593872 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1644 (0.1652)  loss_ce: 0.1644 (0.1652)  loss_ce_unscaled: 0.1644 (0.1652)  loss_point_unscaled: 52.3494 (79.8948)\n",
      "[ep 1641][lr 0.0000100][3.15s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1761 (0.1798)  loss_ce: 0.1761 (0.1798)  loss_ce_unscaled: 0.1761 (0.1798)  loss_point_unscaled: 53.6769 (71.9223)\n",
      "[ep 1642][lr 0.0000100][2.38s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1651 (0.1763)  loss_ce: 0.1651 (0.1763)  loss_ce_unscaled: 0.1651 (0.1763)  loss_point_unscaled: 52.2044 (60.5221)\n",
      "[ep 1643][lr 0.0000100][2.47s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1690 (0.1681)  loss_ce: 0.1690 (0.1681)  loss_ce_unscaled: 0.1690 (0.1681)  loss_point_unscaled: 53.6043 (72.8890)\n",
      "[ep 1644][lr 0.0000100][2.64s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1784 (0.1744)  loss_ce: 0.1784 (0.1744)  loss_ce_unscaled: 0.1784 (0.1744)  loss_point_unscaled: 54.3055 (80.2630)\n",
      "[ep 1645][lr 0.0000100][3.25s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1756 (0.1828)  loss_ce: 0.1756 (0.1828)  loss_ce_unscaled: 0.1756 (0.1828)  loss_point_unscaled: 51.8994 (75.8404)\n",
      "[ep 1646][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1664 (0.1673)  loss_ce: 0.1664 (0.1673)  loss_ce_unscaled: 0.1664 (0.1673)  loss_point_unscaled: 52.8582 (114.9183)\n",
      "[ep 1647][lr 0.0000100][3.27s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1687 (0.1728)  loss_ce: 0.1687 (0.1728)  loss_ce_unscaled: 0.1687 (0.1728)  loss_point_unscaled: 52.9586 (71.8302)\n",
      "[ep 1648][lr 0.0000100][2.50s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1712 (0.1756)  loss_ce: 0.1712 (0.1756)  loss_ce_unscaled: 0.1712 (0.1756)  loss_point_unscaled: 58.7876 (96.5730)\n",
      "[ep 1649][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1745 (0.1733)  loss_ce: 0.1745 (0.1733)  loss_ce_unscaled: 0.1745 (0.1733)  loss_point_unscaled: 51.3141 (79.9351)\n",
      "[ep 1650][lr 0.0000100][3.10s]\n",
      "=======================================test=======================================\n",
      "mae: 143.83516483516485 mse: 224.1058423411326 time: 2.303792715072632 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1669 (0.1716)  loss_ce: 0.1669 (0.1716)  loss_ce_unscaled: 0.1669 (0.1716)  loss_point_unscaled: 54.1443 (110.5404)\n",
      "[ep 1651][lr 0.0000100][2.80s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1616 (0.1676)  loss_ce: 0.1616 (0.1676)  loss_ce_unscaled: 0.1616 (0.1676)  loss_point_unscaled: 77.8235 (106.9714)\n",
      "[ep 1652][lr 0.0000100][3.17s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1619 (0.1713)  loss_ce: 0.1619 (0.1713)  loss_ce_unscaled: 0.1619 (0.1713)  loss_point_unscaled: 55.3178 (78.6259)\n",
      "[ep 1653][lr 0.0000100][2.41s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1692 (0.1761)  loss_ce: 0.1692 (0.1761)  loss_ce_unscaled: 0.1692 (0.1761)  loss_point_unscaled: 51.5671 (66.1960)\n",
      "[ep 1654][lr 0.0000100][3.02s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1687 (0.1738)  loss_ce: 0.1687 (0.1738)  loss_ce_unscaled: 0.1687 (0.1738)  loss_point_unscaled: 50.9057 (151.9917)\n",
      "[ep 1655][lr 0.0000100][2.52s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1750 (0.1810)  loss_ce: 0.1750 (0.1810)  loss_ce_unscaled: 0.1750 (0.1810)  loss_point_unscaled: 51.3596 (72.1280)\n",
      "[ep 1656][lr 0.0000100][3.43s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1693 (0.1746)  loss_ce: 0.1693 (0.1746)  loss_ce_unscaled: 0.1693 (0.1746)  loss_point_unscaled: 53.1886 (83.2528)\n",
      "[ep 1657][lr 0.0000100][3.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1670 (0.1745)  loss_ce: 0.1670 (0.1745)  loss_ce_unscaled: 0.1670 (0.1745)  loss_point_unscaled: 54.5229 (83.8945)\n",
      "[ep 1658][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1790 (0.1801)  loss_ce: 0.1790 (0.1801)  loss_ce_unscaled: 0.1790 (0.1801)  loss_point_unscaled: 56.5576 (85.6288)\n",
      "[ep 1659][lr 0.0000100][3.12s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1680 (0.1702)  loss_ce: 0.1680 (0.1702)  loss_ce_unscaled: 0.1680 (0.1702)  loss_point_unscaled: 50.5644 (78.5811)\n",
      "[ep 1660][lr 0.0000100][2.63s]\n",
      "=======================================test=======================================\n",
      "mae: 149.46703296703296 mse: 231.13381548215887 time: 4.253861904144287 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1696 (0.1756)  loss_ce: 0.1696 (0.1756)  loss_ce_unscaled: 0.1696 (0.1756)  loss_point_unscaled: 55.4250 (93.9943)\n",
      "[ep 1661][lr 0.0000100][3.12s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1673 (0.1742)  loss_ce: 0.1673 (0.1742)  loss_ce_unscaled: 0.1673 (0.1742)  loss_point_unscaled: 54.9048 (66.5734)\n",
      "[ep 1662][lr 0.0000100][2.96s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1679 (0.1727)  loss_ce: 0.1679 (0.1727)  loss_ce_unscaled: 0.1679 (0.1727)  loss_point_unscaled: 52.0341 (77.4057)\n",
      "[ep 1663][lr 0.0000100][2.36s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1716 (0.1744)  loss_ce: 0.1716 (0.1744)  loss_ce_unscaled: 0.1716 (0.1744)  loss_point_unscaled: 54.5968 (88.9881)\n",
      "[ep 1664][lr 0.0000100][3.26s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1772 (0.1753)  loss_ce: 0.1772 (0.1753)  loss_ce_unscaled: 0.1772 (0.1753)  loss_point_unscaled: 53.1477 (63.8747)\n",
      "[ep 1665][lr 0.0000100][3.36s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1770 (0.1751)  loss_ce: 0.1770 (0.1751)  loss_ce_unscaled: 0.1770 (0.1751)  loss_point_unscaled: 51.4664 (114.9307)\n",
      "[ep 1666][lr 0.0000100][3.10s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1702 (0.1748)  loss_ce: 0.1702 (0.1748)  loss_ce_unscaled: 0.1702 (0.1748)  loss_point_unscaled: 51.0721 (65.4645)\n",
      "[ep 1667][lr 0.0000100][2.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1715 (0.1738)  loss_ce: 0.1715 (0.1738)  loss_ce_unscaled: 0.1715 (0.1738)  loss_point_unscaled: 52.5961 (78.9953)\n",
      "[ep 1668][lr 0.0000100][3.13s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1740 (0.1805)  loss_ce: 0.1740 (0.1805)  loss_ce_unscaled: 0.1740 (0.1805)  loss_point_unscaled: 61.1024 (110.2404)\n",
      "[ep 1669][lr 0.0000100][3.27s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1782 (0.1808)  loss_ce: 0.1782 (0.1808)  loss_ce_unscaled: 0.1782 (0.1808)  loss_point_unscaled: 52.7546 (77.0913)\n",
      "[ep 1670][lr 0.0000100][2.46s]\n",
      "=======================================test=======================================\n",
      "mae: 145.23626373626374 mse: 227.19576928811324 time: 4.261272192001343 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1675 (0.1658)  loss_ce: 0.1675 (0.1658)  loss_ce_unscaled: 0.1675 (0.1658)  loss_point_unscaled: 54.3991 (94.8983)\n",
      "[ep 1671][lr 0.0000100][3.01s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1784 (0.1776)  loss_ce: 0.1784 (0.1776)  loss_ce_unscaled: 0.1784 (0.1776)  loss_point_unscaled: 56.7785 (84.9212)\n",
      "[ep 1672][lr 0.0000100][3.02s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1725 (0.1757)  loss_ce: 0.1725 (0.1757)  loss_ce_unscaled: 0.1725 (0.1757)  loss_point_unscaled: 61.1004 (94.8882)\n",
      "[ep 1673][lr 0.0000100][3.35s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1639 (0.1742)  loss_ce: 0.1639 (0.1742)  loss_ce_unscaled: 0.1639 (0.1742)  loss_point_unscaled: 56.1273 (86.3749)\n",
      "[ep 1674][lr 0.0000100][2.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1806 (0.1766)  loss_ce: 0.1806 (0.1766)  loss_ce_unscaled: 0.1806 (0.1766)  loss_point_unscaled: 52.1877 (67.4145)\n",
      "[ep 1675][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1646 (0.1736)  loss_ce: 0.1646 (0.1736)  loss_ce_unscaled: 0.1646 (0.1736)  loss_point_unscaled: 50.7959 (55.7488)\n",
      "[ep 1676][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1718 (0.1729)  loss_ce: 0.1718 (0.1729)  loss_ce_unscaled: 0.1718 (0.1729)  loss_point_unscaled: 52.7571 (80.0501)\n",
      "[ep 1677][lr 0.0000100][2.47s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1659 (0.1724)  loss_ce: 0.1659 (0.1724)  loss_ce_unscaled: 0.1659 (0.1724)  loss_point_unscaled: 54.3191 (88.2630)\n",
      "[ep 1678][lr 0.0000100][3.37s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1710 (0.1764)  loss_ce: 0.1710 (0.1764)  loss_ce_unscaled: 0.1710 (0.1764)  loss_point_unscaled: 51.0896 (80.5414)\n",
      "[ep 1679][lr 0.0000100][3.07s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1781 (0.1760)  loss_ce: 0.1781 (0.1760)  loss_ce_unscaled: 0.1781 (0.1760)  loss_point_unscaled: 53.7032 (81.6173)\n",
      "[ep 1680][lr 0.0000100][3.22s]\n",
      "=======================================test=======================================\n",
      "mae: 147.5934065934066 mse: 231.72148401442226 time: 2.248138427734375 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1674 (0.1706)  loss_ce: 0.1674 (0.1706)  loss_ce_unscaled: 0.1674 (0.1706)  loss_point_unscaled: 49.8887 (78.2475)\n",
      "[ep 1681][lr 0.0000100][3.15s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1751 (0.1771)  loss_ce: 0.1751 (0.1771)  loss_ce_unscaled: 0.1751 (0.1771)  loss_point_unscaled: 59.0048 (79.6789)\n",
      "[ep 1682][lr 0.0000100][3.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1618 (0.1714)  loss_ce: 0.1618 (0.1714)  loss_ce_unscaled: 0.1618 (0.1714)  loss_point_unscaled: 52.5204 (79.3787)\n",
      "[ep 1683][lr 0.0000100][3.07s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1665 (0.1723)  loss_ce: 0.1665 (0.1723)  loss_ce_unscaled: 0.1665 (0.1723)  loss_point_unscaled: 52.9533 (84.6861)\n",
      "[ep 1684][lr 0.0000100][3.03s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1668 (0.1724)  loss_ce: 0.1668 (0.1724)  loss_ce_unscaled: 0.1668 (0.1724)  loss_point_unscaled: 53.6710 (59.9039)\n",
      "[ep 1685][lr 0.0000100][3.12s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1722 (0.1785)  loss_ce: 0.1722 (0.1785)  loss_ce_unscaled: 0.1722 (0.1785)  loss_point_unscaled: 53.8604 (97.4127)\n",
      "[ep 1686][lr 0.0000100][3.42s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1722 (0.1748)  loss_ce: 0.1722 (0.1748)  loss_ce_unscaled: 0.1722 (0.1748)  loss_point_unscaled: 56.9464 (103.9999)\n",
      "[ep 1687][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1727 (0.1718)  loss_ce: 0.1727 (0.1718)  loss_ce_unscaled: 0.1727 (0.1718)  loss_point_unscaled: 52.8108 (99.5547)\n",
      "[ep 1688][lr 0.0000100][3.07s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1770 (0.1816)  loss_ce: 0.1770 (0.1816)  loss_ce_unscaled: 0.1770 (0.1816)  loss_point_unscaled: 51.8773 (70.4868)\n",
      "[ep 1689][lr 0.0000100][2.45s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1738 (0.1659)  loss_ce: 0.1738 (0.1659)  loss_ce_unscaled: 0.1738 (0.1659)  loss_point_unscaled: 51.3235 (92.0845)\n",
      "[ep 1690][lr 0.0000100][3.32s]\n",
      "=======================================test=======================================\n",
      "mae: 146.33516483516485 mse: 228.7911975752391 time: 4.170531749725342 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1680 (0.1767)  loss_ce: 0.1680 (0.1767)  loss_ce_unscaled: 0.1680 (0.1767)  loss_point_unscaled: 51.5000 (53.6817)\n",
      "[ep 1691][lr 0.0000100][2.87s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1644 (0.1736)  loss_ce: 0.1644 (0.1736)  loss_ce_unscaled: 0.1644 (0.1736)  loss_point_unscaled: 51.8327 (67.5686)\n",
      "[ep 1692][lr 0.0000100][3.12s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1710 (0.1713)  loss_ce: 0.1710 (0.1713)  loss_ce_unscaled: 0.1710 (0.1713)  loss_point_unscaled: 55.3139 (83.2932)\n",
      "[ep 1693][lr 0.0000100][3.26s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1770 (0.1741)  loss_ce: 0.1770 (0.1741)  loss_ce_unscaled: 0.1770 (0.1741)  loss_point_unscaled: 53.7018 (91.6415)\n",
      "[ep 1694][lr 0.0000100][3.17s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1687 (0.1767)  loss_ce: 0.1687 (0.1767)  loss_ce_unscaled: 0.1687 (0.1767)  loss_point_unscaled: 51.7069 (79.4882)\n",
      "[ep 1695][lr 0.0000100][3.08s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1720 (0.1719)  loss_ce: 0.1720 (0.1719)  loss_ce_unscaled: 0.1720 (0.1719)  loss_point_unscaled: 53.4989 (80.5714)\n",
      "[ep 1696][lr 0.0000100][3.07s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1709 (0.1724)  loss_ce: 0.1709 (0.1724)  loss_ce_unscaled: 0.1709 (0.1724)  loss_point_unscaled: 51.5146 (75.2879)\n",
      "[ep 1697][lr 0.0000100][3.32s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1693 (0.1668)  loss_ce: 0.1693 (0.1668)  loss_ce_unscaled: 0.1693 (0.1668)  loss_point_unscaled: 61.0149 (81.9159)\n",
      "[ep 1698][lr 0.0000100][3.12s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1707 (0.1710)  loss_ce: 0.1707 (0.1710)  loss_ce_unscaled: 0.1707 (0.1710)  loss_point_unscaled: 50.0175 (61.8308)\n",
      "[ep 1699][lr 0.0000100][3.02s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1677 (0.1740)  loss_ce: 0.1677 (0.1740)  loss_ce_unscaled: 0.1677 (0.1740)  loss_point_unscaled: 53.4512 (94.2156)\n",
      "[ep 1700][lr 0.0000100][3.34s]\n",
      "=======================================test=======================================\n",
      "mae: 148.7032967032967 mse: 231.33470827514674 time: 2.301436424255371 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1676 (0.1719)  loss_ce: 0.1676 (0.1719)  loss_ce_unscaled: 0.1676 (0.1719)  loss_point_unscaled: 53.1677 (70.1375)\n",
      "[ep 1701][lr 0.0000100][2.78s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1738 (0.1768)  loss_ce: 0.1738 (0.1768)  loss_ce_unscaled: 0.1738 (0.1768)  loss_point_unscaled: 57.2216 (83.3117)\n",
      "[ep 1702][lr 0.0000100][3.48s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1693 (0.1754)  loss_ce: 0.1693 (0.1754)  loss_ce_unscaled: 0.1693 (0.1754)  loss_point_unscaled: 53.3896 (98.3026)\n",
      "[ep 1703][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1739 (0.1723)  loss_ce: 0.1739 (0.1723)  loss_ce_unscaled: 0.1739 (0.1723)  loss_point_unscaled: 51.5559 (66.9098)\n",
      "[ep 1704][lr 0.0000100][2.44s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1613 (0.1663)  loss_ce: 0.1613 (0.1663)  loss_ce_unscaled: 0.1613 (0.1663)  loss_point_unscaled: 55.7807 (66.5511)\n",
      "[ep 1705][lr 0.0000100][2.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1651 (0.1744)  loss_ce: 0.1651 (0.1744)  loss_ce_unscaled: 0.1651 (0.1744)  loss_point_unscaled: 50.7182 (95.5139)\n",
      "[ep 1706][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1793 (0.1780)  loss_ce: 0.1793 (0.1780)  loss_ce_unscaled: 0.1793 (0.1780)  loss_point_unscaled: 56.3355 (77.9441)\n",
      "[ep 1707][lr 0.0000100][3.18s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1705 (0.1715)  loss_ce: 0.1705 (0.1715)  loss_ce_unscaled: 0.1705 (0.1715)  loss_point_unscaled: 53.4518 (78.9125)\n",
      "[ep 1708][lr 0.0000100][3.21s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1672 (0.1731)  loss_ce: 0.1672 (0.1731)  loss_ce_unscaled: 0.1672 (0.1731)  loss_point_unscaled: 52.0239 (63.8064)\n",
      "[ep 1709][lr 0.0000100][2.37s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1827 (0.1801)  loss_ce: 0.1827 (0.1801)  loss_ce_unscaled: 0.1827 (0.1801)  loss_point_unscaled: 73.7080 (149.0012)\n",
      "[ep 1710][lr 0.0000100][3.43s]\n",
      "=======================================test=======================================\n",
      "mae: 152.21978021978023 mse: 243.1752635265536 time: 4.206652402877808 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1663 (0.1687)  loss_ce: 0.1663 (0.1687)  loss_ce_unscaled: 0.1663 (0.1687)  loss_point_unscaled: 58.9982 (85.6994)\n",
      "[ep 1711][lr 0.0000100][3.08s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1694 (0.1710)  loss_ce: 0.1694 (0.1710)  loss_ce_unscaled: 0.1694 (0.1710)  loss_point_unscaled: 53.2696 (83.5597)\n",
      "[ep 1712][lr 0.0000100][3.36s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1786 (0.1794)  loss_ce: 0.1786 (0.1794)  loss_ce_unscaled: 0.1786 (0.1794)  loss_point_unscaled: 55.1434 (89.6554)\n",
      "[ep 1713][lr 0.0000100][2.54s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1782 (0.1833)  loss_ce: 0.1782 (0.1833)  loss_ce_unscaled: 0.1782 (0.1833)  loss_point_unscaled: 54.3125 (79.4946)\n",
      "[ep 1714][lr 0.0000100][3.36s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1712 (0.1741)  loss_ce: 0.1712 (0.1741)  loss_ce_unscaled: 0.1712 (0.1741)  loss_point_unscaled: 55.4571 (86.0542)\n",
      "[ep 1715][lr 0.0000100][3.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1695 (0.1745)  loss_ce: 0.1695 (0.1745)  loss_ce_unscaled: 0.1695 (0.1745)  loss_point_unscaled: 53.6440 (74.8142)\n",
      "[ep 1716][lr 0.0000100][3.24s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1646 (0.1747)  loss_ce: 0.1646 (0.1747)  loss_ce_unscaled: 0.1646 (0.1747)  loss_point_unscaled: 53.1910 (83.7670)\n",
      "[ep 1717][lr 0.0000100][2.42s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1758 (0.1774)  loss_ce: 0.1758 (0.1774)  loss_ce_unscaled: 0.1758 (0.1774)  loss_point_unscaled: 55.1243 (94.9947)\n",
      "[ep 1718][lr 0.0000100][2.83s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1716 (0.1732)  loss_ce: 0.1716 (0.1732)  loss_ce_unscaled: 0.1716 (0.1732)  loss_point_unscaled: 54.4274 (92.3126)\n",
      "[ep 1719][lr 0.0000100][2.43s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1640 (0.1716)  loss_ce: 0.1640 (0.1716)  loss_ce_unscaled: 0.1640 (0.1716)  loss_point_unscaled: 53.6275 (85.6817)\n",
      "[ep 1720][lr 0.0000100][2.42s]\n",
      "=======================================test=======================================\n",
      "mae: 150.4120879120879 mse: 235.8316474395791 time: 4.189297199249268 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1550 (0.1648)  loss_ce: 0.1550 (0.1648)  loss_ce_unscaled: 0.1550 (0.1648)  loss_point_unscaled: 52.7561 (86.9160)\n",
      "[ep 1721][lr 0.0000100][3.35s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1705 (0.1775)  loss_ce: 0.1705 (0.1775)  loss_ce_unscaled: 0.1705 (0.1775)  loss_point_unscaled: 51.1995 (96.8142)\n",
      "[ep 1722][lr 0.0000100][3.28s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1719 (0.1711)  loss_ce: 0.1719 (0.1711)  loss_ce_unscaled: 0.1719 (0.1711)  loss_point_unscaled: 51.1537 (60.9125)\n",
      "[ep 1723][lr 0.0000100][2.46s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1711 (0.1717)  loss_ce: 0.1711 (0.1717)  loss_ce_unscaled: 0.1711 (0.1717)  loss_point_unscaled: 52.9823 (70.2437)\n",
      "[ep 1724][lr 0.0000100][3.23s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1634 (0.1690)  loss_ce: 0.1634 (0.1690)  loss_ce_unscaled: 0.1634 (0.1690)  loss_point_unscaled: 53.0433 (73.7900)\n",
      "[ep 1725][lr 0.0000100][2.88s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1735 (0.1752)  loss_ce: 0.1735 (0.1752)  loss_ce_unscaled: 0.1735 (0.1752)  loss_point_unscaled: 52.6740 (97.9201)\n",
      "[ep 1726][lr 0.0000100][3.37s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1639 (0.1766)  loss_ce: 0.1639 (0.1766)  loss_ce_unscaled: 0.1639 (0.1766)  loss_point_unscaled: 54.3648 (74.7392)\n",
      "[ep 1727][lr 0.0000100][3.31s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1717 (0.1708)  loss_ce: 0.1717 (0.1708)  loss_ce_unscaled: 0.1717 (0.1708)  loss_point_unscaled: 53.6279 (83.7004)\n",
      "[ep 1728][lr 0.0000100][3.05s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1629 (0.1698)  loss_ce: 0.1629 (0.1698)  loss_ce_unscaled: 0.1629 (0.1698)  loss_point_unscaled: 54.3562 (59.9196)\n",
      "[ep 1729][lr 0.0000100][3.05s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1635 (0.1732)  loss_ce: 0.1635 (0.1732)  loss_ce_unscaled: 0.1635 (0.1732)  loss_point_unscaled: 55.2815 (115.1328)\n",
      "[ep 1730][lr 0.0000100][2.50s]\n",
      "=======================================test=======================================\n",
      "mae: 151.85714285714286 mse: 238.22696722262137 time: 4.203905820846558 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1738 (0.1748)  loss_ce: 0.1738 (0.1748)  loss_ce_unscaled: 0.1738 (0.1748)  loss_point_unscaled: 53.9572 (87.5027)\n",
      "[ep 1731][lr 0.0000100][3.30s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1708 (0.1747)  loss_ce: 0.1708 (0.1747)  loss_ce_unscaled: 0.1708 (0.1747)  loss_point_unscaled: 56.8617 (83.8505)\n",
      "[ep 1732][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1632 (0.1780)  loss_ce: 0.1632 (0.1780)  loss_ce_unscaled: 0.1632 (0.1780)  loss_point_unscaled: 52.2698 (73.8615)\n",
      "[ep 1733][lr 0.0000100][2.46s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1797 (0.1765)  loss_ce: 0.1797 (0.1765)  loss_ce_unscaled: 0.1797 (0.1765)  loss_point_unscaled: 53.0582 (60.0036)\n",
      "[ep 1734][lr 0.0000100][3.08s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1682 (0.1701)  loss_ce: 0.1682 (0.1701)  loss_ce_unscaled: 0.1682 (0.1701)  loss_point_unscaled: 59.8264 (101.6302)\n",
      "[ep 1735][lr 0.0000100][3.07s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1725 (0.1735)  loss_ce: 0.1725 (0.1735)  loss_ce_unscaled: 0.1725 (0.1735)  loss_point_unscaled: 54.6974 (95.6009)\n",
      "[ep 1736][lr 0.0000100][2.49s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1749 (0.1711)  loss_ce: 0.1749 (0.1711)  loss_ce_unscaled: 0.1749 (0.1711)  loss_point_unscaled: 53.4412 (78.4882)\n",
      "[ep 1737][lr 0.0000100][2.35s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1674 (0.1745)  loss_ce: 0.1674 (0.1745)  loss_ce_unscaled: 0.1674 (0.1745)  loss_point_unscaled: 52.3060 (69.3982)\n",
      "[ep 1738][lr 0.0000100][3.03s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1639 (0.1697)  loss_ce: 0.1639 (0.1697)  loss_ce_unscaled: 0.1639 (0.1697)  loss_point_unscaled: 53.2337 (69.3023)\n",
      "[ep 1739][lr 0.0000100][3.05s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1658 (0.1647)  loss_ce: 0.1658 (0.1647)  loss_ce_unscaled: 0.1658 (0.1647)  loss_point_unscaled: 53.5935 (66.0397)\n",
      "[ep 1740][lr 0.0000100][3.10s]\n",
      "=======================================test=======================================\n",
      "mae: 147.43406593406593 mse: 235.4840583951316 time: 3.2264137268066406 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1746 (0.1787)  loss_ce: 0.1746 (0.1787)  loss_ce_unscaled: 0.1746 (0.1787)  loss_point_unscaled: 51.8391 (62.5786)\n",
      "[ep 1741][lr 0.0000100][3.18s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1735 (0.1747)  loss_ce: 0.1735 (0.1747)  loss_ce_unscaled: 0.1735 (0.1747)  loss_point_unscaled: 51.5306 (130.6219)\n",
      "[ep 1742][lr 0.0000100][2.42s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1632 (0.1665)  loss_ce: 0.1632 (0.1665)  loss_ce_unscaled: 0.1632 (0.1665)  loss_point_unscaled: 50.4965 (81.3447)\n",
      "[ep 1743][lr 0.0000100][3.25s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1744 (0.1766)  loss_ce: 0.1744 (0.1766)  loss_ce_unscaled: 0.1744 (0.1766)  loss_point_unscaled: 56.0834 (129.1101)\n",
      "[ep 1744][lr 0.0000100][3.23s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1628 (0.1709)  loss_ce: 0.1628 (0.1709)  loss_ce_unscaled: 0.1628 (0.1709)  loss_point_unscaled: 51.9486 (97.2125)\n",
      "[ep 1745][lr 0.0000100][2.44s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1634 (0.1714)  loss_ce: 0.1634 (0.1714)  loss_ce_unscaled: 0.1634 (0.1714)  loss_point_unscaled: 53.5052 (66.9858)\n",
      "[ep 1746][lr 0.0000100][3.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1653 (0.1697)  loss_ce: 0.1653 (0.1697)  loss_ce_unscaled: 0.1653 (0.1697)  loss_point_unscaled: 54.8988 (61.7647)\n",
      "[ep 1747][lr 0.0000100][3.16s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1634 (0.1672)  loss_ce: 0.1634 (0.1672)  loss_ce_unscaled: 0.1634 (0.1672)  loss_point_unscaled: 53.4178 (67.7737)\n",
      "[ep 1748][lr 0.0000100][3.33s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1752 (0.1751)  loss_ce: 0.1752 (0.1751)  loss_ce_unscaled: 0.1752 (0.1751)  loss_point_unscaled: 56.1316 (66.2961)\n",
      "[ep 1749][lr 0.0000100][2.85s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1664 (0.1746)  loss_ce: 0.1664 (0.1746)  loss_ce_unscaled: 0.1664 (0.1746)  loss_point_unscaled: 50.3247 (99.8848)\n",
      "[ep 1750][lr 0.0000100][3.37s]\n",
      "=======================================test=======================================\n",
      "mae: 150.6978021978022 mse: 236.953639922512 time: 2.253389835357666 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1726 (0.1737)  loss_ce: 0.1726 (0.1737)  loss_ce_unscaled: 0.1726 (0.1737)  loss_point_unscaled: 51.7553 (79.5968)\n",
      "[ep 1751][lr 0.0000100][2.77s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1647 (0.1697)  loss_ce: 0.1647 (0.1697)  loss_ce_unscaled: 0.1647 (0.1697)  loss_point_unscaled: 54.9487 (58.2310)\n",
      "[ep 1752][lr 0.0000100][3.05s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1821 (0.1774)  loss_ce: 0.1821 (0.1774)  loss_ce_unscaled: 0.1821 (0.1774)  loss_point_unscaled: 50.0853 (76.8351)\n",
      "[ep 1753][lr 0.0000100][3.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1642 (0.1672)  loss_ce: 0.1642 (0.1672)  loss_ce_unscaled: 0.1642 (0.1672)  loss_point_unscaled: 57.2558 (91.5027)\n",
      "[ep 1754][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1744 (0.1772)  loss_ce: 0.1744 (0.1772)  loss_ce_unscaled: 0.1744 (0.1772)  loss_point_unscaled: 54.5171 (73.4006)\n",
      "[ep 1755][lr 0.0000100][3.25s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1709 (0.1841)  loss_ce: 0.1709 (0.1841)  loss_ce_unscaled: 0.1709 (0.1841)  loss_point_unscaled: 60.1229 (98.5682)\n",
      "[ep 1756][lr 0.0000100][3.43s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1757 (0.1748)  loss_ce: 0.1757 (0.1748)  loss_ce_unscaled: 0.1757 (0.1748)  loss_point_unscaled: 54.1610 (81.8549)\n",
      "[ep 1757][lr 0.0000100][3.19s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1783 (0.1750)  loss_ce: 0.1783 (0.1750)  loss_ce_unscaled: 0.1783 (0.1750)  loss_point_unscaled: 51.5158 (64.5280)\n",
      "[ep 1758][lr 0.0000100][2.42s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1720 (0.1788)  loss_ce: 0.1720 (0.1788)  loss_ce_unscaled: 0.1720 (0.1788)  loss_point_unscaled: 85.0622 (129.5761)\n",
      "[ep 1759][lr 0.0000100][2.52s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1683 (0.1726)  loss_ce: 0.1683 (0.1726)  loss_ce_unscaled: 0.1683 (0.1726)  loss_point_unscaled: 51.2375 (62.6669)\n",
      "[ep 1760][lr 0.0000100][3.08s]\n",
      "=======================================test=======================================\n",
      "mae: 148.4120879120879 mse: 233.47173158767947 time: 3.4594287872314453 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1636 (0.1766)  loss_ce: 0.1636 (0.1766)  loss_ce_unscaled: 0.1636 (0.1766)  loss_point_unscaled: 51.7561 (59.3484)\n",
      "[ep 1761][lr 0.0000100][3.23s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1638 (0.1662)  loss_ce: 0.1638 (0.1662)  loss_ce_unscaled: 0.1638 (0.1662)  loss_point_unscaled: 50.0705 (67.6432)\n",
      "[ep 1762][lr 0.0000100][2.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1634 (0.1672)  loss_ce: 0.1634 (0.1672)  loss_ce_unscaled: 0.1634 (0.1672)  loss_point_unscaled: 52.5767 (70.6309)\n",
      "[ep 1763][lr 0.0000100][2.43s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1849 (0.1809)  loss_ce: 0.1849 (0.1809)  loss_ce_unscaled: 0.1849 (0.1809)  loss_point_unscaled: 52.4648 (61.1294)\n",
      "[ep 1764][lr 0.0000100][2.50s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1708 (0.1714)  loss_ce: 0.1708 (0.1714)  loss_ce_unscaled: 0.1708 (0.1714)  loss_point_unscaled: 65.9623 (75.4557)\n",
      "[ep 1765][lr 0.0000100][3.27s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1705 (0.1760)  loss_ce: 0.1705 (0.1760)  loss_ce_unscaled: 0.1705 (0.1760)  loss_point_unscaled: 52.9911 (56.2745)\n",
      "[ep 1766][lr 0.0000100][3.29s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1573 (0.1689)  loss_ce: 0.1573 (0.1689)  loss_ce_unscaled: 0.1573 (0.1689)  loss_point_unscaled: 53.8057 (77.7666)\n",
      "[ep 1767][lr 0.0000100][3.08s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1722 (0.1731)  loss_ce: 0.1722 (0.1731)  loss_ce_unscaled: 0.1722 (0.1731)  loss_point_unscaled: 53.1109 (113.2569)\n",
      "[ep 1768][lr 0.0000100][3.32s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1718 (0.1722)  loss_ce: 0.1718 (0.1722)  loss_ce_unscaled: 0.1718 (0.1722)  loss_point_unscaled: 50.3038 (79.1642)\n",
      "[ep 1769][lr 0.0000100][3.07s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1708 (0.1699)  loss_ce: 0.1708 (0.1699)  loss_ce_unscaled: 0.1708 (0.1699)  loss_point_unscaled: 52.5270 (73.2166)\n",
      "[ep 1770][lr 0.0000100][3.29s]\n",
      "=======================================test=======================================\n",
      "mae: 150.97252747252747 mse: 233.79853025041186 time: 2.2705204486846924 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1668 (0.1753)  loss_ce: 0.1668 (0.1753)  loss_ce_unscaled: 0.1668 (0.1753)  loss_point_unscaled: 53.5151 (95.4850)\n",
      "[ep 1771][lr 0.0000100][2.46s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1769 (0.1743)  loss_ce: 0.1769 (0.1743)  loss_ce_unscaled: 0.1769 (0.1743)  loss_point_unscaled: 53.1786 (77.0886)\n",
      "[ep 1772][lr 0.0000100][2.54s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1650 (0.1659)  loss_ce: 0.1650 (0.1659)  loss_ce_unscaled: 0.1650 (0.1659)  loss_point_unscaled: 54.2298 (130.0741)\n",
      "[ep 1773][lr 0.0000100][3.07s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1693 (0.1707)  loss_ce: 0.1693 (0.1707)  loss_ce_unscaled: 0.1693 (0.1707)  loss_point_unscaled: 49.2750 (71.6961)\n",
      "[ep 1774][lr 0.0000100][2.41s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1667 (0.1724)  loss_ce: 0.1667 (0.1724)  loss_ce_unscaled: 0.1667 (0.1724)  loss_point_unscaled: 49.9627 (86.5773)\n",
      "[ep 1775][lr 0.0000100][3.14s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1747 (0.1778)  loss_ce: 0.1747 (0.1778)  loss_ce_unscaled: 0.1747 (0.1778)  loss_point_unscaled: 53.3970 (91.0120)\n",
      "[ep 1776][lr 0.0000100][3.23s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1589 (0.1705)  loss_ce: 0.1589 (0.1705)  loss_ce_unscaled: 0.1589 (0.1705)  loss_point_unscaled: 53.6382 (72.5528)\n",
      "[ep 1777][lr 0.0000100][3.17s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1741 (0.1777)  loss_ce: 0.1741 (0.1777)  loss_ce_unscaled: 0.1741 (0.1777)  loss_point_unscaled: 51.3115 (114.0107)\n",
      "[ep 1778][lr 0.0000100][2.73s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1720 (0.1724)  loss_ce: 0.1720 (0.1724)  loss_ce_unscaled: 0.1720 (0.1724)  loss_point_unscaled: 56.6071 (100.3873)\n",
      "[ep 1779][lr 0.0000100][3.41s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1675 (0.1716)  loss_ce: 0.1675 (0.1716)  loss_ce_unscaled: 0.1675 (0.1716)  loss_point_unscaled: 56.1804 (86.4680)\n",
      "[ep 1780][lr 0.0000100][3.18s]\n",
      "=======================================test=======================================\n",
      "mae: 146.6978021978022 mse: 230.69468667816878 time: 2.3160293102264404 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1865 (0.1837)  loss_ce: 0.1865 (0.1837)  loss_ce_unscaled: 0.1865 (0.1837)  loss_point_unscaled: 49.8413 (68.6676)\n",
      "[ep 1781][lr 0.0000100][3.11s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1726 (0.1806)  loss_ce: 0.1726 (0.1806)  loss_ce_unscaled: 0.1726 (0.1806)  loss_point_unscaled: 53.0240 (74.6086)\n",
      "[ep 1782][lr 0.0000100][2.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1701 (0.1709)  loss_ce: 0.1701 (0.1709)  loss_ce_unscaled: 0.1701 (0.1709)  loss_point_unscaled: 53.8611 (105.3032)\n",
      "[ep 1783][lr 0.0000100][2.37s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1785 (0.1818)  loss_ce: 0.1785 (0.1818)  loss_ce_unscaled: 0.1785 (0.1818)  loss_point_unscaled: 53.6142 (79.0760)\n",
      "[ep 1784][lr 0.0000100][3.24s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1739 (0.1801)  loss_ce: 0.1739 (0.1801)  loss_ce_unscaled: 0.1739 (0.1801)  loss_point_unscaled: 50.9111 (91.6114)\n",
      "[ep 1785][lr 0.0000100][2.45s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1520 (0.1699)  loss_ce: 0.1520 (0.1699)  loss_ce_unscaled: 0.1520 (0.1699)  loss_point_unscaled: 50.9548 (80.2146)\n",
      "[ep 1786][lr 0.0000100][3.23s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1777 (0.1752)  loss_ce: 0.1777 (0.1752)  loss_ce_unscaled: 0.1777 (0.1752)  loss_point_unscaled: 53.4607 (99.6248)\n",
      "[ep 1787][lr 0.0000100][2.49s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1722 (0.1729)  loss_ce: 0.1722 (0.1729)  loss_ce_unscaled: 0.1722 (0.1729)  loss_point_unscaled: 53.0733 (75.4964)\n",
      "[ep 1788][lr 0.0000100][3.15s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1739 (0.1733)  loss_ce: 0.1739 (0.1733)  loss_ce_unscaled: 0.1739 (0.1733)  loss_point_unscaled: 53.1914 (86.4926)\n",
      "[ep 1789][lr 0.0000100][3.05s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1783 (0.1756)  loss_ce: 0.1783 (0.1756)  loss_ce_unscaled: 0.1783 (0.1756)  loss_point_unscaled: 54.2141 (97.1265)\n",
      "[ep 1790][lr 0.0000100][3.25s]\n",
      "=======================================test=======================================\n",
      "mae: 148.95604395604394 mse: 233.95009799452606 time: 2.265780448913574 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1639 (0.1690)  loss_ce: 0.1639 (0.1690)  loss_ce_unscaled: 0.1639 (0.1690)  loss_point_unscaled: 51.1442 (92.8272)\n",
      "[ep 1791][lr 0.0000100][3.33s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1721 (0.1704)  loss_ce: 0.1721 (0.1704)  loss_ce_unscaled: 0.1721 (0.1704)  loss_point_unscaled: 51.0197 (72.1520)\n",
      "[ep 1792][lr 0.0000100][2.97s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1770 (0.1773)  loss_ce: 0.1770 (0.1773)  loss_ce_unscaled: 0.1770 (0.1773)  loss_point_unscaled: 58.9352 (72.0249)\n",
      "[ep 1793][lr 0.0000100][2.44s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1712 (0.1710)  loss_ce: 0.1712 (0.1710)  loss_ce_unscaled: 0.1712 (0.1710)  loss_point_unscaled: 57.6335 (98.2618)\n",
      "[ep 1794][lr 0.0000100][2.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1616 (0.1658)  loss_ce: 0.1616 (0.1658)  loss_ce_unscaled: 0.1616 (0.1658)  loss_point_unscaled: 51.2698 (63.9560)\n",
      "[ep 1795][lr 0.0000100][3.07s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1658 (0.1673)  loss_ce: 0.1658 (0.1673)  loss_ce_unscaled: 0.1658 (0.1673)  loss_point_unscaled: 51.3134 (103.2227)\n",
      "[ep 1796][lr 0.0000100][3.26s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1685 (0.1702)  loss_ce: 0.1685 (0.1702)  loss_ce_unscaled: 0.1685 (0.1702)  loss_point_unscaled: 53.7929 (92.1420)\n",
      "[ep 1797][lr 0.0000100][3.31s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1653 (0.1733)  loss_ce: 0.1653 (0.1733)  loss_ce_unscaled: 0.1653 (0.1733)  loss_point_unscaled: 51.4137 (63.2292)\n",
      "[ep 1798][lr 0.0000100][3.09s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1600 (0.1651)  loss_ce: 0.1600 (0.1651)  loss_ce_unscaled: 0.1600 (0.1651)  loss_point_unscaled: 54.1064 (63.0255)\n",
      "[ep 1799][lr 0.0000100][2.52s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1642 (0.1713)  loss_ce: 0.1642 (0.1713)  loss_ce_unscaled: 0.1642 (0.1713)  loss_point_unscaled: 56.4631 (72.3988)\n",
      "[ep 1800][lr 0.0000100][2.35s]\n",
      "=======================================test=======================================\n",
      "mae: 143.71978021978023 mse: 227.02934650794415 time: 2.651935338973999 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1652 (0.1746)  loss_ce: 0.1652 (0.1746)  loss_ce_unscaled: 0.1652 (0.1746)  loss_point_unscaled: 55.0240 (90.1149)\n",
      "[ep 1801][lr 0.0000100][3.15s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1730 (0.1771)  loss_ce: 0.1730 (0.1771)  loss_ce_unscaled: 0.1730 (0.1771)  loss_point_unscaled: 51.5703 (85.2155)\n",
      "[ep 1802][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1732 (0.1691)  loss_ce: 0.1732 (0.1691)  loss_ce_unscaled: 0.1732 (0.1691)  loss_point_unscaled: 52.9292 (105.2053)\n",
      "[ep 1803][lr 0.0000100][3.26s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1698 (0.1732)  loss_ce: 0.1698 (0.1732)  loss_ce_unscaled: 0.1698 (0.1732)  loss_point_unscaled: 56.1403 (63.1835)\n",
      "[ep 1804][lr 0.0000100][3.13s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1670 (0.1721)  loss_ce: 0.1670 (0.1721)  loss_ce_unscaled: 0.1670 (0.1721)  loss_point_unscaled: 50.9573 (94.3716)\n",
      "[ep 1805][lr 0.0000100][3.25s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1592 (0.1627)  loss_ce: 0.1592 (0.1627)  loss_ce_unscaled: 0.1592 (0.1627)  loss_point_unscaled: 56.8570 (107.0694)\n",
      "[ep 1806][lr 0.0000100][2.91s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1752 (0.1756)  loss_ce: 0.1752 (0.1756)  loss_ce_unscaled: 0.1752 (0.1756)  loss_point_unscaled: 52.4231 (108.4552)\n",
      "[ep 1807][lr 0.0000100][3.46s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1674 (0.1675)  loss_ce: 0.1674 (0.1675)  loss_ce_unscaled: 0.1674 (0.1675)  loss_point_unscaled: 51.0004 (83.4727)\n",
      "[ep 1808][lr 0.0000100][3.12s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1756 (0.1760)  loss_ce: 0.1756 (0.1760)  loss_ce_unscaled: 0.1756 (0.1760)  loss_point_unscaled: 53.3120 (75.6396)\n",
      "[ep 1809][lr 0.0000100][3.16s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1614 (0.1744)  loss_ce: 0.1614 (0.1744)  loss_ce_unscaled: 0.1614 (0.1744)  loss_point_unscaled: 52.9360 (84.7275)\n",
      "[ep 1810][lr 0.0000100][2.45s]\n",
      "=======================================test=======================================\n",
      "mae: 149.65934065934067 mse: 237.5958800101597 time: 2.2346181869506836 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1775 (0.1757)  loss_ce: 0.1775 (0.1757)  loss_ce_unscaled: 0.1775 (0.1757)  loss_point_unscaled: 62.4537 (90.5936)\n",
      "[ep 1811][lr 0.0000100][2.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1541 (0.1674)  loss_ce: 0.1541 (0.1674)  loss_ce_unscaled: 0.1541 (0.1674)  loss_point_unscaled: 53.4836 (74.1559)\n",
      "[ep 1812][lr 0.0000100][3.12s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1664 (0.1730)  loss_ce: 0.1664 (0.1730)  loss_ce_unscaled: 0.1664 (0.1730)  loss_point_unscaled: 51.0369 (60.2555)\n",
      "[ep 1813][lr 0.0000100][2.35s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1681 (0.1726)  loss_ce: 0.1681 (0.1726)  loss_ce_unscaled: 0.1681 (0.1726)  loss_point_unscaled: 51.2918 (68.6793)\n",
      "[ep 1814][lr 0.0000100][3.16s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1743 (0.1757)  loss_ce: 0.1743 (0.1757)  loss_ce_unscaled: 0.1743 (0.1757)  loss_point_unscaled: 51.8178 (90.9557)\n",
      "[ep 1815][lr 0.0000100][3.24s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1714 (0.1769)  loss_ce: 0.1714 (0.1769)  loss_ce_unscaled: 0.1714 (0.1769)  loss_point_unscaled: 51.9327 (69.3054)\n",
      "[ep 1816][lr 0.0000100][3.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1711 (0.1738)  loss_ce: 0.1711 (0.1738)  loss_ce_unscaled: 0.1711 (0.1738)  loss_point_unscaled: 51.5466 (82.6915)\n",
      "[ep 1817][lr 0.0000100][2.43s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1716 (0.1739)  loss_ce: 0.1716 (0.1739)  loss_ce_unscaled: 0.1716 (0.1739)  loss_point_unscaled: 52.4458 (96.7959)\n",
      "[ep 1818][lr 0.0000100][3.35s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1678 (0.1696)  loss_ce: 0.1678 (0.1696)  loss_ce_unscaled: 0.1678 (0.1696)  loss_point_unscaled: 52.9549 (113.9975)\n",
      "[ep 1819][lr 0.0000100][3.24s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1722 (0.1785)  loss_ce: 0.1722 (0.1785)  loss_ce_unscaled: 0.1722 (0.1785)  loss_point_unscaled: 54.5006 (77.4712)\n",
      "[ep 1820][lr 0.0000100][3.31s]\n",
      "=======================================test=======================================\n",
      "mae: 146.4945054945055 mse: 232.5211794678044 time: 2.2621352672576904 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1709 (0.1746)  loss_ce: 0.1709 (0.1746)  loss_ce_unscaled: 0.1709 (0.1746)  loss_point_unscaled: 52.9232 (84.5689)\n",
      "[ep 1821][lr 0.0000100][3.14s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1640 (0.1709)  loss_ce: 0.1640 (0.1709)  loss_ce_unscaled: 0.1640 (0.1709)  loss_point_unscaled: 53.8783 (81.9304)\n",
      "[ep 1822][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1678 (0.1703)  loss_ce: 0.1678 (0.1703)  loss_ce_unscaled: 0.1678 (0.1703)  loss_point_unscaled: 52.9582 (72.2067)\n",
      "[ep 1823][lr 0.0000100][2.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1655 (0.1705)  loss_ce: 0.1655 (0.1705)  loss_ce_unscaled: 0.1655 (0.1705)  loss_point_unscaled: 55.6108 (66.1843)\n",
      "[ep 1824][lr 0.0000100][2.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1815 (0.1741)  loss_ce: 0.1815 (0.1741)  loss_ce_unscaled: 0.1815 (0.1741)  loss_point_unscaled: 52.2627 (73.1463)\n",
      "[ep 1825][lr 0.0000100][2.46s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1761 (0.1732)  loss_ce: 0.1761 (0.1732)  loss_ce_unscaled: 0.1761 (0.1732)  loss_point_unscaled: 55.5099 (64.1859)\n",
      "[ep 1826][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1716 (0.1785)  loss_ce: 0.1716 (0.1785)  loss_ce_unscaled: 0.1716 (0.1785)  loss_point_unscaled: 51.5383 (83.6074)\n",
      "[ep 1827][lr 0.0000100][3.36s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1634 (0.1763)  loss_ce: 0.1634 (0.1763)  loss_ce_unscaled: 0.1634 (0.1763)  loss_point_unscaled: 54.5707 (89.2893)\n",
      "[ep 1828][lr 0.0000100][2.38s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1741 (0.1750)  loss_ce: 0.1741 (0.1750)  loss_ce_unscaled: 0.1741 (0.1750)  loss_point_unscaled: 51.1208 (88.3121)\n",
      "[ep 1829][lr 0.0000100][2.63s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1686 (0.1733)  loss_ce: 0.1686 (0.1733)  loss_ce_unscaled: 0.1686 (0.1733)  loss_point_unscaled: 56.0483 (112.0610)\n",
      "[ep 1830][lr 0.0000100][3.09s]\n",
      "=======================================test=======================================\n",
      "mae: 148.3131868131868 mse: 237.2966683103848 time: 4.1668126583099365 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1701 (0.1728)  loss_ce: 0.1701 (0.1728)  loss_ce_unscaled: 0.1701 (0.1728)  loss_point_unscaled: 50.9556 (84.8050)\n",
      "[ep 1831][lr 0.0000100][3.21s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1703 (0.1786)  loss_ce: 0.1703 (0.1786)  loss_ce_unscaled: 0.1703 (0.1786)  loss_point_unscaled: 55.5015 (73.3102)\n",
      "[ep 1832][lr 0.0000100][2.44s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1675 (0.1752)  loss_ce: 0.1675 (0.1752)  loss_ce_unscaled: 0.1675 (0.1752)  loss_point_unscaled: 52.9475 (71.0151)\n",
      "[ep 1833][lr 0.0000100][3.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1750 (0.1760)  loss_ce: 0.1750 (0.1760)  loss_ce_unscaled: 0.1750 (0.1760)  loss_point_unscaled: 55.3941 (76.5185)\n",
      "[ep 1834][lr 0.0000100][3.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1712 (0.1759)  loss_ce: 0.1712 (0.1759)  loss_ce_unscaled: 0.1712 (0.1759)  loss_point_unscaled: 50.6431 (102.6608)\n",
      "[ep 1835][lr 0.0000100][2.50s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1663 (0.1682)  loss_ce: 0.1663 (0.1682)  loss_ce_unscaled: 0.1663 (0.1682)  loss_point_unscaled: 56.6028 (79.6215)\n",
      "[ep 1836][lr 0.0000100][3.28s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1811 (0.1777)  loss_ce: 0.1811 (0.1777)  loss_ce_unscaled: 0.1811 (0.1777)  loss_point_unscaled: 63.1069 (94.4353)\n",
      "[ep 1837][lr 0.0000100][2.47s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1774 (0.1791)  loss_ce: 0.1774 (0.1791)  loss_ce_unscaled: 0.1774 (0.1791)  loss_point_unscaled: 55.7824 (89.2108)\n",
      "[ep 1838][lr 0.0000100][3.15s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1745 (0.1800)  loss_ce: 0.1745 (0.1800)  loss_ce_unscaled: 0.1745 (0.1800)  loss_point_unscaled: 54.2256 (97.5965)\n",
      "[ep 1839][lr 0.0000100][3.21s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1688 (0.1779)  loss_ce: 0.1688 (0.1779)  loss_ce_unscaled: 0.1688 (0.1779)  loss_point_unscaled: 51.9554 (66.5819)\n",
      "[ep 1840][lr 0.0000100][2.90s]\n",
      "=======================================test=======================================\n",
      "mae: 149.96703296703296 mse: 236.36947237857825 time: 2.240467071533203 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1611 (0.1719)  loss_ce: 0.1611 (0.1719)  loss_ce_unscaled: 0.1611 (0.1719)  loss_point_unscaled: 50.9124 (89.7822)\n",
      "[ep 1841][lr 0.0000100][3.25s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1595 (0.1663)  loss_ce: 0.1595 (0.1663)  loss_ce_unscaled: 0.1595 (0.1663)  loss_point_unscaled: 52.0866 (70.0805)\n",
      "[ep 1842][lr 0.0000100][2.79s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1777 (0.1773)  loss_ce: 0.1777 (0.1773)  loss_ce_unscaled: 0.1777 (0.1773)  loss_point_unscaled: 56.1690 (107.2824)\n",
      "[ep 1843][lr 0.0000100][3.21s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1721 (0.1742)  loss_ce: 0.1721 (0.1742)  loss_ce_unscaled: 0.1721 (0.1742)  loss_point_unscaled: 52.7382 (85.5306)\n",
      "[ep 1844][lr 0.0000100][2.47s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1747 (0.1789)  loss_ce: 0.1747 (0.1789)  loss_ce_unscaled: 0.1747 (0.1789)  loss_point_unscaled: 52.3325 (79.2008)\n",
      "[ep 1845][lr 0.0000100][3.27s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1658 (0.1685)  loss_ce: 0.1658 (0.1685)  loss_ce_unscaled: 0.1658 (0.1685)  loss_point_unscaled: 51.6383 (54.3668)\n",
      "[ep 1846][lr 0.0000100][3.17s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1698 (0.1740)  loss_ce: 0.1698 (0.1740)  loss_ce_unscaled: 0.1698 (0.1740)  loss_point_unscaled: 56.4386 (108.0361)\n",
      "[ep 1847][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1746 (0.1767)  loss_ce: 0.1746 (0.1767)  loss_ce_unscaled: 0.1746 (0.1767)  loss_point_unscaled: 52.5736 (73.1937)\n",
      "[ep 1848][lr 0.0000100][3.28s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1757 (0.1796)  loss_ce: 0.1757 (0.1796)  loss_ce_unscaled: 0.1757 (0.1796)  loss_point_unscaled: 54.6569 (64.6856)\n",
      "[ep 1849][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1690 (0.1704)  loss_ce: 0.1690 (0.1704)  loss_ce_unscaled: 0.1690 (0.1704)  loss_point_unscaled: 55.8552 (69.9398)\n",
      "[ep 1850][lr 0.0000100][3.28s]\n",
      "=======================================test=======================================\n",
      "mae: 148.22527472527472 mse: 232.19576573739158 time: 4.222733974456787 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1623 (0.1726)  loss_ce: 0.1623 (0.1726)  loss_ce_unscaled: 0.1623 (0.1726)  loss_point_unscaled: 51.3823 (82.3063)\n",
      "[ep 1851][lr 0.0000100][3.27s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1675 (0.1693)  loss_ce: 0.1675 (0.1693)  loss_ce_unscaled: 0.1675 (0.1693)  loss_point_unscaled: 51.0896 (87.2267)\n",
      "[ep 1852][lr 0.0000100][3.19s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1631 (0.1725)  loss_ce: 0.1631 (0.1725)  loss_ce_unscaled: 0.1631 (0.1725)  loss_point_unscaled: 62.4167 (78.4149)\n",
      "[ep 1853][lr 0.0000100][3.21s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1682 (0.1691)  loss_ce: 0.1682 (0.1691)  loss_ce_unscaled: 0.1682 (0.1691)  loss_point_unscaled: 51.9440 (77.4972)\n",
      "[ep 1854][lr 0.0000100][3.10s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1675 (0.1818)  loss_ce: 0.1675 (0.1818)  loss_ce_unscaled: 0.1675 (0.1818)  loss_point_unscaled: 52.9987 (64.7444)\n",
      "[ep 1855][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1649 (0.1717)  loss_ce: 0.1649 (0.1717)  loss_ce_unscaled: 0.1649 (0.1717)  loss_point_unscaled: 50.2106 (79.7431)\n",
      "[ep 1856][lr 0.0000100][2.50s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1767 (0.1750)  loss_ce: 0.1767 (0.1750)  loss_ce_unscaled: 0.1767 (0.1750)  loss_point_unscaled: 51.7696 (73.5062)\n",
      "[ep 1857][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1783 (0.1769)  loss_ce: 0.1783 (0.1769)  loss_ce_unscaled: 0.1783 (0.1769)  loss_point_unscaled: 53.7273 (97.9251)\n",
      "[ep 1858][lr 0.0000100][2.42s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1657 (0.1717)  loss_ce: 0.1657 (0.1717)  loss_ce_unscaled: 0.1657 (0.1717)  loss_point_unscaled: 53.3551 (95.3127)\n",
      "[ep 1859][lr 0.0000100][3.35s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1732 (0.1831)  loss_ce: 0.1732 (0.1831)  loss_ce_unscaled: 0.1732 (0.1831)  loss_point_unscaled: 49.1020 (87.0449)\n",
      "[ep 1860][lr 0.0000100][3.05s]\n",
      "=======================================test=======================================\n",
      "mae: 150.05494505494505 mse: 235.54293138931706 time: 2.275193452835083 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1658 (0.1693)  loss_ce: 0.1658 (0.1693)  loss_ce_unscaled: 0.1658 (0.1693)  loss_point_unscaled: 54.2124 (79.8157)\n",
      "[ep 1861][lr 0.0000100][3.13s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1660 (0.1799)  loss_ce: 0.1660 (0.1799)  loss_ce_unscaled: 0.1660 (0.1799)  loss_point_unscaled: 54.1723 (92.0286)\n",
      "[ep 1862][lr 0.0000100][2.75s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1637 (0.1726)  loss_ce: 0.1637 (0.1726)  loss_ce_unscaled: 0.1637 (0.1726)  loss_point_unscaled: 52.7417 (85.1295)\n",
      "[ep 1863][lr 0.0000100][2.36s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1730 (0.1722)  loss_ce: 0.1730 (0.1722)  loss_ce_unscaled: 0.1730 (0.1722)  loss_point_unscaled: 53.6130 (56.3212)\n",
      "[ep 1864][lr 0.0000100][3.26s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1524 (0.1699)  loss_ce: 0.1524 (0.1699)  loss_ce_unscaled: 0.1524 (0.1699)  loss_point_unscaled: 52.6651 (66.7765)\n",
      "[ep 1865][lr 0.0000100][2.42s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1771 (0.1748)  loss_ce: 0.1771 (0.1748)  loss_ce_unscaled: 0.1771 (0.1748)  loss_point_unscaled: 54.6704 (72.6671)\n",
      "[ep 1866][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1798 (0.1720)  loss_ce: 0.1798 (0.1720)  loss_ce_unscaled: 0.1798 (0.1720)  loss_point_unscaled: 52.1932 (71.9847)\n",
      "[ep 1867][lr 0.0000100][2.49s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1642 (0.1681)  loss_ce: 0.1642 (0.1681)  loss_ce_unscaled: 0.1642 (0.1681)  loss_point_unscaled: 54.4572 (77.7419)\n",
      "[ep 1868][lr 0.0000100][3.09s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1819 (0.1746)  loss_ce: 0.1819 (0.1746)  loss_ce_unscaled: 0.1819 (0.1746)  loss_point_unscaled: 58.1663 (111.2829)\n",
      "[ep 1869][lr 0.0000100][2.50s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1609 (0.1691)  loss_ce: 0.1609 (0.1691)  loss_ce_unscaled: 0.1609 (0.1691)  loss_point_unscaled: 51.0310 (73.7382)\n",
      "[ep 1870][lr 0.0000100][3.24s]\n",
      "=======================================test=======================================\n",
      "mae: 149.0054945054945 mse: 237.47426696913388 time: 2.2513375282287598 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1824 (0.1794)  loss_ce: 0.1824 (0.1794)  loss_ce_unscaled: 0.1824 (0.1794)  loss_point_unscaled: 48.6338 (69.3932)\n",
      "[ep 1871][lr 0.0000100][3.18s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1718 (0.1765)  loss_ce: 0.1718 (0.1765)  loss_ce_unscaled: 0.1718 (0.1765)  loss_point_unscaled: 51.2718 (94.3061)\n",
      "[ep 1872][lr 0.0000100][3.25s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1552 (0.1666)  loss_ce: 0.1552 (0.1666)  loss_ce_unscaled: 0.1552 (0.1666)  loss_point_unscaled: 52.2961 (71.6339)\n",
      "[ep 1873][lr 0.0000100][2.95s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1634 (0.1723)  loss_ce: 0.1634 (0.1723)  loss_ce_unscaled: 0.1634 (0.1723)  loss_point_unscaled: 55.8237 (83.7246)\n",
      "[ep 1874][lr 0.0000100][3.19s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1570 (0.1699)  loss_ce: 0.1570 (0.1699)  loss_ce_unscaled: 0.1570 (0.1699)  loss_point_unscaled: 55.2928 (65.5705)\n",
      "[ep 1875][lr 0.0000100][2.38s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1689 (0.1736)  loss_ce: 0.1689 (0.1736)  loss_ce_unscaled: 0.1689 (0.1736)  loss_point_unscaled: 51.6778 (69.0560)\n",
      "[ep 1876][lr 0.0000100][2.82s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1645 (0.1757)  loss_ce: 0.1645 (0.1757)  loss_ce_unscaled: 0.1645 (0.1757)  loss_point_unscaled: 53.0079 (87.5363)\n",
      "[ep 1877][lr 0.0000100][3.25s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1705 (0.1748)  loss_ce: 0.1705 (0.1748)  loss_ce_unscaled: 0.1705 (0.1748)  loss_point_unscaled: 53.3966 (105.5846)\n",
      "[ep 1878][lr 0.0000100][3.27s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1784 (0.1765)  loss_ce: 0.1784 (0.1765)  loss_ce_unscaled: 0.1784 (0.1765)  loss_point_unscaled: 53.6057 (89.8797)\n",
      "[ep 1879][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1677 (0.1772)  loss_ce: 0.1677 (0.1772)  loss_ce_unscaled: 0.1677 (0.1772)  loss_point_unscaled: 51.3824 (101.4645)\n",
      "[ep 1880][lr 0.0000100][2.39s]\n",
      "=======================================test=======================================\n",
      "mae: 144.3131868131868 mse: 227.7586321207047 time: 2.243234157562256 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1711 (0.1717)  loss_ce: 0.1711 (0.1717)  loss_ce_unscaled: 0.1711 (0.1717)  loss_point_unscaled: 53.2903 (76.1919)\n",
      "[ep 1881][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1701 (0.1756)  loss_ce: 0.1701 (0.1756)  loss_ce_unscaled: 0.1701 (0.1756)  loss_point_unscaled: 56.3830 (79.5526)\n",
      "[ep 1882][lr 0.0000100][2.94s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1716 (0.1786)  loss_ce: 0.1716 (0.1786)  loss_ce_unscaled: 0.1716 (0.1786)  loss_point_unscaled: 53.3532 (78.7995)\n",
      "[ep 1883][lr 0.0000100][3.11s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1673 (0.1725)  loss_ce: 0.1673 (0.1725)  loss_ce_unscaled: 0.1673 (0.1725)  loss_point_unscaled: 51.4098 (75.3492)\n",
      "[ep 1884][lr 0.0000100][3.25s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1744 (0.1747)  loss_ce: 0.1744 (0.1747)  loss_ce_unscaled: 0.1744 (0.1747)  loss_point_unscaled: 50.7493 (81.0488)\n",
      "[ep 1885][lr 0.0000100][3.31s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1880 (0.1832)  loss_ce: 0.1880 (0.1832)  loss_ce_unscaled: 0.1880 (0.1832)  loss_point_unscaled: 48.9089 (70.9677)\n",
      "[ep 1886][lr 0.0000100][3.12s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1709 (0.1753)  loss_ce: 0.1709 (0.1753)  loss_ce_unscaled: 0.1709 (0.1753)  loss_point_unscaled: 53.4939 (90.0974)\n",
      "[ep 1887][lr 0.0000100][2.82s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1784 (0.1778)  loss_ce: 0.1784 (0.1778)  loss_ce_unscaled: 0.1784 (0.1778)  loss_point_unscaled: 57.5667 (98.4585)\n",
      "[ep 1888][lr 0.0000100][3.16s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1719 (0.1727)  loss_ce: 0.1719 (0.1727)  loss_ce_unscaled: 0.1719 (0.1727)  loss_point_unscaled: 51.5728 (66.8038)\n",
      "[ep 1889][lr 0.0000100][3.08s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1700 (0.1699)  loss_ce: 0.1700 (0.1699)  loss_ce_unscaled: 0.1700 (0.1699)  loss_point_unscaled: 54.2427 (73.0707)\n",
      "[ep 1890][lr 0.0000100][3.05s]\n",
      "=======================================test=======================================\n",
      "mae: 147.5054945054945 mse: 233.52621802593066 time: 2.627901077270508 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1666 (0.1717)  loss_ce: 0.1666 (0.1717)  loss_ce_unscaled: 0.1666 (0.1717)  loss_point_unscaled: 51.8207 (87.4639)\n",
      "[ep 1891][lr 0.0000100][3.18s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1702 (0.1747)  loss_ce: 0.1702 (0.1747)  loss_ce_unscaled: 0.1702 (0.1747)  loss_point_unscaled: 51.6206 (57.4519)\n",
      "[ep 1892][lr 0.0000100][2.43s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1630 (0.1694)  loss_ce: 0.1630 (0.1694)  loss_ce_unscaled: 0.1630 (0.1694)  loss_point_unscaled: 51.2211 (93.9169)\n",
      "[ep 1893][lr 0.0000100][3.71s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1690 (0.1691)  loss_ce: 0.1690 (0.1691)  loss_ce_unscaled: 0.1690 (0.1691)  loss_point_unscaled: 51.4808 (83.2071)\n",
      "[ep 1894][lr 0.0000100][3.06s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1727 (0.1793)  loss_ce: 0.1727 (0.1793)  loss_ce_unscaled: 0.1727 (0.1793)  loss_point_unscaled: 55.6102 (90.6930)\n",
      "[ep 1895][lr 0.0000100][3.31s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1708 (0.1743)  loss_ce: 0.1708 (0.1743)  loss_ce_unscaled: 0.1708 (0.1743)  loss_point_unscaled: 56.2475 (80.4695)\n",
      "[ep 1896][lr 0.0000100][3.29s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1758 (0.1809)  loss_ce: 0.1758 (0.1809)  loss_ce_unscaled: 0.1758 (0.1809)  loss_point_unscaled: 56.3135 (100.9700)\n",
      "[ep 1897][lr 0.0000100][2.45s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1732 (0.1764)  loss_ce: 0.1732 (0.1764)  loss_ce_unscaled: 0.1732 (0.1764)  loss_point_unscaled: 57.0825 (95.4803)\n",
      "[ep 1898][lr 0.0000100][2.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1818 (0.1767)  loss_ce: 0.1818 (0.1767)  loss_ce_unscaled: 0.1818 (0.1767)  loss_point_unscaled: 50.1469 (64.6793)\n",
      "[ep 1899][lr 0.0000100][3.08s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1722 (0.1681)  loss_ce: 0.1722 (0.1681)  loss_ce_unscaled: 0.1722 (0.1681)  loss_point_unscaled: 52.2128 (85.6258)\n",
      "[ep 1900][lr 0.0000100][2.42s]\n",
      "=======================================test=======================================\n",
      "mae: 145.32417582417582 mse: 228.41175226206855 time: 4.195826530456543 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1707 (0.1780)  loss_ce: 0.1707 (0.1780)  loss_ce_unscaled: 0.1707 (0.1780)  loss_point_unscaled: 50.3079 (55.1901)\n",
      "[ep 1901][lr 0.0000100][2.43s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1747 (0.1742)  loss_ce: 0.1747 (0.1742)  loss_ce_unscaled: 0.1747 (0.1742)  loss_point_unscaled: 53.9811 (142.4208)\n",
      "[ep 1902][lr 0.0000100][2.42s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1640 (0.1770)  loss_ce: 0.1640 (0.1770)  loss_ce_unscaled: 0.1640 (0.1770)  loss_point_unscaled: 53.6331 (100.7029)\n",
      "[ep 1903][lr 0.0000100][3.17s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1803 (0.1777)  loss_ce: 0.1803 (0.1777)  loss_ce_unscaled: 0.1803 (0.1777)  loss_point_unscaled: 54.4465 (77.3875)\n",
      "[ep 1904][lr 0.0000100][2.38s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1625 (0.1679)  loss_ce: 0.1625 (0.1679)  loss_ce_unscaled: 0.1625 (0.1679)  loss_point_unscaled: 54.4288 (117.9979)\n",
      "[ep 1905][lr 0.0000100][2.45s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1731 (0.1764)  loss_ce: 0.1731 (0.1764)  loss_ce_unscaled: 0.1731 (0.1764)  loss_point_unscaled: 59.5821 (103.4875)\n",
      "[ep 1906][lr 0.0000100][2.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1678 (0.1732)  loss_ce: 0.1678 (0.1732)  loss_ce_unscaled: 0.1678 (0.1732)  loss_point_unscaled: 54.3192 (85.5274)\n",
      "[ep 1907][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1679 (0.1713)  loss_ce: 0.1679 (0.1713)  loss_ce_unscaled: 0.1679 (0.1713)  loss_point_unscaled: 53.9417 (67.1844)\n",
      "[ep 1908][lr 0.0000100][3.18s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1745 (0.1752)  loss_ce: 0.1745 (0.1752)  loss_ce_unscaled: 0.1745 (0.1752)  loss_point_unscaled: 53.6841 (96.7033)\n",
      "[ep 1909][lr 0.0000100][3.39s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1641 (0.1724)  loss_ce: 0.1641 (0.1724)  loss_ce_unscaled: 0.1641 (0.1724)  loss_point_unscaled: 53.2545 (69.9760)\n",
      "[ep 1910][lr 0.0000100][3.28s]\n",
      "=======================================test=======================================\n",
      "mae: 149.4065934065934 mse: 233.9132693594307 time: 4.1866419315338135 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1678 (0.1677)  loss_ce: 0.1678 (0.1677)  loss_ce_unscaled: 0.1678 (0.1677)  loss_point_unscaled: 50.2702 (58.7327)\n",
      "[ep 1911][lr 0.0000100][2.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1737 (0.1754)  loss_ce: 0.1737 (0.1754)  loss_ce_unscaled: 0.1737 (0.1754)  loss_point_unscaled: 53.4476 (112.6742)\n",
      "[ep 1912][lr 0.0000100][3.30s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1679 (0.1725)  loss_ce: 0.1679 (0.1725)  loss_ce_unscaled: 0.1679 (0.1725)  loss_point_unscaled: 51.8130 (98.6887)\n",
      "[ep 1913][lr 0.0000100][2.47s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1712 (0.1720)  loss_ce: 0.1712 (0.1720)  loss_ce_unscaled: 0.1712 (0.1720)  loss_point_unscaled: 54.3949 (73.0621)\n",
      "[ep 1914][lr 0.0000100][3.22s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1740 (0.1747)  loss_ce: 0.1740 (0.1747)  loss_ce_unscaled: 0.1740 (0.1747)  loss_point_unscaled: 54.5374 (81.5353)\n",
      "[ep 1915][lr 0.0000100][2.34s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1752 (0.1762)  loss_ce: 0.1752 (0.1762)  loss_ce_unscaled: 0.1752 (0.1762)  loss_point_unscaled: 53.4303 (116.1795)\n",
      "[ep 1916][lr 0.0000100][3.19s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1704 (0.1698)  loss_ce: 0.1704 (0.1698)  loss_ce_unscaled: 0.1704 (0.1698)  loss_point_unscaled: 52.6199 (78.5614)\n",
      "[ep 1917][lr 0.0000100][2.44s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1562 (0.1694)  loss_ce: 0.1562 (0.1694)  loss_ce_unscaled: 0.1562 (0.1694)  loss_point_unscaled: 53.9847 (87.7212)\n",
      "[ep 1918][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1681 (0.1723)  loss_ce: 0.1681 (0.1723)  loss_ce_unscaled: 0.1681 (0.1723)  loss_point_unscaled: 52.4762 (84.9206)\n",
      "[ep 1919][lr 0.0000100][2.49s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1795 (0.1765)  loss_ce: 0.1795 (0.1765)  loss_ce_unscaled: 0.1795 (0.1765)  loss_point_unscaled: 52.1016 (90.4232)\n",
      "[ep 1920][lr 0.0000100][2.75s]\n",
      "=======================================test=======================================\n",
      "mae: 148.6978021978022 mse: 234.33704711025027 time: 4.22625994682312 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1702 (0.1743)  loss_ce: 0.1702 (0.1743)  loss_ce_unscaled: 0.1702 (0.1743)  loss_point_unscaled: 53.8707 (59.4058)\n",
      "[ep 1921][lr 0.0000100][2.50s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1615 (0.1678)  loss_ce: 0.1615 (0.1678)  loss_ce_unscaled: 0.1615 (0.1678)  loss_point_unscaled: 53.1989 (85.1847)\n",
      "[ep 1922][lr 0.0000100][2.53s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1621 (0.1652)  loss_ce: 0.1621 (0.1652)  loss_ce_unscaled: 0.1621 (0.1652)  loss_point_unscaled: 53.0733 (80.0335)\n",
      "[ep 1923][lr 0.0000100][3.21s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1628 (0.1653)  loss_ce: 0.1628 (0.1653)  loss_ce_unscaled: 0.1628 (0.1653)  loss_point_unscaled: 53.6160 (100.2109)\n",
      "[ep 1924][lr 0.0000100][3.29s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1752 (0.1729)  loss_ce: 0.1752 (0.1729)  loss_ce_unscaled: 0.1752 (0.1729)  loss_point_unscaled: 55.7631 (104.5148)\n",
      "[ep 1925][lr 0.0000100][2.44s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1779 (0.1764)  loss_ce: 0.1779 (0.1764)  loss_ce_unscaled: 0.1779 (0.1764)  loss_point_unscaled: 52.0340 (79.3303)\n",
      "[ep 1926][lr 0.0000100][3.26s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1697 (0.1737)  loss_ce: 0.1697 (0.1737)  loss_ce_unscaled: 0.1697 (0.1737)  loss_point_unscaled: 54.2550 (79.9091)\n",
      "[ep 1927][lr 0.0000100][2.43s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1794 (0.1758)  loss_ce: 0.1794 (0.1758)  loss_ce_unscaled: 0.1794 (0.1758)  loss_point_unscaled: 53.6846 (75.3802)\n",
      "[ep 1928][lr 0.0000100][2.47s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1795 (0.1787)  loss_ce: 0.1795 (0.1787)  loss_ce_unscaled: 0.1795 (0.1787)  loss_point_unscaled: 52.2461 (78.0478)\n",
      "[ep 1929][lr 0.0000100][2.42s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1643 (0.1668)  loss_ce: 0.1643 (0.1668)  loss_ce_unscaled: 0.1643 (0.1668)  loss_point_unscaled: 54.1173 (114.6641)\n",
      "[ep 1930][lr 0.0000100][3.27s]\n",
      "=======================================test=======================================\n",
      "mae: 149.8956043956044 mse: 234.9401724171169 time: 2.8650524616241455 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1656 (0.1736)  loss_ce: 0.1656 (0.1736)  loss_ce_unscaled: 0.1656 (0.1736)  loss_point_unscaled: 64.0820 (112.4807)\n",
      "[ep 1931][lr 0.0000100][3.26s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1596 (0.1671)  loss_ce: 0.1596 (0.1671)  loss_ce_unscaled: 0.1596 (0.1671)  loss_point_unscaled: 50.9269 (98.2820)\n",
      "[ep 1932][lr 0.0000100][2.40s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1781 (0.1812)  loss_ce: 0.1781 (0.1812)  loss_ce_unscaled: 0.1781 (0.1812)  loss_point_unscaled: 49.4898 (71.1773)\n",
      "[ep 1933][lr 0.0000100][2.55s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1754 (0.1745)  loss_ce: 0.1754 (0.1745)  loss_ce_unscaled: 0.1754 (0.1745)  loss_point_unscaled: 52.2269 (93.2538)\n",
      "[ep 1934][lr 0.0000100][3.37s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1745 (0.1798)  loss_ce: 0.1745 (0.1798)  loss_ce_unscaled: 0.1745 (0.1798)  loss_point_unscaled: 54.8804 (67.4119)\n",
      "[ep 1935][lr 0.0000100][3.20s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1633 (0.1713)  loss_ce: 0.1633 (0.1713)  loss_ce_unscaled: 0.1633 (0.1713)  loss_point_unscaled: 57.1689 (61.2806)\n",
      "[ep 1936][lr 0.0000100][2.37s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1660 (0.1689)  loss_ce: 0.1660 (0.1689)  loss_ce_unscaled: 0.1660 (0.1689)  loss_point_unscaled: 55.4112 (63.8865)\n",
      "[ep 1937][lr 0.0000100][3.16s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1783 (0.1753)  loss_ce: 0.1783 (0.1753)  loss_ce_unscaled: 0.1783 (0.1753)  loss_point_unscaled: 53.9046 (73.5158)\n",
      "[ep 1938][lr 0.0000100][3.23s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1665 (0.1747)  loss_ce: 0.1665 (0.1747)  loss_ce_unscaled: 0.1665 (0.1747)  loss_point_unscaled: 52.6785 (67.9479)\n",
      "[ep 1939][lr 0.0000100][3.33s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1665 (0.1723)  loss_ce: 0.1665 (0.1723)  loss_ce_unscaled: 0.1665 (0.1723)  loss_point_unscaled: 58.0204 (98.6477)\n",
      "[ep 1940][lr 0.0000100][3.34s]\n",
      "=======================================test=======================================\n",
      "mae: 148.63186813186815 mse: 230.30568553817508 time: 2.2575066089630127 best mae: 139.3901098901099\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.000010  loss: 0.1750 (0.1728)  loss_ce: 0.1750 (0.1728)  loss_ce_unscaled: 0.1750 (0.1728)  loss_point_unscaled: 58.3982 (85.5485)\n",
      "[ep 1941][lr 0.0000100][2.42s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1696 (0.1764)  loss_ce: 0.1696 (0.1764)  loss_ce_unscaled: 0.1696 (0.1764)  loss_point_unscaled: 57.3673 (74.9158)\n",
      "[ep 1942][lr 0.0000100][3.23s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1709 (0.1752)  loss_ce: 0.1709 (0.1752)  loss_ce_unscaled: 0.1709 (0.1752)  loss_point_unscaled: 54.7162 (112.3440)\n",
      "[ep 1943][lr 0.0000100][3.28s]\n",
      "Averaged stats: lr: 0.000010  loss: 0.1753 (0.1740)  loss_ce: 0.1753 (0.1740)  loss_ce_unscaled: 0.1753 (0.1740)  loss_point_unscaled: 52.9927 (75.5556)\n",
      "[ep 1944][lr 0.0000100][3.08s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_490386/4190827515.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     stat = train_one_epoch(\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         args.clip_max_norm)\n",
      "\u001b[0;32m~/P2PNet/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_meter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSmoothedValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{value:.6f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# iterate all training samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0mdigest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'md5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mWELCOME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAuthenticationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'digest sent was rejected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from engine import *\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{}'.format(args.gpu_id)\n",
    "# create the logging file\n",
    "run_log_name = os.path.join(args.output_dir, 'run_log.txt')\n",
    "\n",
    "\n",
    "with open(run_log_name, \"w+\") as log_file:\n",
    "    log_file.write('Eval Log %s\\n' % time.strftime(\"%c\"))\n",
    "\n",
    "#if args.frozen_weights is not None:\n",
    "#    assert args.masks, \"Frozen training is meant for segmentation only\"\n",
    "# backup the arguments\n",
    "print(args)\n",
    "with open(run_log_name, \"a\") as log_file:\n",
    "    log_file.write(\"{}\".format(args))\n",
    "device = torch.device('cuda')\n",
    "'''\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "'''\n",
    "# get the P2PNet model\n",
    "model, criterion = build(args, training=True)\n",
    "# move to GPU\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "# use different optimation params for different parts of the model\n",
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "        \"lr\": args.lr_backbone,\n",
    "    },\n",
    "]\n",
    "# Adam is used by default\n",
    "optimizer = torch.optim.Adam(param_dicts, lr=args.lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "# create the training and valiation set\n",
    "train_set, val_set = loading_data(args.data_root)\n",
    "# create the sampler used during training\n",
    "sampler_train = torch.utils.data.RandomSampler(train_set)\n",
    "sampler_val = torch.utils.data.SequentialSampler(val_set)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args.batch_size, drop_last=True)\n",
    "# the dataloader for training\n",
    "data_loader_train = DataLoader(train_set, batch_sampler=batch_sampler_train,\n",
    "                               collate_fn=utils.collate_fn_crowd, num_workers=args.num_workers)\n",
    "\n",
    "data_loader_val = DataLoader(val_set, 1, sampler=sampler_val,\n",
    "                                drop_last=False, collate_fn=utils.collate_fn_crowd, num_workers=args.num_workers)\n",
    "\n",
    "# continue to train previous model\n",
    "args.resume = '/home/ding/P2PNet/ckpt/best_mae.pth'\n",
    "if args.frozen_weights is not None:\n",
    "    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "# resume the weights and training state if exists\n",
    "if args.resume:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "# save the performance during the training\n",
    "mae = []\n",
    "mse = []\n",
    "# the logger writer\n",
    "writer = SummaryWriter(args.tensorboard_dir)\n",
    "\n",
    "step = 0\n",
    "# training starts here\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    t1 = time.time()\n",
    "    stat = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "        args.clip_max_norm)\n",
    "\n",
    "    # record the training states after every epoch\n",
    "    if writer is not None:\n",
    "        with open(run_log_name, \"a\") as log_file:\n",
    "            log_file.write(\"loss/loss@{}: {}\".format(epoch, stat['loss']))\n",
    "            log_file.write(\"loss/loss_ce@{}: {}\".format(epoch, stat['loss_ce']))\n",
    "\n",
    "        writer.add_scalar('loss/loss', stat['loss'], epoch)\n",
    "        writer.add_scalar('loss/loss_ce', stat['loss_ce'], epoch)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print('[ep %d][lr %.7f][%.2fs]' % \\\n",
    "          (epoch, optimizer.param_groups[0]['lr'], t2 - t1))\n",
    "    with open(run_log_name, \"a\") as log_file:\n",
    "        log_file.write('[ep %d][lr %.7f][%.2fs]' % (epoch, optimizer.param_groups[0]['lr'], t2 - t1))\n",
    "    # change lr according to the scheduler\n",
    "    lr_scheduler.step()\n",
    "    # save latest weights every epoch\n",
    "    checkpoint_latest_path = os.path.join(args.checkpoints_dir, 'latest.pth')\n",
    "    torch.save({\n",
    "        'model': model_without_ddp.state_dict(),\n",
    "    }, checkpoint_latest_path)\n",
    "    # run evaluation\n",
    "    if epoch % args.eval_freq == 0 and epoch != 0:\n",
    "        t1 = time.time()\n",
    "        result = evaluate_crowd_no_overlap(model, data_loader_val, device)\n",
    "        t2 = time.time()\n",
    "\n",
    "        mae.append(result[0])\n",
    "        mse.append(result[1])\n",
    "        # print the evaluation results\n",
    "        print('=======================================test=======================================')\n",
    "        print(\"mae:\", result[0], \"mse:\", result[1], \"time:\", t2 - t1, \"best mae:\", np.min(mae), )\n",
    "        with open(run_log_name, \"a\") as log_file:\n",
    "            log_file.write(\"mae:{}, mse:{}, time:{}, best mae:{}\".format(result[0], \n",
    "                            result[1], t2 - t1, np.min(mae)))\n",
    "        print('=======================================test=======================================')\n",
    "        # recored the evaluation results\n",
    "        if writer is not None:\n",
    "            with open(run_log_name, \"a\") as log_file:\n",
    "                log_file.write(\"metric/mae@{}: {}\".format(step, result[0]))\n",
    "                log_file.write(\"metric/mse@{}: {}\".format(step, result[1]))\n",
    "            writer.add_scalar('metric/mae', result[0], step)\n",
    "            writer.add_scalar('metric/mse', result[1], step)\n",
    "            step += 1\n",
    "\n",
    "        # save the best model since begining\n",
    "        if abs(np.min(mae) - result[0]) < 0.01:\n",
    "            checkpoint_best_path = os.path.join(args.checkpoints_dir, 'best_mae.pth')\n",
    "            torch.save({\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "            }, checkpoint_best_path)\n",
    "# total time for training\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "537a1f7f-f2ad-4eac-bbeb-035d2c623759",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion = build(args, training=True)\n",
    "model.to(device)\n",
    "features = model.backbone(samples)\n",
    "features_fpn = model.neck(features)\n",
    "\n",
    "batch_size = features[0].shape[0]\n",
    "# run the regression and classification branch\n",
    "regression = model.regression(features_fpn[1]) * 64 # 8x\n",
    "classification = model.classification(features_fpn[1])\n",
    "anchor_points = model.anchor_points(samples).repeat(batch_size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "26a6a5a6-49fb-40b3-b618-52fed6a0bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 96, 128])\n",
      "torch.Size([1, 128, 48, 64])\n",
      "torch.Size([1, 256, 24, 32])\n"
     ]
    }
   ],
   "source": [
    "for i in features_fpn:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d3029744-ca0a-44a0-8d5b-62713e4b6901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12288, 2])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "028779b7-374f-4fa1-8447-1ec0dcfa0895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12288, 2])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "60ed76ae-2ac6-4ff7-bee3-0d0a6e771ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12288, 2])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "79c31903-1c36-45b5-ab77-d67c43d3f40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 96, 128])\n",
      "torch.Size([1, 128, 48, 64])\n",
      "torch.Size([1, 256, 24, 32])\n"
     ]
    }
   ],
   "source": [
    "for i in features_fpn:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2acdd47-fe17-43ef-ab65-062dcd540b42",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3072) must match the size of tensor b (49152) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3504486/655222878.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_coord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregression\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0manchor_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3072) must match the size of tensor b (49152) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "output_coord = regression + anchor_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c2100a8-5bcb-4a28-bad5-19c632794405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from /home/ding/P2PNet/yolov6/yolov6t.pt\n",
      "\n",
      "Fusing model...\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from yolov6.layers.common import DetectBackend\n",
    "\n",
    "cuda = device != 'cpu' and torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "\n",
    "weights = '/home/ding/P2PNet/yolov6/yolov6t.pt'\n",
    "model = DetectBackend(weights, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24ed3834-0a27-403c-93c9-3b49fa30a295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ding/Datasets/test_video/2_img/img_1.jpg'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(img_file, img_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9fb40c2f-2562-4748-80a3-510c05a4c4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ding/P2PNet'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26b924ce-a257-41c0-8e78-d4565b1c20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "790010fe-f343-4da4-8fc0-925c485383a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DetectBackend()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9d16dd81-8991-49dd-a8e6-f7448718e82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config (path: ./configs/yolov6s.py): {'model': {'type': 'YOLOv6s', 'pretrained': None, 'depth_multiple': 0.33, 'width_multiple': 0.5, 'backbone': {'type': 'EfficientRep', 'num_repeats': [1, 6, 12, 18, 6], 'out_channels': [64, 128, 256, 512, 1024]}, 'neck': {'type': 'RepPAN', 'num_repeats': [12, 12, 12, 12], 'out_channels': [256, 128, 128, 256, 256, 512]}, 'head': {'type': 'EffiDeHead', 'in_channels': [128, 256, 512], 'num_layers': 3, 'begin_indices': 24, 'anchors': 1, 'out_indices': [17, 20, 23], 'strides': [8, 16, 32], 'iou_type': 'siou'}}, 'solver': {'optim': 'SGD', 'lr_scheduler': 'Cosine', 'lr0': 0.01, 'lrf': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1}, 'data_aug': {'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "99a45c8f-12d6-45e5-b88a-d8704bfdabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov6.models.efficientrep import EfficientRep\n",
    "from yolov6.models.reppan import RepPANNeck\n",
    "def build_network(config, channels, num_classes):\n",
    "    depth_mul = config.model.depth_multiple\n",
    "    width_mul = config.model.width_multiple\n",
    "    num_repeat_backbone = config.model.backbone.num_repeats\n",
    "    channels_list_backbone = config.model.backbone.out_channels\n",
    "    num_repeat_neck = config.model.neck.num_repeats\n",
    "    channels_list_neck = config.model.neck.out_channels\n",
    "    num_anchors = config.model.head.anchors\n",
    "    num_repeat = [(max(round(i * depth_mul), 1) if i > 1 else i) for i in (num_repeat_backbone + num_repeat_neck)]\n",
    "    channels_list = [make_divisible(i * width_mul, 8) for i in (channels_list_backbone + channels_list_neck)]\n",
    "\n",
    "    backbone = EfficientRep(\n",
    "        in_channels=channels,\n",
    "        channels_list=channels_list,\n",
    "        num_repeats=num_repeat\n",
    "    )\n",
    "\n",
    "    neck = RepPANNeck(\n",
    "        channels_list=channels_list,\n",
    "        num_repeats=num_repeat\n",
    "    )\n",
    "\n",
    "    return backbone, neck\n",
    "\n",
    "def make_divisible(x, divisor):\n",
    "    # Upward revision the value x to make it evenly divisible by the divisor.\n",
    "    return math.ceil(x / divisor) * divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8bb7db8b-0fec-4162-af12-3bf7e25cd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone, neck = build_network(cfg, channels=3, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "11d161cb-daff-4160-b744-76c74b6a2343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features =  model.backbone(samples)\n",
    "fpn = model.neck(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9df370de-e6a1-4560-991e-82e5138af10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 128, 240])\n",
      "torch.Size([1, 128, 64, 120])\n",
      "torch.Size([1, 256, 32, 60])\n"
     ]
    }
   ],
   "source": [
    "for i in fpn:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f29d52-64b0-4051-b2a8-a9081d549afb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
